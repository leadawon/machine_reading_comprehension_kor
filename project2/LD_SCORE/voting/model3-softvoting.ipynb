{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "le5gUu-qOx95"
      },
      "source": [
        "## Kaggle\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "id": "LAL888NDOj2V",
        "outputId": "427d7f69-aee8-45d5-a728-e325d1903176"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.8/dist-packages (1.5.12)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.8/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from kaggle) (2.25.1)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.8/dist-packages (from kaggle) (1.15.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.8/dist-packages (from kaggle) (2022.12.7)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.8/dist-packages (from kaggle) (1.24.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from kaggle) (4.64.1)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.8/dist-packages (from kaggle) (7.0.0)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.8/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->kaggle) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->kaggle) (2.10)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-71a7182a-947d-43c0-b8e2-57b76d7f0083\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-71a7182a-947d-43c0-b8e2-57b76d7f0083\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving kaggle.json to kaggle (1).json\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'kaggle.json': b'{\"username\":\"jeonhyotaek\",\"key\":\"d2ddaf4c586e0bf63051e1a4c6a74dd4\"}'}"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "!pip install kaggle\n",
        "from google.colab import files\n",
        "files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NNoJ4AkSOuVI",
        "outputId": "865390bd-0ad1-40c7-e8b4-f8692cb0b4d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "kaggle.json\n"
          ]
        }
      ],
      "source": [
        "ls -1ha kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qU0E6zxLOuTa"
      },
      "outputs": [],
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F4LYW2I4OuRD",
        "outputId": "317acb50-5049-4b03-a908-0bf493a54366"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /root/.kaggle/kaggle.json'\n",
            "6th-goorm-project-2-korean-mrc.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
          ]
        }
      ],
      "source": [
        "!kaggle competitions download -c 6th-goorm-project-2-korean-mrc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_qDBFgn5OuOx",
        "outputId": "a5872b99-2f12-4489-99ab-ccc7d31f2882"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  6th-goorm-project-2-korean-mrc.zip\n",
            "replace baseline.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ],
      "source": [
        "!unzip 6th-goorm-project-2-korean-mrc.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkiNxKnFO05Z"
      },
      "source": [
        "## data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T4euzcqrOuMh"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5JxfHVARuja"
      },
      "source": [
        "## gogo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_yskKCUsOuIw"
      },
      "outputs": [],
      "source": [
        "tokenizer_name = 'snunlp/KR-Medium'\n",
        "model_name = 'snunlp/KR-Medium'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DQr5Ivl7OuGZ"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "from tqdm.notebook import tqdm\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, AdamW"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IkWsSNujOuER",
        "outputId": "2317afa6-80b7-4953-8c4b-2684f642bb9b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'title': '제주도 장마 시작 … 중부는 이달 말부터',\n",
              " 'paragraphs': [{'context': '올여름 장마가 17일 제주도에서 시작됐다. 서울 등 중부지방은 예년보다 사나흘 정도 늦은 이달 말께 장마가 시작될 전망이다.17일 기상청에 따르면 제주도 남쪽 먼바다에 있는 장마전선의 영향으로 이날 제주도 산간 및 내륙지역에 호우주의보가 내려지면서 곳곳에 100㎜에 육박하는 많은 비가 내렸다. 제주의 장마는 평년보다 2~3일, 지난해보다는 하루 일찍 시작됐다. 장마는 고온다습한 북태평양 기단과 한랭 습윤한 오호츠크해 기단이 만나 형성되는 장마전선에서 내리는 비를 뜻한다.장마전선은 18일 제주도 먼 남쪽 해상으로 내려갔다가 20일께 다시 북상해 전남 남해안까지 영향을 줄 것으로 보인다. 이에 따라 20~21일 남부지방에도 예년보다 사흘 정도 장마가 일찍 찾아올 전망이다. 그러나 장마전선을 밀어올리는 북태평양 고기압 세력이 약해 서울 등 중부지방은 평년보다 사나흘가량 늦은 이달 말부터 장마가 시작될 것이라는 게 기상청의 설명이다. 장마전선은 이후 한 달가량 한반도 중남부를 오르내리며 곳곳에 비를 뿌릴 전망이다. 최근 30년간 평균치에 따르면 중부지방의 장마 시작일은 6월24~25일이었으며 장마기간은 32일, 강수일수는 17.2일이었다.기상청은 올해 장마기간의 평균 강수량이 350~400㎜로 평년과 비슷하거나 적을 것으로 내다봤다. 브라질 월드컵 한국과 러시아의 경기가 열리는 18일 오전 서울은 대체로 구름이 많이 끼지만 비는 오지 않을 것으로 예상돼 거리 응원에는 지장이 없을 전망이다.',\n",
              "   'qas': [{'question': '북태평양 기단과 오호츠크해 기단이 만나 국내에 머무르는 기간은?',\n",
              "     'answers': [{'text': '한 달가량', 'answer_start': 478},\n",
              "      {'text': '한 달', 'answer_start': 478}],\n",
              "     'guid': '798db07f0b9046759deed9d4a35ce31e'}]}],\n",
              " 'news_category': '종합',\n",
              " 'source': 'hankyung'}"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "with open(\"train.json\", 'rb') as f:\n",
        "    input_dict = json.load(f)\n",
        "input_dict[\"data\"][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0DxIyRfTOuCI",
        "outputId": "e9a404af-d3df-4995-a959-04ddb3895342"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'title': '부산정보산업진흥원, 과기부 지역SW서비스사업화 지원사업 4개 과제 선정',\n",
              " 'paragraphs': [{'context': '부산시와 (재)부산정보산업진흥원(원장 이인숙)이 ‘2020~2021년 지역SW서비스사업화 지원사업’ 공모사업에 4개 과제가 선정되어 본격적인 사업 착수에 나선다. 과학기술정보통신부가 주관하는 ‘지역SW서비스사업화 지원사업’은 강소SW기업 및 초기 스타트업의 SW서비스 사업화 지원과 신시장 진출 지원을 통해 기업 경쟁력 강화와 지역경제 활성화를 도모하는 사업이다. 올해부터 2개년으로 진행되며, 국비와 시비, 민자 등 2년간 약 37억원의 예산이 투입된다. 앞서 진흥원은 부산의 미래 먹거리산업인 스마트해양, 지능형기계, 지능정보서비스 분야로 사전 수요조사를 진행했고, 평가를 통해 선정된 5개 과제를 공모사업에 신청했다. 그 결과 부산의 4개 과제가 최종 선정되는 쾌거를 거뒀다. 당 사업은 전국 진흥기관을 대상으로 공모를 시작해, 총 17개 지역에서 42개 과제가 선정되었으며, 4개 과제가 선정된 곳은 부산과 강원지역 뿐이다. 금번 선정된 과제들은 ‘인공지능융합센서와 서보 이송 로봇을 이용한 전단보강재의 자동용접시스템 개발’ 등 총 4개 과제다. 부산시가 지원하고, 부산정보산업진흥원과 지역기업, 대학, 연구소 등이 컨소시엄을 구성하여 기술개발 및 사업화 지원을 추진한다. 2개의 Track으로 구분되는 이번사업은 Track 1(SW중소기업)에서 ㈜에이아이플랫폼, 엔컴(주), Track 2(스타트업)에서는 ㈜토즈, 삼보테크놀로지를 지원한다. ○ ‘Track 1‘의 (주)에이아이플랫폼이 주관기업으로 진행하는 <인공지능 기반 망막 내 아밀로이드 플라크 영상 분석을 통한 치매조기진단 플랫폼 상용화>는 치매 확진의 원인이 되는 중요 단백질(아밀로이드 플라크)을 자체개발 관측장비로 진단한다. 이를 통해 치매를 조기 발견하여, 각종 경제적 비용과 치료 및 예방 등 사회적 문제를 해 결하고 시민들이 쉽게 접근 가능한 실효성 있는 치매관리체계 개발을 목표로 한다. ○ 엔컴(주)이 주관기업으로 참여하는 <AI영상분석 기반 가공철근 생산성 향상 시스템 기술개발 및 사업화>는 산업안전, 환경규제, 생산체계의 변화로 침체된 부산 핵심 산업인 철강업 활성화에 나선다. 실시간으로 절곡되어 나오는 가공철근의 형상을 인식하고 불량 형상 판단 시 적합한 교정 값을 절곡설비에 전달함으로써, 무중단 생산이 가능한 영상분석 기술과 생산설비 자동화 제어기술을 개발한다. ○ ‘Track 2’의 ㈜토즈는 자립기반이 약한 국내 중소형 조선소의 산업기술 변화에 혁신적인 대응을 위해 <가상현실 기반 원격 다자간 선박 및 해양구조물 사전 검사 시스템>을 개발한다. 선박 건조 前, 설계 단계에서 설계자 뿐만 아니라 생산관리자, 품질관리자, 선급검사관, 선주감독관 등의 이해관계자가 공동으로 가상의 환경에서 선박 및 해양구조물의 자재 배치와 간섭, 작업성, 설계 오작 등에 대한 검사를 진행할 수 있는 기술을 확보하여 조선소의 업무효율을 극대화 할 예정이다. ○ 삼보테크놀로지는 재래식 건설 부자재의 시공성, 안전성, 내구성 등의 문제점을 보완하여 시민 안전과 건설근로자의 환경개선, 생산성 및 수익성 향상을 위해 <인공지능융합센서와 새들형 토치 서보 이송 로봇을 이용한 고속 SRD 전단보강재 자동용접시스템>을 개발한다. 로봇응용 SRD 용접자동화 설비를 제작하고, 용접 모니터링 및 품질검사 소프트웨어를 개발하여 건설분야에 4차산업 대비 지능형 생산자동화 기반기술을 확보할 예정이다. (재)부산정보산업진흥원 이인숙 원장은 “이번 코로나19 사태로 인해 부산 기업들이 매출과 고용유지, 자재수급 등에 큰 타격을 입었지만, 지역SW서비스사업화 지원사업을 통해 지역과 기업차원에서 기반을 다지는 계기가 됐으면 좋겠다‘며 ”진흥원은 어려운 사태를 대비해 지역 기업들을 지원할 수 있는 다른 방편을 계속 모색 중이며, 더욱 성장해 나갈 수 있도록 적극 지원하겠다“고 전했다.',\n",
              "   'qas': [{'question': '지능형 생산자동화 기반기술을 개발중인 스타트업은?',\n",
              "     'answers': [{'text': '삼보테크놀로지', 'answer_start': 1422}],\n",
              "     'guid': '67c85e4f86ae43939b807684537c909c'}]}],\n",
              " 'news_category': '경제',\n",
              " 'source': 'acrofan'}"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "input_dict[\"data\"][1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2EvB1moGOt55"
      },
      "outputs": [],
      "source": [
        "from copy import deepcopy\n",
        "def split_input_dict(input_dict, ratio = 0.1, seed = 42):\n",
        "    split_point = int(len(input_dict['data']) * ratio)\n",
        "    random.seed(seed)\n",
        "    random.shuffle(input_dict['data'])\n",
        "    valid_dict = deepcopy(input_dict)\n",
        "    train_dict = input_dict\n",
        "\n",
        "    valid_dict['data'] = input_dict['data'][:split_point]\n",
        "    train_dict['data'] = input_dict['data'][split_point:]\n",
        "    return train_dict, valid_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DHJppuYaO9Nl"
      },
      "outputs": [],
      "source": [
        "def read_input(path):\n",
        "    with open(path, 'rb') as f:\n",
        "        input_dict = json.load(f)\n",
        "    train_dict,valid_dict =split_input_dict(input_dict)\n",
        "    train_contexts = []\n",
        "    train_questions = []\n",
        "    train_answers = []\n",
        "    for group in tqdm(train_dict['data']):       #딕셔너리 하나씩 꺼낸다.\n",
        "        for passage in group['paragraphs']:     #딕셔너리의 paragraphs\n",
        "            context = passage['context']        #paragraphs의 context\n",
        "            for qa in passage['qas']:           #paragraphs의 qas\n",
        "                question = qa['question']       #paragraphs의 question\n",
        "                for answer in qa['answers']:    #question의 answers\n",
        "                    ### context 넘겨서 자르기 ###\n",
        "                    target_context = context\n",
        "                    if answer['answer_start'] > 1400:\n",
        "                        target_context = target_context[1200:]\n",
        "                        answer['answer_start'] -= 1200 \n",
        "                    elif answer['answer_start'] > 1200:\n",
        "                        target_context = target_context[1000:]\n",
        "                        answer['answer_start'] -= 1000\n",
        "                    elif answer['answer_start'] > 1000:\n",
        "                        target_context = target_context[800:]\n",
        "                        answer['answer_start'] -= 800\n",
        "                    elif answer['answer_start'] > 800:\n",
        "                        target_context = target_context[600:]\n",
        "                        answer['answer_start'] -= 600\n",
        "                    \n",
        "                    \n",
        "                    train_contexts.append(target_context)    #answers의 한 answer당 해당하는 context 저장\n",
        "                    ### context 넘겨서 자르기 ###\n",
        "                    train_questions.append(question)  #answers의 한 answer당 해당하는 question 저장\n",
        "                    train_answers.append(answer)      #answers의 한 answer 저장\n",
        "  \n",
        "    valid_contexts = []\n",
        "    valid_questions = []\n",
        "    valid_answers = []\n",
        "    for group in tqdm(valid_dict['data']):       #딕셔너리 하나씩 꺼낸다.\n",
        "        for passage in group['paragraphs']:     #딕셔너리의 paragraphs\n",
        "            context = passage['context']        #paragraphs의 context\n",
        "            for qa in passage['qas']:           #paragraphs의 qas\n",
        "                question = qa['question']       #paragraphs의 question\n",
        "                for answer in qa['answers']:    #question의 answers\n",
        "                    ### context 넘겨서 자르기 ###\n",
        "                    target_context = context\n",
        "                    if answer['answer_start'] > 1400:\n",
        "                        target_context = target_context[1200:]\n",
        "                        answer['answer_start'] -= 1200 \n",
        "                    elif answer['answer_start'] > 1200:\n",
        "                        target_context = target_context[1000:]\n",
        "                        answer['answer_start'] -= 1000\n",
        "                    elif answer['answer_start'] > 1000:\n",
        "                        target_context = target_context[800:]\n",
        "                        answer['answer_start'] -= 800\n",
        "                    elif answer['answer_start'] > 800:\n",
        "                        target_context = target_context[600:]\n",
        "                        answer['answer_start'] -= 600\n",
        "\n",
        "                    \n",
        "                    valid_contexts.append(target_context)    #answers의 한 answer당 해당하는 context 저장\n",
        "                    ### context 넘겨서 자르기 ###\n",
        "                    valid_questions.append(question)  #answers의 한 answer당 해당하는 question 저장\n",
        "                    valid_answers.append(answer)      #answers의 한 answer 저장\n",
        "\n",
        "    return train_contexts, train_questions, train_answers, valid_contexts, valid_questions, valid_answers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vKGCz_4WO9Lg"
      },
      "outputs": [],
      "source": [
        "def add_end_idx(answers, contexts):\n",
        "    for answer, context in zip(answers, contexts):\n",
        "        gold_text = answer['text']\n",
        "        start_idx = answer['answer_start']\n",
        "        end_idx = start_idx + len(gold_text)\n",
        "\n",
        "        if context[start_idx:end_idx] == gold_text:\n",
        "            answer['answer_end'] = end_idx\n",
        "        elif context[start_idx-1:end_idx-1] == gold_text:\n",
        "            print(\"there is an unitended error in dataset\") #이렇게까지 할 필요가 있나?\n",
        "            answer['answer_start'] = start_idx - 1\n",
        "            answer['answer_end'] = end_idx - 1\n",
        "        elif context[start_idx-2:end_idx-2] == gold_text:\n",
        "            print(\"there is an unitended error in dataset\")\n",
        "            answer['answer_start'] = start_idx - 2\n",
        "            answer['answer_end'] = end_idx - 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1kXdeER8O9Jp"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hz1kYCgCO9Hx",
        "outputId": "65ede2dc-210f-4db2-a72c-7f5e17c420ed"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'input_ids': [2, 3623, 5353, 5429, 3778, 19151, 13790, 12636, 8454, 8756, 9087, 18, 8603, 2538, 3898, 12705, 5112, 5019, 3612, 5049, 8530, 3195, 5098, 5670, 8789, 2377, 5019, 16527, 2808, 5186, 3778, 19151, 8756, 5058, 19286, 18, 13790, 17747, 18997, 10053, 12636, 17838, 2838, 17394, 5051, 8494, 3778, 5224, 5151, 13366, 9782, 8452, 9173, 12636, 3199, 5162, 2953, 2257, 5941, 9987, 5051, 4568, 5296, 8548, 15961, 9070, 16883, 2016, 11671, 8750, 7299, 5051, 3708, 5430, 8453, 8730, 3099, 5040, 19947, 18, 11856, 5105, 3778, 19160, 4444, 5049, 8530, 22, 98, 11817, 16, 9767, 10581, 9054, 15458, 8756, 9087, 18, 3778, 19160, 2007, 10125, 5673, 5062, 3062, 5465, 16608, 2099, 5069, 5138, 4494, 5234, 3372, 5655, 5062, 3619, 5147, 5711, 5356, 5033, 2099, 11222, 10344, 14672, 8606, 3778, 5224, 5151, 13702, 14212, 5035, 3099, 5029, 2644, 8470, 18, 3778, 5224, 5151, 13025, 13743, 12636, 2838, 17838, 4511, 9910, 9070, 10082, 5040, 13144, 5186, 8686, 3062, 5139, 5033, 13416, 2246, 5033, 5357, 8477, 11394, 3894, 8804, 9975, 18, 9522, 8758, 8517, 98, 13784, 2246, 12705, 10958, 5067, 3612, 5049, 8530, 3195, 5670, 8789, 3778, 19151, 15458, 9309, 5719, 19286, 18, 9264, 3778, 5224, 5151, 10186, 10767, 5719, 8557, 3062, 5465, 16608, 18433, 5255, 18773, 3546, 5033, 8603, 2538, 3898, 12705, 5112, 5019, 4444, 5049, 8530, 3195, 5098, 5670, 5040, 5801, 2377, 5019, 16527, 2808, 8504, 3778, 19151, 8756, 5058, 14737, 1977, 17747, 5237, 5105, 9677, 8459, 18, 3778, 5224, 5151, 13025, 8932, 4494, 2401, 5040, 5801, 10949, 3898, 5175, 10199, 11721, 5207, 12320, 2016, 11671, 3099, 5029, 3174, 5607, 19286, 18, 9597, 12551, 5162, 10855, 17137, 10053, 3898, 12705, 5112, 5105, 3778, 5224, 8756, 10928, 10795, 19532, 98, 13387, 9139, 8809, 3778, 5224, 10592, 5019, 13228, 5037, 16, 1932, 5124, 18606, 5035, 9402, 18, 13158, 9876, 18, 17747, 16640, 9605, 3778, 5224, 10592, 5105, 10855, 1932, 5124, 12308, 12037, 5080, 98, 15273, 7299, 5016, 4444, 5049, 5138, 10198, 10086, 3806, 5008, 8804, 19679, 12736, 18, 13680, 5064, 15395, 17071, 9873, 5105, 12524, 16489, 13743, 11344, 8603, 5019, 9376, 5016, 2042, 5429, 5022, 8648, 2230, 8482, 3099, 5035, 10966, 10117, 8804, 10302, 5106, 10609, 9081, 8540, 3940, 8768, 10143, 19286, 18, 3, 3062, 5465, 16608, 2099, 5069, 5138, 3619, 5147, 5711, 5356, 5033, 2099, 11222, 10344, 19141, 2836, 5030, 9356, 10505, 5019, 35, 3], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "question = \"북태평양 기단과 오호츠크해 기단이 만나 국내에 머무르는 기간은?\"\n",
        "context = \"올여름 장마가 17일 제주도에서 시작됐다. 서울 등 중부지방은 예년보다 사나흘 정도 늦은 이달 말께 장마가 시작될 전망이다.17일 기상청에 따르면 제주도 남쪽 먼바다에 있는 장마전선의 영향으로 이날 제주도 산간 및 내륙지역에 호우주의보가 내려지면서 곳곳에 100㎜에 육박하는 많은 비가 내렸다. 제주의 장마는 평년보다 2~3일, 지난해보다는 하루 일찍 시작됐다. 장마는 고온다습한 북태평양 기단과 한랭 습윤한 오호츠크해 기단이 만나 형성되는 장마전선에서 내리는 비를 뜻한다.장마전선은 18일 제주도 먼 남쪽 해상으로 내려갔다가 20일께 다시 북상해 전남 남해안까지 영향을 줄 것으로 보인다. 이에 따라 20~21일 남부지방에도 예년보다 사흘 정도 장마가 일찍 찾아올 전망이다. 그러나 장마전선을 밀어올리는 북태평양 고기압 세력이 약해 서울 등 중부지방은 평년보다 사나흘가량 늦은 이달 말부터 장마가 시작될 것이라는 게 기상청의 설명이다. 장마전선은 이후 한 달가량 한반도 중남부를 오르내리며 곳곳에 비를 뿌릴 전망이다. 최근 30년간 평균치에 따르면 중부지방의 장마 시작일은 6월24~25일이었으며 장마기간은 32일, 강수일수는 17.2일이었다.기상청은 올해 장마기간의 평균 강수량이 350~400㎜로 평년과 비슷하거나 적을 것으로 내다봤다. 브라질 월드컵 한국과 러시아의 경기가 열리는 18일 오전 서울은 대체로 구름이 많이 끼지만 비는 오지 않을 것으로 예상돼 거리 응원에는 지장이 없을 전망이다.\"\n",
        "tokenizer(context, question)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1E9UJAXRO9Fi"
      },
      "outputs": [],
      "source": [
        "class Dataset_base(Dataset):\n",
        "    def __init__(self, contexts, questions, answers, model_max_position_embedings, tokenizer):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.answers = answers\n",
        "        self.questions = questions\n",
        "        self.contexts = contexts\n",
        "        self.model_max_position_embedings = model_max_position_embedings\n",
        "        print(\"Tokenizing ...\")\n",
        "        self.encodings = self.tokenizer(self.contexts, \n",
        "                                        self.questions,\n",
        "                                        max_length=512, #512 truncation // \n",
        "                                        truncation=True,\n",
        "                                        padding=\"max_length\",\n",
        "                                        return_token_type_ids=False)\n",
        "        print(\"Done !!!\")\n",
        "        self.add_token_positions()\n",
        "        \n",
        "    def add_token_positions(self):\n",
        "        start_positions = []\n",
        "        end_positions = []\n",
        "        for i in range(len(self.answers)):\n",
        "            start_positions.append(self.encodings.char_to_token(i, self.answers[i]['answer_start']))\n",
        "            end_positions.append(self.encodings.char_to_token(i, self.answers[i]['answer_end'] - 1)) # -1으로 : 진짜로 답이 있는 end_position 의 인덱스를 구함.(char_to_token은 인덱스를 구함)\n",
        "            #https://huggingface.co/docs/tokenizers/v0.13.2/en/api/encoding#tokenizers.Encoding.char_to_token\n",
        "\n",
        "            # positions 값이 None 값이라면, answer가 포함된 context가 잘렸다는 의미\n",
        "            if start_positions[-1] is None:\n",
        "                print(\"there is an error 1\")\n",
        "                start_positions[-1] = self.model_max_position_embedings\n",
        "            if end_positions[-1] is None:\n",
        "                print(\"there is an error 2\")\n",
        "                end_positions[-1] = self.model_max_position_embedings\n",
        "\n",
        "        self.encodings.update({'start_positions': start_positions, 'end_positions': end_positions})\n",
        "\n",
        "        \n",
        "    def get_data(self):\n",
        "        return {\"contexts\":self.contexts, 'questions':self.questions, 'answers':self.answers}\n",
        "    \n",
        "    \n",
        "    def get_encodings(self):\n",
        "        return self.encodings\n",
        "        \n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return {key:torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.encodings['input_ids'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191,
          "referenced_widgets": [
            "dd81ae5b8a204a1fb452b81b532284ce",
            "7a55a2e5f684464e94afe6f75b2b4b82",
            "4494a918ad88472589c09a8480f58410",
            "c9ec77da4b8044aeab7345e25b850099",
            "1f1ab3e347b24a5e90fa136432e021ba",
            "e8b11f3e36ad448f93a71bff29340c27",
            "3bada5a50a054b768105b1ff8f8861da",
            "e8e9d7c40aea4ff9983955965daff451",
            "db4ad55a99e8448f8c7be331bf2c3f98",
            "50c7366cd7f84b4d9d271a0d6302109e",
            "d79f3327d9504f9388a522079e70316e",
            "b77193eb20bb424b898bc38d3000da16",
            "15f39a9837d84f51b4d47fab6c2b3dec",
            "97a2baba34184dd5b9c97a82083c4a7d",
            "0c08c10ffa1d4c669a4a0a889b631bca",
            "7e81f9a147594f58917fec26151893fb",
            "a37677e6b0ad47dc9b98e8201e7a5b35",
            "acb8d937aed347af8947b4c7abea5a1a",
            "7637d6159ae74c37af946abf8d2d7668",
            "742652c90036434b914a0f2f3fd07599",
            "8d01d9168e21415595b017e4c3f6f72e",
            "2438c5eff890449186aec9fac257e0d0"
          ]
        },
        "id": "ynDpvL5YRujc",
        "outputId": "fc27ff44-45b1-4995-f043-bcb42ee32787"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dd81ae5b8a204a1fb452b81b532284ce",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/8811 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b77193eb20bb424b898bc38d3000da16",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/978 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokenizing ...\n",
            "Done !!!\n",
            "there is an error 2\n",
            "there is an error 2\n",
            "Tokenizing ...\n",
            "Done !!!\n"
          ]
        }
      ],
      "source": [
        "train_contexts, train_questions, train_answers, valid_contexts, valid_questions, valid_answers = read_input(\"train.json\")\n",
        "add_end_idx(train_answers, train_contexts)                                                      #anwer 마다 answer_end 달아준다.\n",
        "train_dataset = Dataset_base(train_contexts, train_questions, train_answers, 512, tokenizer)\n",
        "\n",
        "add_end_idx(valid_answers, valid_contexts)                                                      #anwer 마다 answer_end 달아준다.\n",
        "valid_dataset = Dataset_base(valid_contexts, valid_questions, valid_answers, 512, tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AcZAGePiPL_I",
        "outputId": "9793391e-9a6d-40d0-edc0-7b79c976dbbf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at snunlp/KR-Medium were not used when initializing BertForQuestionAnswering: ['cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at snunlp/KR-Medium and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "model = AutoModelForQuestionAnswering.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cWXsoBx--_26"
      },
      "outputs": [],
      "source": [
        "!pip install wandb -qqq\n",
        "import wandb\n",
        "wandb.login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PDPGeivV_M1w"
      },
      "outputs": [],
      "source": [
        "sweep_config = {\n",
        "      'name' : 'project2_TEST1.ipynb',\n",
        "      'method' : 'grid',\n",
        "      'metric':{\n",
        "          'name': 'total_valid_loss',\n",
        "          'goal': 'minimize'  \n",
        "      },\n",
        "      'parameters' : {\n",
        "          'learning_rate' : {\n",
        "              'values' : [1e-4, 2.5e-5, 5e-5, 7.5e-5,1e-5]  \n",
        "          },   \n",
        "          'batch_size' :{\n",
        "              'values' : [4,8,16]\n",
        "          },\n",
        "          'epochs' : {\n",
        "              'values' : [2] \n",
        "          }\n",
        "      }\n",
        "}\n",
        "\n",
        "sweep_id = wandb.sweep(sweep_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rTg8-yjFPL85"
      },
      "outputs": [],
      "source": [
        "EPOCH = 1\n",
        "LEARNING_RATE = 5e-5\n",
        "BATCH_SIZE = 8\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "NYZrOKTqQDBp",
        "outputId": "e8610a59-9c62-46f9-afc7-104377334aed"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nlevenshtein_distance 값이 튀는 현상 방지.\\n - 길이가 너무 긴 정답 삭제 max_length = 20\\n - 정답과 어느정도 길이차이나는 값 삭제(위와 동일) 5.9 -> 2정도 차이나는 값 삭제 7.9\\n \\n\\n자연어처리\\n자연어처리과정 2\\n0 5\\n\\n정답 길이보다 예측 정답 길이가 길지만 2배이상 차이가 나지 않는 경우에\\n0으로 처리하면 LD_SCORE가 더 안나옴\\n따라서 위 상황에서는 1.5 ~ 1.8 정도 해당하는 길이로 잘라서 반환.\\n\\ntest 답 X\\ntrain 과정 나온 답 -> 바꿔서 바꾼 데이터로 \\n\\n\\n** max_len = 5.9 * 2 = 12\\n\\ntrain 2번\\n1 train 원래 데이터.\\n2 train LD 변환한 데이터로 한번.\\n\\n'"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Levenshtein_distance (Evaluation)\n",
        "\n",
        "import numpy \n",
        "import torch\n",
        "import os\n",
        "\n",
        "def levenshtein_distance(s1,s2, debug=False): #레벤슈타인 거리 eval / 정답 s1과 도출한 모델 s2 비교 평가\n",
        "    if len(s1) < len(s2):\n",
        "        return levenshtein_distance(s2, s1, debug)\n",
        "\n",
        "    if len(s2) == 0:\n",
        "        return len(s1)\n",
        "\n",
        "    previous_row = range(len(s2) + 1)\n",
        "    for i, c1 in enumerate(s1):\n",
        "        current_row = [i + 1]\n",
        "        for j, c2 in enumerate(s2):\n",
        "            insertions = previous_row[j + 1] + 1\n",
        "            deletions = current_row[j] + 1\n",
        "            substitutions = previous_row[j] + (c1 != c2)\n",
        "            current_row.append(min(insertions, deletions, substitutions))\n",
        "\n",
        "        if debug:\n",
        "            print(current_row[1:])\n",
        "\n",
        "        previous_row = current_row\n",
        "\n",
        "    return previous_row[-1] # levenshtein_distance 값 출력력\n",
        "\n",
        "\n",
        "def LD_SCORE(start_positions, end_positions, input_ids, STA_logits, END_logits):\n",
        "\n",
        "    answer1 = [] # 정답 저장\n",
        "    answer2 = [] # 예측 정답 저장\n",
        "\n",
        "    if len(input_ids) != BATCH_SIZE: #오류 해결\n",
        "       print(\"input_ids ERROR\")\n",
        "       return 0\n",
        "\n",
        "    for i in range(BATCH_SIZE): # 기존 정답 // index 1 is out of bounds for dimension 0 with size 1 오류 발생\n",
        "        if input_ids[i] == [] :\n",
        "            print(\"input_ids ERROR\")\n",
        "            continue       \n",
        "        else:\n",
        "            PRED_IDE = input_ids[i][start_positions[i]: end_positions[i] + 1]\n",
        "            PRED_ANS = tokenizer.decode(PRED_IDE)\n",
        "            answer1.append(PRED_ANS)\n",
        "            #print(PRED_ANS)\n",
        "\n",
        "    for i in range(BATCH_SIZE): # 예측 정답\n",
        "        if input_ids[i] == [] :\n",
        "            print(\"input_ids ERROR\")\n",
        "            continue\n",
        "        else:\n",
        "            TK_start_index, TK_end_index = STA_logits.argmax(dim=-1), END_logits.argmax(dim=-1)\n",
        "            PRED_IDE2 = input_ids[i][TK_start_index[i]: TK_end_index[i] + 1]\n",
        "            PRED_ANS2 = tokenizer.decode(PRED_IDE2)\n",
        "            answer2.append(PRED_ANS2)\n",
        "            # print(PRED_ANS2)\n",
        "\n",
        "    batch_score = LD_comparison(answer1, answer2)\n",
        "\n",
        "    return batch_score\n",
        "\n",
        "\n",
        "def LD_comparison(answer1,answer2):\n",
        "\n",
        "    # 배치마다 레벤슈타인 거리 평균 구해서 출력. (train, valid 과정에서 배치마다 평균거리 구하고.)\n",
        "    # train 전체 평균 거리\n",
        "    # valid 전체 평균 거리\n",
        "\n",
        "    batch_LD_score = []\n",
        "\n",
        "    for i in range(BATCH_SIZE):\n",
        "        if answer1[i] == answer2[i]: # 같으면 LD 구하는 과정 생략.\n",
        "            batch_LD_score.append(0)\n",
        "        else:\n",
        "            batch_LD_score.append(levenshtein_distance(answer1[i],answer2[i]))\n",
        "\n",
        "    sum_LD_score = sum(batch_LD_score)\n",
        "    LD_avg = sum_LD_score / BATCH_SIZE # 배치의 레벤슈타인 거리 평균\n",
        "    #print(LD_avg)\n",
        "\n",
        "    return LD_avg\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "levenshtein_distance 값이 튀는 현상 방지.\n",
        " - 길이가 너무 긴 정답 삭제 max_length = 20\n",
        " - 정답과 어느정도 길이차이나는 값 삭제(위와 동일) 5.9 -> 2정도 차이나는 값 삭제 7.9\n",
        " \n",
        "\n",
        "자연어처리\n",
        "자연어처리과정 2\n",
        "0 5\n",
        "\n",
        "정답 길이보다 예측 정답 길이가 길지만 2배이상 차이가 나지 않는 경우에\n",
        "0으로 처리하면 LD_SCORE가 더 안나옴\n",
        "따라서 위 상황에서는 1.5 ~ 1.8 정도 해당하는 길이로 잘라서 반환.\n",
        "\n",
        "test 답 X\n",
        "train 과정 나온 답 -> 바꿔서 바꾼 데이터로 \n",
        "\n",
        "\n",
        "** max_len = 5.9 * 2 = 12\n",
        "\n",
        "train 2번\n",
        "1 train 원래 데이터.\n",
        "2 train LD 변환한 데이터로 한번.\n",
        "\n",
        "'''\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "69QT4ZwJPL4A"
      },
      "outputs": [],
      "source": [
        "# LD SCORE 저장\n",
        "train_LD_avg = [] \n",
        "valid_LD_avg = []\n",
        "\n",
        "def train_runner(model, train_dataset, valid_dataset , batch_size, num_train_epochs, learning_rate):\n",
        "\n",
        "    #wandb.init(project = 'project2_test1',reinit=True)\n",
        "\n",
        "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "    \n",
        "    model.to(device)\n",
        "    model.train()\n",
        "    train_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size)\n",
        "    valid_dataloader = DataLoader(dataset = valid_dataset, batch_size = batch_size)\n",
        "\n",
        "    lowest_total_valid_loss = 9999.\n",
        "\n",
        "    global_total_step = len(train_dataloader) * num_train_epochs\n",
        "    optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=0)\n",
        "    print(\"TRAIN START\")\n",
        "    with tqdm(total=global_total_step, unit='step') as t:\n",
        "        total = 0\n",
        "        total_loss = 0\n",
        "        for epoch in range(num_train_epochs):\n",
        "            for iteration,batch in enumerate(train_dataloader):\n",
        "                optimizer.zero_grad()\n",
        "                input_ids = batch['input_ids'].to(device)\n",
        "                attention_mask = batch['attention_mask'].to(device)\n",
        "                start_positions = batch['start_positions'].to(device)\n",
        "                end_positions = batch['end_positions'].to(device)                \n",
        "\n",
        "                outputs = model(input_ids,\n",
        "                             attention_mask=attention_mask,\n",
        "                             start_positions=start_positions,\n",
        "                             end_positions=end_positions)\n",
        "                \n",
        "                ### LD_SCORE - train\n",
        "                STA_logits, END_logits = outputs.start_logits, outputs.end_logits\n",
        "                score_save1 = LD_SCORE(start_positions, end_positions, input_ids, STA_logits, END_logits)\n",
        "                train_LD_avg.append(score_save1)\n",
        "                #wandb.log({'train_batch_LD':score_save1})\n",
        "\n",
        "                ####\n",
        "                #loss_logit_change(start_positions, end_positions, input_ids, STA_logits, END_logits)\n",
        "                loss = outputs.loss\n",
        "                #wandb loss\n",
        "                #wandb.log({'Train_loss':loss.item()})\n",
        "\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                \n",
        "                batch_loss = loss.item() * len(input_ids)\n",
        "                total += len(input_ids)\n",
        "                total_loss += batch_loss\n",
        "                global_total_step += 1\n",
        "                t.set_postfix(loss=\"{:.6f}\".format(total_loss / total), batch_loss=\"{:.6f}\".format(batch_loss))\n",
        "                t.update(1)\n",
        "\n",
        "                # wandb loss\n",
        "                #wandb.log({'Train_batch_loss':batch_loss})\n",
        "                #wandb.log({'LOSS':total_loss / total})\n",
        "                \n",
        "                del input_ids\n",
        "                del attention_mask\n",
        "                del start_positions\n",
        "                del end_positions\n",
        "                del outputs\n",
        "                del loss\n",
        "\n",
        "                ## validation ##\n",
        "                if iteration != 0 and iteration % int(len(train_dataloader) / 5) == 0:\n",
        "                    total_valid_loss = 0\n",
        "                    for batch_val in valid_dataloader:\n",
        "                        model.eval()\n",
        "                        optimizer.zero_grad()\n",
        "\n",
        "                        input_ids = batch_val['input_ids'].to(device)\n",
        "                        attention_mask = batch_val['attention_mask'].to(device)\n",
        "                        start_positions = batch_val['start_positions'].to(device)\n",
        "                        end_positions = batch_val['end_positions'].to(device)\n",
        "                \n",
        "                        with torch.no_grad():\n",
        "                            outputs = model(input_ids,\n",
        "                                    attention_mask=attention_mask,\n",
        "                                    start_positions=start_positions,\n",
        "                                    end_positions=end_positions)\n",
        "                            \n",
        "                            ### LD_SCORE - valid\n",
        "                            STA_logits, END_logits = outputs.start_logits, outputs.end_logits\n",
        "                            score_save2 = LD_SCORE(start_positions, end_positions, input_ids, STA_logits, END_logits)\n",
        "                            valid_LD_avg.append(score_save2)\n",
        "\n",
        "                            # wandb LD\n",
        "                            #wandb.log({'Valid_batch_LD':score_save2})\n",
        "\n",
        "                            loss = outputs.loss\n",
        "                            total_valid_loss += loss.item()\n",
        "\n",
        "                            # wandb loss\n",
        "                            #wandb.log({'Valid_loss':loss.item()})\n",
        "                            #wandb.log({'total_valid_loss':total_valid_loss})\n",
        "                    \n",
        "                    if total_valid_loss < lowest_total_valid_loss:\n",
        "                        print(f\"lowest_total_valid_loss: {total_valid_loss} epoch : {epoch} iteration : {iteration}\")\n",
        "                        torch.save(model.state_dict(),'./output_model_best')\n",
        "                        lowest_total_valid_loss = total_valid_loss\n",
        "                ## validation ##\n",
        "\n",
        "    #model.save_pretrained(\"./klue_output_model\")\n",
        "    # train, valid 평균 LD 거리\n",
        "    train_AVG = sum(train_LD_avg) / len(train_LD_avg)\n",
        "    valid_AVG = sum(valid_LD_avg) / len(valid_LD_avg)\n",
        "    #wandb.log({'train_AVG_LD':train_AVG})\n",
        "    #wandb.log({'valid_AVG_LD':valid_AVG})\n",
        "\n",
        "    print('train LD average',train_AVG,'valid LD average',valid_AVG)\n",
        "    print(\"TRAIN END\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zXl1Yo7y_3JH"
      },
      "outputs": [],
      "source": [
        "# wandb sweep train 실행.\n",
        "\n",
        "# wandb.agent( sweep_id , function=train_runner, count=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 124,
          "referenced_widgets": [
            "9415fc87515f4ecf8216f33ce30f1f44",
            "1411439e6514499e840f813b434f4029",
            "2dc2988f9a6046cb928f7cbabe915f67",
            "c8dcc8eeb4674acd822759f66e7bac7e",
            "9ec4e0e4d05b4088a6153a806f60a8dd",
            "d4a45098fe194121babc81417b3379bb",
            "777a74fcc38044ccab25b7813d5a5eb1",
            "d40ffd7f50dc4db09d4d56ad5b3652f7",
            "2eb9f9cc6bb548fa94281ec27e733f64",
            "8dcf67b7c9d6412bb56d6ee9856e7931",
            "d11a90be8f1244de9a50471667080617"
          ]
        },
        "id": "ywX4WIJXRuje",
        "outputId": "c4a9d8b2-576e-4083-fad1-d997b0a25cb0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TRAIN START\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9415fc87515f4ecf8216f33ce30f1f44",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1986 [00:00<?, ?step/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "input_ids ERROR\n",
            "lowest_total_valid_loss: 1007.4517998695374 epoch : 0 iteration : 397\n",
            "input_ids ERROR\n",
            "lowest_total_valid_loss: 968.1918995380402 epoch : 0 iteration : 794\n",
            "input_ids ERROR\n",
            "lowest_total_valid_loss: 917.1872675418854 epoch : 0 iteration : 1191\n",
            "input_ids ERROR\n",
            "lowest_total_valid_loss: 881.3048665523529 epoch : 0 iteration : 1588\n",
            "input_ids ERROR\n",
            "input_ids ERROR\n",
            "lowest_total_valid_loss: 850.602402806282 epoch : 0 iteration : 1985\n",
            "train LD average 64.12342648539779 valid LD average 64.4127802690583\n",
            "TRAIN END\n"
          ]
        }
      ],
      "source": [
        "train_runner(model,train_dataset,valid_dataset, BATCH_SIZE, EPOCH, LEARNING_RATE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "he9L3jYZC8Rr"
      },
      "outputs": [],
      "source": [
        "MAX_LEN = 10\n",
        "\n",
        "# train / valid 과정에서 나온 이상값 제거 (정확한 점수 X)\n",
        "\n",
        "train_LD_avg = [v for v in train_LD_avg if v < MAX_LEN]  \n",
        "TR_AVG = sum(train_LD_avg) / len(train_LD_avg) \n",
        "\n",
        "valid_LD_avg = [v for v in valid_LD_avg if v < MAX_LEN]\n",
        "VA_AVG = sum(valid_LD_avg) / len(valid_LD_avg)\n",
        "\n",
        "print('predicted Train Avgrage LD',TR_AVG,'predicted Valid Avgrage LD',VA_AVG) # 예측값 확인."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZDVfXLBsPLrQ"
      },
      "outputs": [],
      "source": [
        "def read_dev(path):\n",
        "    with open(path, 'rb') as f:\n",
        "        kdict = json.load(f)\n",
        "\n",
        "    contexts = []\n",
        "    questions = []\n",
        "    answers = []\n",
        "    guids = []\n",
        "\n",
        "    for group in tqdm(kdict['data']):\n",
        "        for passage in group['paragraphs']:\n",
        "            context = passage['context']\n",
        "            for qa in passage['qas']:\n",
        "                question = qa['question']\n",
        "                guid = qa['guid']\n",
        "                #temp_answer = []\n",
        "                #for answer in qa['answers']:\n",
        "                    #temp_answer.append(answer['text'])\n",
        "                #if len(temp_answer) != 0: # answers의 길이가 0 == 답변할 수 없는 질문\n",
        "                    #contexts.append(context)\n",
        "                    #questions.append(question)\n",
        "                    #answers.append(temp_answer)\n",
        "                contexts.append(context)##\n",
        "                questions.append(question)##\n",
        "                guids.append(guid)\n",
        "\n",
        "    #return contexts, questions, answers\n",
        "    return contexts, questions , guids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OAg0_WkAPLjw"
      },
      "outputs": [],
      "source": [
        "#dev_contexts, dev_questions, dev_answers = read_dev(\"test.json\")\n",
        "dev_contexts, dev_questions, dev_guids = read_dev(\"test.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qS8G0_kBhMhW"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "def remove_post(text):\n",
        "        ''' 불필요한 기호 제거 '''\n",
        "        text = text.strip()\n",
        "        text = re.sub(\"'\", \"\", text)\n",
        "        text = re.sub('\"', \"\", text)\n",
        "        text = re.sub('《', \"\", text)\n",
        "        text = re.sub('》', \"\", text)\n",
        "        text = re.sub('<', \"\", text)\n",
        "        text = re.sub('>', \"\", text)\n",
        "        text = re.sub('〈', \"\", text)\n",
        "        text = re.sub('〉', \"\", text)\n",
        "        text = re.sub(\"\\(\", \"\", text)\n",
        "        text = re.sub(\"\\)\", \"\", text)\n",
        "        text = re.sub(\"‘\", \"\", text)\n",
        "        text = re.sub(\"’\", \"\", text)\n",
        "        text = re.sub(\"  \", \" \", text)\n",
        "        text = re.sub(\"#\", \"\", text)\n",
        "        return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4HmQRZQp1NKT"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "idea  \n",
        "틀린 답을 바로 빈칸으로 내보내는게 과연 이득일까?\n",
        "logit 확률값을 적용해서 틀린답이면 다음으로 높은 확률값이 정답일수도 있지않을까?\n",
        "top 5의 확률값을 다 적용해도 틀린 답이라면 그때 빈칸을 내보내도 괜찮을듯.\n",
        "'''\n",
        "# start, end logit의 확률값을 이용한 예측 정답값\n",
        "# logit의 상위 5개 확률을 리스트로 뽑아 틀린 정답이었다면 다음 확률로 넘어가서 확인.\n",
        "\n",
        "def to_list(tensor):\n",
        "    return tensor.detach().cpu().tolist()\n",
        "\n",
        "def logits_change(input_ids, STA_logits, END_logits):\n",
        "\n",
        "    # 로짓의 확률값 ~ 상위 5개를 선택 \n",
        "    # 틀린 추론이었다면 다음 선택 (틀린 추론 : start > end, 길이가 너무 긴 문장.)\n",
        "    change_logit = 0\n",
        "    cnt = 0\n",
        "    \n",
        "    # 기존 정답\n",
        "    save_s = STA_logits\n",
        "    save_e = END_logits\n",
        "\n",
        "    STK_start_index, STK_end_index = save_s.argmax(dim=-1), save_e.argmax(dim=-1)\n",
        "    save_pred_ids = tokenizer.decode(input_ids[0][STK_start_index: STK_end_index + 1])\n",
        "    #print(save_pred_ids) \n",
        "\n",
        "    # 바뀐 정답\n",
        "    # Tensor형태의 logit의 확률값을 리스트로 만들어줌 \n",
        "    STA_logits = to_list(STA_logits)[0]\n",
        "    END_logits = to_list(END_logits)[0]\n",
        "\n",
        "    # 리스트로 만든 확률값 -> 큰 순서대로 정렬 + 인덱스 \n",
        "    start_idx_and_logit = sorted(enumerate(STA_logits), key=lambda x: x[1], reverse=True)\n",
        "    end_idx_and_logit = sorted(enumerate(END_logits), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    # 확률값 큰 순서대로 Top 5 \n",
        "    start_idx_and_logit = start_idx_and_logit[:5]\n",
        "    end_idx_and_logit = end_idx_and_logit[:5]\n",
        "\n",
        "    TK_start_index, TK_end_index = start_idx_and_logit, end_idx_and_logit\n",
        "\n",
        "    # 확률이 높은 순서대로 점검. 틀린 값이면 다음 확률로 넘어가고 맞는 답이면 저장후 반복문 종료\n",
        "    for i in range(5):\n",
        "        if TK_start_index[i][0] > TK_end_index[i][0] or TK_end_index[i][0] - TK_start_index[i][0] > 10 : \n",
        "            cnt += 1\n",
        "            continue\n",
        "        else : \n",
        "            change_logit += 1\n",
        "            pred_ids = input_ids[0][TK_start_index[i][0]: TK_end_index[i][0] + 1]\n",
        "            pred_ids = tokenizer.decode(pred_ids)\n",
        "            #print(pred_ids, 'change')\n",
        "            break\n",
        "\n",
        "    if change_logit == 0 :\n",
        "        return save_pred_ids\n",
        "    elif cnt == 5:\n",
        "        return ''\n",
        "    else : \n",
        "        if pred_ids == save_pred_ids:\n",
        "            #print('same answer')\n",
        "            return pred_ids\n",
        "        else :\n",
        "            #print('different answer')\n",
        "            return pred_ids\n",
        "\n",
        "    \n",
        "\n",
        "# start_logits , end_logits\n",
        "# index를 추적하면서 시작, 종료 index에 대한 확률이 가장 높은것을 선택하는 방법.\n",
        "# 만약에 차이가 큰 start, end 값을 반환할때 이 정보들을 저장하지 않고 넘긴다면? \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rHaddLdiPf5z"
      },
      "outputs": [],
      "source": [
        "def prediction(contexts, questions, guids):\n",
        "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "    model.load_state_dict(torch.load('./output_model_best'))\n",
        "    model.to(device)\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    result = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        \n",
        "        for context, question, guid in zip(contexts, questions, guids):\n",
        "            encodings = tokenizer(context, question, max_length=512, truncation=True,\n",
        "                                     padding=\"max_length\", return_token_type_ids=False)\n",
        "            encodings = {key: torch.tensor([val]) for key, val in encodings.items()}\n",
        "            \n",
        "            input_ids = encodings[\"input_ids\"].to(device)\n",
        "            attention_mask = encodings[\"attention_mask\"].to(device)\n",
        "            \n",
        "            outputs = model(input_ids, attention_mask=attention_mask)\n",
        "            start_logits, end_logits = outputs.start_logits, outputs.end_logits\n",
        "\n",
        "            ######\n",
        "            pred =logits_change(input_ids, start_logits, end_logits)\n",
        "            '''\n",
        "            token_start_index, token_end_index = start_logits.argmax(dim=-1), end_logits.argmax(dim=-1)\n",
        "            pred_ids = input_ids[0][token_start_index: token_end_index + 1]\n",
        "            \n",
        "            if token_start_index > token_end_index:\n",
        "                pred = ''\n",
        "            else:\n",
        "                pred = tokenizer.decode(pred_ids)\n",
        "                pred = pred[:10] # 1.5~1.8\n",
        "            '''\n",
        "            \n",
        "\n",
        "\n",
        "            pred = pred[:8] # 6~12 테스트 결과 8이 가장 좋은 점수.\n",
        "            pred = remove_post(pred)\n",
        "\n",
        "            tp = (guid,pred)\n",
        "            \n",
        "            result.append(tp)\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L20_dlBvPf3p"
      },
      "outputs": [],
      "source": [
        "pred_answers = prediction(dev_contexts, dev_questions, dev_guids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-FdYepZSPf1i"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "f = open('sangrimlee_bert-base-multilingual-cased-korquad_validation_pred_truncation.csv','w', newline='')\n",
        "wr = csv.writer(f)\n",
        "wr.writerow(['Id','Predicted'])\n",
        "\n",
        "for tp in pred_answers:\n",
        "    wr.writerow([tp[0],tp[1]])\n",
        "\n",
        "f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SXMI6CaEmKlM"
      },
      "outputs": [],
      "source": [
        "# pred_answers에서 도출한 정답 비교\n",
        "import csv\n",
        "\n",
        "def calculate_Leven(source, ref, result_file):\n",
        "    with open(source, 'r') as input1:\n",
        "        with open(ref, 'r') as input2:\n",
        "            with open(result_file, 'w') as csvoutput:\n",
        "                reader1 = csv.reader(input1)\n",
        "                reader2 = list(csv.reader(input2))\n",
        "                writer = csv.writer(csvoutput)\n",
        "                result = []\n",
        "                mean = []\n",
        "                headers = next(reader1)\n",
        "                result.append(headers)\n",
        "                index = 0\n",
        "                for row1 in reader1:\n",
        "                    #print(\"First row\")\n",
        "                    #print(row1[1])\n",
        "                    index+=1\n",
        "                    #print(reader2[index][1])\n",
        "                    a = levenshtein_distance(row1[1], reader2[index][1])\n",
        "                    print(row1[1],'////',reader2[index][1])\n",
        "                    '''\n",
        "                    max = 0\n",
        "                    while max < 1:\n",
        "                        for row2 in reader2:\n",
        "                            a = distance(row1[1],row2[1])\n",
        "                            print(a)\n",
        "                            b = 1 - a/len(row1[1])\n",
        "                            if b > max:\n",
        "                                max = b\n",
        "                                SKU = row2[1]\n",
        "                    '''\n",
        "                    mean.append(a)\n",
        "                    row1.append(a)\n",
        "                    result.append(row1)\n",
        "                mean1 = sum(mean) / len(mean)\n",
        "                print(mean1)\n",
        "                writer.writerows(result)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nKbGAvSZmRyu"
      },
      "outputs": [],
      "source": [
        "calculate_Leven('klue.csv', 'sangrimlee_bert-base-multilingual-cased-korquad_validation_pred_truncation.csv', 'result_file.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ihJBQMJDqFXv"
      },
      "outputs": [],
      "source": [
        "# logit 값에 대한 soft voting - 3 model\n",
        "\n",
        "def to_list(tensor):\n",
        "    return tensor.detach().cpu().tolist()\n",
        "\n",
        "def logits_voting(A_model, B_model, C_model):\n",
        "    # 각 모델마다 input_ids, start_logits, end_logits값 저장. 순서대로 저장 리스트에 + guids // context 필요.\n",
        "    # 모델 logit 확률 top 5 구해서 모델 1 top1 모델 2 top1 모델3 top1 모델1 top2 순으로 체크.\n",
        "\n",
        "    ind = 0\n",
        "    RESULT = []\n",
        "\n",
        "    if len(A_model)!= len(B_model) or len(A_model) != len(C_model) or len(B_model) != len(C_model):\n",
        "        print('error')\n",
        "    else:\n",
        "        llogit = len(A_model)\n",
        "\n",
        "    while 1:\n",
        "        # input_ids\n",
        "        A_input_ids = A_model[ind][0]\n",
        "        B_input_ids = B_model[ind][0]\n",
        "        C_input_ids = C_model[ind][0]\n",
        "\n",
        "        # guid\n",
        "        A_guid = A_model[ind][3]\n",
        "        B_guid = A_model[ind][3]\n",
        "        C_guid = A_model[ind][3]\n",
        "\n",
        "        if A_input_ids != B_input_ids or B_input_ids != C_input_ids or A_input_ids != C_input_ids:\n",
        "            print('input_error')\n",
        "        if A_guid != B_guid or B_guid != C_guid or A_guid != C_guid:\n",
        "            print('guid_error')\n",
        "\n",
        "        # logit 값 순서대로 불러오기기\n",
        "        A_start_logit = A_model[ind][1]\n",
        "        A_end_logit = A_model[ind][2]\n",
        "        B_start_logit = B_model[ind][1]\n",
        "        B_end_logit = B_model[ind][2]\n",
        "        C_start_logit = C_model[ind][1]\n",
        "        C_end_logit = C_model[ind][2]\n",
        "\n",
        "        # 로짓값 -> 확률 큰 순서대로 리스트형태 + 인덱스 -> top 5 자르기기\n",
        "        A_start_idx_and_logit = (sorted(enumerate(A_start_logit), key=lambda x: x[1], reverse=True))[:5]\n",
        "        A_end_idx_and_logit = (sorted(enumerate(A_end_logit), key=lambda x: x[1], reverse=True))[:5]\n",
        "        B_start_idx_and_logit = (sorted(enumerate(B_start_logit), key=lambda x: x[1], reverse=True))[:5]\n",
        "        B_end_idx_and_logit = (sorted(enumerate(B_end_logit), key=lambda x: x[1], reverse=True))[:5]\n",
        "        C_start_idx_and_logit = (sorted(enumerate(C_start_logit), key=lambda x: x[1], reverse=True))[:5]\n",
        "        C_end_idx_and_logit = (sorted(enumerate(C_end_logit), key=lambda x: x[1], reverse=True))[:5]\n",
        "\n",
        "\n",
        "        cnt = 0\n",
        "        for i in range(5):\n",
        "            # 모델 A, B, C 에 대한 각각의 로짓값 저장해서 확률 5 리스트로 저장 후 비교.\n",
        "            # A->B->C->A 순? A2 > B1 ? 어떤 순서대로 비교할건지 \n",
        "            # 정답인 경우 멈추고 값 저장 후 종료\n",
        "            # 토크나이저 다 다른데 어떻게 비교함? -> 정답 context \n",
        "            # soft voting -> start + end 확률값 더해서 순위대로 정렬 후 정답인지 확인 -> 정답일때는 해당 모델에 대한 input 불러와서 answer, guid 저장.\n",
        "\n",
        "            if A_start_idx_and_logit[i][0] < A_end_idx_and_logit[i][0] or A_end_idx_and_logit[i][0] - A_start_idx_and_logit[i][0] <= 10 : \n",
        "                cnt += 1\n",
        "                pred_ids = A_input_ids[0][A_start_idx_and_logit[i][0]: A_end_idx_and_logit[i][0] + 1] # 해당 정답에 대한 인덱스 ~> context 변환 하는 과정 필요.\n",
        "                \n",
        "                guid = A_guid[i]\n",
        "                break\n",
        "\n",
        "            elif B_start_idx_and_logit[i][0] < B_end_idx_and_logit[i][0] or B_end_idx_and_logit[i][0] - B_start_idx_and_logit[i][0] <= 10 : \n",
        "                cnt += 1\n",
        "                pred_ids = B_input_ids[0][B_start_idx_and_logit[i][0]: B_end_idx_and_logit[i][0] + 1]\n",
        "\n",
        "                guid = B_guid[i]\n",
        "                break\n",
        "\n",
        "            elif C_start_idx_and_logit[i][0] < C_end_idx_and_logit[i][0] or C_end_idx_and_logit[i][0] - C_start_idx_and_logit[i][0] <= 10 : \n",
        "                cnt += 1\n",
        "                pred_ids = C_input_ids[0][C_start_idx_and_logit[i][0]: C_end_idx_and_logit[i][0] + 1]\n",
        "\n",
        "                guid = C_guid[i]\n",
        "                break\n",
        "\n",
        "\n",
        "                 \n",
        "\n",
        "        # guids??? 저장해야함.\n",
        "        if cnt == 0:\n",
        "           tp = (guid,'')\n",
        "           RESULT.append(tp)\n",
        "        else:\n",
        "           tp = (guid,pred_ids)\n",
        "           RESULT.append(tp)\n",
        "\n",
        "        if llogit == ids:\n",
        "            break\n",
        "        ids += 1\n",
        "\n",
        "\n",
        "            # 1 23 맞는 정답만 도출.\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "torch",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13 (main, Oct 13 2022, 21:23:06) [MSC v.1916 64 bit (AMD64)]"
    },
    "vscode": {
      "interpreter": {
        "hash": "c044c91b12eb47f047c895d462d020aa8e4307e7bfafc2c2e5cdad580c7ef67a"
      }
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0c08c10ffa1d4c669a4a0a889b631bca": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8d01d9168e21415595b017e4c3f6f72e",
            "placeholder": "​",
            "style": "IPY_MODEL_2438c5eff890449186aec9fac257e0d0",
            "value": " 978/978 [00:00&lt;00:00, 47409.70it/s]"
          }
        },
        "1411439e6514499e840f813b434f4029": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d4a45098fe194121babc81417b3379bb",
            "placeholder": "​",
            "style": "IPY_MODEL_777a74fcc38044ccab25b7813d5a5eb1",
            "value": " 10%"
          }
        },
        "15f39a9837d84f51b4d47fab6c2b3dec": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a37677e6b0ad47dc9b98e8201e7a5b35",
            "placeholder": "​",
            "style": "IPY_MODEL_acb8d937aed347af8947b4c7abea5a1a",
            "value": "100%"
          }
        },
        "1f1ab3e347b24a5e90fa136432e021ba": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2438c5eff890449186aec9fac257e0d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2dc2988f9a6046cb928f7cbabe915f67": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d40ffd7f50dc4db09d4d56ad5b3652f7",
            "max": 1986,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2eb9f9cc6bb548fa94281ec27e733f64",
            "value": 189
          }
        },
        "2eb9f9cc6bb548fa94281ec27e733f64": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3bada5a50a054b768105b1ff8f8861da": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4494a918ad88472589c09a8480f58410": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e8e9d7c40aea4ff9983955965daff451",
            "max": 8811,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_db4ad55a99e8448f8c7be331bf2c3f98",
            "value": 8811
          }
        },
        "50c7366cd7f84b4d9d271a0d6302109e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "742652c90036434b914a0f2f3fd07599": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7637d6159ae74c37af946abf8d2d7668": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "777a74fcc38044ccab25b7813d5a5eb1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7a55a2e5f684464e94afe6f75b2b4b82": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e8b11f3e36ad448f93a71bff29340c27",
            "placeholder": "​",
            "style": "IPY_MODEL_3bada5a50a054b768105b1ff8f8861da",
            "value": "100%"
          }
        },
        "7e81f9a147594f58917fec26151893fb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8d01d9168e21415595b017e4c3f6f72e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8dcf67b7c9d6412bb56d6ee9856e7931": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9415fc87515f4ecf8216f33ce30f1f44": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1411439e6514499e840f813b434f4029",
              "IPY_MODEL_2dc2988f9a6046cb928f7cbabe915f67",
              "IPY_MODEL_c8dcc8eeb4674acd822759f66e7bac7e"
            ],
            "layout": "IPY_MODEL_9ec4e0e4d05b4088a6153a806f60a8dd"
          }
        },
        "97a2baba34184dd5b9c97a82083c4a7d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7637d6159ae74c37af946abf8d2d7668",
            "max": 978,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_742652c90036434b914a0f2f3fd07599",
            "value": 978
          }
        },
        "9ec4e0e4d05b4088a6153a806f60a8dd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a37677e6b0ad47dc9b98e8201e7a5b35": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "acb8d937aed347af8947b4c7abea5a1a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b77193eb20bb424b898bc38d3000da16": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_15f39a9837d84f51b4d47fab6c2b3dec",
              "IPY_MODEL_97a2baba34184dd5b9c97a82083c4a7d",
              "IPY_MODEL_0c08c10ffa1d4c669a4a0a889b631bca"
            ],
            "layout": "IPY_MODEL_7e81f9a147594f58917fec26151893fb"
          }
        },
        "c8dcc8eeb4674acd822759f66e7bac7e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8dcf67b7c9d6412bb56d6ee9856e7931",
            "placeholder": "​",
            "style": "IPY_MODEL_d11a90be8f1244de9a50471667080617",
            "value": " 189/1986 [02:32&lt;24:36,  1.22step/s, batch_loss=34.388680, loss=4.812579]"
          }
        },
        "c9ec77da4b8044aeab7345e25b850099": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_50c7366cd7f84b4d9d271a0d6302109e",
            "placeholder": "​",
            "style": "IPY_MODEL_d79f3327d9504f9388a522079e70316e",
            "value": " 8811/8811 [00:00&lt;00:00, 156970.40it/s]"
          }
        },
        "d11a90be8f1244de9a50471667080617": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d40ffd7f50dc4db09d4d56ad5b3652f7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d4a45098fe194121babc81417b3379bb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d79f3327d9504f9388a522079e70316e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "db4ad55a99e8448f8c7be331bf2c3f98": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "dd81ae5b8a204a1fb452b81b532284ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7a55a2e5f684464e94afe6f75b2b4b82",
              "IPY_MODEL_4494a918ad88472589c09a8480f58410",
              "IPY_MODEL_c9ec77da4b8044aeab7345e25b850099"
            ],
            "layout": "IPY_MODEL_1f1ab3e347b24a5e90fa136432e021ba"
          }
        },
        "e8b11f3e36ad448f93a71bff29340c27": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e8e9d7c40aea4ff9983955965daff451": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}