{"cells":[{"cell_type":"markdown","metadata":{"id":"le5gUu-qOx95"},"source":["## Kaggle\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":315},"id":"LAL888NDOj2V","outputId":"b21d31c4-ed59-47bb-bc58-b674aee6bf17"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: kaggle in /usr/local/lib/python3.8/dist-packages (1.5.12)\n","Requirement already satisfied: python-dateutil in /usr/local/lib/python3.8/dist-packages (from kaggle) (2.8.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from kaggle) (2.25.1)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from kaggle) (4.64.1)\n","Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.8/dist-packages (from kaggle) (1.15.0)\n","Requirement already satisfied: python-slugify in /usr/local/lib/python3.8/dist-packages (from kaggle) (7.0.0)\n","Requirement already satisfied: urllib3 in /usr/local/lib/python3.8/dist-packages (from kaggle) (1.24.3)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.8/dist-packages (from kaggle) (2022.12.7)\n","Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.8/dist-packages (from python-slugify->kaggle) (1.3)\n","Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->kaggle) (4.0.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->kaggle) (2.10)\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","     <input type=\"file\" id=\"files-3461e97b-6f0c-4b1e-9075-6b4f66007806\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-3461e97b-6f0c-4b1e-9075-6b4f66007806\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script>// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) => {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable<!Object>} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) => {\n","    inputElement.addEventListener('change', (e) => {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) => {\n","    cancel.onclick = () => {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) => {\n","      const reader = new FileReader();\n","      reader.onload = (e) => {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position < fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","</script> "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saving kaggle.json to kaggle.json\n"]},{"output_type":"execute_result","data":{"text/plain":["{'kaggle.json': b'{\"username\":\"jeonhyotaek\",\"key\":\"d2ddaf4c586e0bf63051e1a4c6a74dd4\"}'}"]},"metadata":{},"execution_count":1}],"source":["!pip install kaggle\n","from google.colab import files\n","files.upload()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NNoJ4AkSOuVI","outputId":"b9ee5b9f-e898-4e8d-cdfc-9c4c43f5fac3"},"outputs":[{"output_type":"stream","name":"stdout","text":["kaggle.json\n"]}],"source":["ls -1ha kaggle.json"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qU0E6zxLOuTa"},"outputs":[],"source":["!mkdir -p ~/.kaggle\n","!cp kaggle.json ~/.kaggle/"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"F4LYW2I4OuRD","outputId":"76ebb154-2c0c-4a52-b68a-95264aaf605f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /root/.kaggle/kaggle.json'\n","Downloading 6th-goorm-project-2-korean-mrc.zip to /content\n"," 85% 13.0M/15.3M [00:01<00:00, 15.3MB/s]\n","100% 15.3M/15.3M [00:01<00:00, 9.49MB/s]\n"]}],"source":["!kaggle competitions download -c 6th-goorm-project-2-korean-mrc"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_qDBFgn5OuOx","outputId":"0fa37386-2eab-4ea4-eb5d-7a0221d8b76b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Archive:  6th-goorm-project-2-korean-mrc.zip\n","  inflating: baseline.csv            \n","  inflating: blank.csv               \n","  inflating: test.json               \n","  inflating: train.json              \n"]}],"source":["!unzip 6th-goorm-project-2-korean-mrc.zip"]},{"cell_type":"markdown","metadata":{"id":"zkiNxKnFO05Z"},"source":["## data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T4euzcqrOuMh","colab":{"base_uri":"https://localhost:8080/"},"outputId":"e305c03c-9beb-4947-cca5-7d88f0fcc098"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: transformers in /usr/local/lib/python3.8/dist-packages (4.25.1)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.13.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.9.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.25.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.11.1)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.26.13)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n","Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (4.0.0)\n"]}],"source":["!pip install transformers"]},{"cell_type":"markdown","metadata":{"id":"I5JxfHVARuja"},"source":["## gogo"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_yskKCUsOuIw"},"outputs":[],"source":["tokenizer_name = \"monologg/koelectra-small-v2-distilled-korquad-384\"\n","model_name = \"monologg/koelectra-small-v2-distilled-korquad-384\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DQr5Ivl7OuGZ"},"outputs":[],"source":["import json\n","import random\n","\n","import torch\n","import numpy as np\n","from tqdm.notebook import tqdm\n","from torch.utils.data import Dataset, DataLoader\n","from transformers import AutoTokenizer, AutoModelForQuestionAnswering, AdamW"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IkWsSNujOuER","outputId":"262158c7-1668-4963-f166-4b7800b08bc7"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'title': '제주도 장마 시작 … 중부는 이달 말부터',\n"," 'paragraphs': [{'context': '올여름 장마가 17일 제주도에서 시작됐다. 서울 등 중부지방은 예년보다 사나흘 정도 늦은 이달 말께 장마가 시작될 전망이다.17일 기상청에 따르면 제주도 남쪽 먼바다에 있는 장마전선의 영향으로 이날 제주도 산간 및 내륙지역에 호우주의보가 내려지면서 곳곳에 100㎜에 육박하는 많은 비가 내렸다. 제주의 장마는 평년보다 2~3일, 지난해보다는 하루 일찍 시작됐다. 장마는 고온다습한 북태평양 기단과 한랭 습윤한 오호츠크해 기단이 만나 형성되는 장마전선에서 내리는 비를 뜻한다.장마전선은 18일 제주도 먼 남쪽 해상으로 내려갔다가 20일께 다시 북상해 전남 남해안까지 영향을 줄 것으로 보인다. 이에 따라 20~21일 남부지방에도 예년보다 사흘 정도 장마가 일찍 찾아올 전망이다. 그러나 장마전선을 밀어올리는 북태평양 고기압 세력이 약해 서울 등 중부지방은 평년보다 사나흘가량 늦은 이달 말부터 장마가 시작될 것이라는 게 기상청의 설명이다. 장마전선은 이후 한 달가량 한반도 중남부를 오르내리며 곳곳에 비를 뿌릴 전망이다. 최근 30년간 평균치에 따르면 중부지방의 장마 시작일은 6월24~25일이었으며 장마기간은 32일, 강수일수는 17.2일이었다.기상청은 올해 장마기간의 평균 강수량이 350~400㎜로 평년과 비슷하거나 적을 것으로 내다봤다. 브라질 월드컵 한국과 러시아의 경기가 열리는 18일 오전 서울은 대체로 구름이 많이 끼지만 비는 오지 않을 것으로 예상돼 거리 응원에는 지장이 없을 전망이다.',\n","   'qas': [{'question': '북태평양 기단과 오호츠크해 기단이 만나 국내에 머무르는 기간은?',\n","     'answers': [{'text': '한 달가량', 'answer_start': 478},\n","      {'text': '한 달', 'answer_start': 478}],\n","     'guid': '798db07f0b9046759deed9d4a35ce31e'}]}],\n"," 'news_category': '종합',\n"," 'source': 'hankyung'}"]},"metadata":{},"execution_count":4}],"source":["with open(\"train.json\", 'rb') as f:\n","    input_dict = json.load(f)\n","input_dict[\"data\"][0]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0DxIyRfTOuCI","outputId":"3d434239-c761-4b65-8d02-1f22253a91b2"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'title': '부산정보산업진흥원, 과기부 지역SW서비스사업화 지원사업 4개 과제 선정',\n"," 'paragraphs': [{'context': '부산시와 (재)부산정보산업진흥원(원장 이인숙)이 ‘2020~2021년 지역SW서비스사업화 지원사업’ 공모사업에 4개 과제가 선정되어 본격적인 사업 착수에 나선다. 과학기술정보통신부가 주관하는 ‘지역SW서비스사업화 지원사업’은 강소SW기업 및 초기 스타트업의 SW서비스 사업화 지원과 신시장 진출 지원을 통해 기업 경쟁력 강화와 지역경제 활성화를 도모하는 사업이다. 올해부터 2개년으로 진행되며, 국비와 시비, 민자 등 2년간 약 37억원의 예산이 투입된다. 앞서 진흥원은 부산의 미래 먹거리산업인 스마트해양, 지능형기계, 지능정보서비스 분야로 사전 수요조사를 진행했고, 평가를 통해 선정된 5개 과제를 공모사업에 신청했다. 그 결과 부산의 4개 과제가 최종 선정되는 쾌거를 거뒀다. 당 사업은 전국 진흥기관을 대상으로 공모를 시작해, 총 17개 지역에서 42개 과제가 선정되었으며, 4개 과제가 선정된 곳은 부산과 강원지역 뿐이다. 금번 선정된 과제들은 ‘인공지능융합센서와 서보 이송 로봇을 이용한 전단보강재의 자동용접시스템 개발’ 등 총 4개 과제다. 부산시가 지원하고, 부산정보산업진흥원과 지역기업, 대학, 연구소 등이 컨소시엄을 구성하여 기술개발 및 사업화 지원을 추진한다. 2개의 Track으로 구분되는 이번사업은 Track 1(SW중소기업)에서 ㈜에이아이플랫폼, 엔컴(주), Track 2(스타트업)에서는 ㈜토즈, 삼보테크놀로지를 지원한다. ○ ‘Track 1‘의 (주)에이아이플랫폼이 주관기업으로 진행하는 <인공지능 기반 망막 내 아밀로이드 플라크 영상 분석을 통한 치매조기진단 플랫폼 상용화>는 치매 확진의 원인이 되는 중요 단백질(아밀로이드 플라크)을 자체개발 관측장비로 진단한다. 이를 통해 치매를 조기 발견하여, 각종 경제적 비용과 치료 및 예방 등 사회적 문제를 해 결하고 시민들이 쉽게 접근 가능한 실효성 있는 치매관리체계 개발을 목표로 한다. ○ 엔컴(주)이 주관기업으로 참여하는 <AI영상분석 기반 가공철근 생산성 향상 시스템 기술개발 및 사업화>는 산업안전, 환경규제, 생산체계의 변화로 침체된 부산 핵심 산업인 철강업 활성화에 나선다. 실시간으로 절곡되어 나오는 가공철근의 형상을 인식하고 불량 형상 판단 시 적합한 교정 값을 절곡설비에 전달함으로써, 무중단 생산이 가능한 영상분석 기술과 생산설비 자동화 제어기술을 개발한다. ○ ‘Track 2’의 ㈜토즈는 자립기반이 약한 국내 중소형 조선소의 산업기술 변화에 혁신적인 대응을 위해 <가상현실 기반 원격 다자간 선박 및 해양구조물 사전 검사 시스템>을 개발한다. 선박 건조 前, 설계 단계에서 설계자 뿐만 아니라 생산관리자, 품질관리자, 선급검사관, 선주감독관 등의 이해관계자가 공동으로 가상의 환경에서 선박 및 해양구조물의 자재 배치와 간섭, 작업성, 설계 오작 등에 대한 검사를 진행할 수 있는 기술을 확보하여 조선소의 업무효율을 극대화 할 예정이다. ○ 삼보테크놀로지는 재래식 건설 부자재의 시공성, 안전성, 내구성 등의 문제점을 보완하여 시민 안전과 건설근로자의 환경개선, 생산성 및 수익성 향상을 위해 <인공지능융합센서와 새들형 토치 서보 이송 로봇을 이용한 고속 SRD 전단보강재 자동용접시스템>을 개발한다. 로봇응용 SRD 용접자동화 설비를 제작하고, 용접 모니터링 및 품질검사 소프트웨어를 개발하여 건설분야에 4차산업 대비 지능형 생산자동화 기반기술을 확보할 예정이다. (재)부산정보산업진흥원 이인숙 원장은 “이번 코로나19 사태로 인해 부산 기업들이 매출과 고용유지, 자재수급 등에 큰 타격을 입었지만, 지역SW서비스사업화 지원사업을 통해 지역과 기업차원에서 기반을 다지는 계기가 됐으면 좋겠다‘며 ”진흥원은 어려운 사태를 대비해 지역 기업들을 지원할 수 있는 다른 방편을 계속 모색 중이며, 더욱 성장해 나갈 수 있도록 적극 지원하겠다“고 전했다.',\n","   'qas': [{'question': '지능형 생산자동화 기반기술을 개발중인 스타트업은?',\n","     'answers': [{'text': '삼보테크놀로지', 'answer_start': 1422}],\n","     'guid': '67c85e4f86ae43939b807684537c909c'}]}],\n"," 'news_category': '경제',\n"," 'source': 'acrofan'}"]},"metadata":{},"execution_count":5}],"source":["input_dict[\"data\"][1]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2EvB1moGOt55"},"outputs":[],"source":["from copy import deepcopy\n","def split_input_dict(input_dict, ratio = 0.1, seed = 42):\n","    split_point = int(len(input_dict['data']) * ratio)\n","    random.seed(seed)\n","    random.shuffle(input_dict['data'])\n","    valid_dict = deepcopy(input_dict)\n","    train_dict = input_dict\n","\n","    valid_dict['data'] = input_dict['data'][:split_point]\n","    train_dict['data'] = input_dict['data'][split_point:]\n","    return train_dict, valid_dict"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DHJppuYaO9Nl"},"outputs":[],"source":["def read_input(path):\n","    with open(path, 'rb') as f:\n","        input_dict = json.load(f)\n","    train_dict,valid_dict =split_input_dict(input_dict)\n","    train_contexts = []\n","    train_questions = []\n","    train_answers = []\n","    for group in tqdm(train_dict['data']):       #딕셔너리 하나씩 꺼낸다.\n","        for passage in group['paragraphs']:     #딕셔너리의 paragraphs\n","            context = passage['context']        #paragraphs의 context\n","            for qa in passage['qas']:           #paragraphs의 qas\n","                question = qa['question']       #paragraphs의 question\n","                for answer in qa['answers']:    #question의 answers\n","                    ### context 넘겨서 자르기 ###\n","                    target_context = context\n","                    if answer['answer_start'] > 1400:\n","                        target_context = target_context[1200:]\n","                        answer['answer_start'] -= 1200 \n","                    elif answer['answer_start'] > 1200:\n","                        target_context = target_context[1000:]\n","                        answer['answer_start'] -= 1000\n","                    elif answer['answer_start'] > 1000:\n","                        target_context = target_context[800:]\n","                        answer['answer_start'] -= 800\n","                    elif answer['answer_start'] > 800:\n","                        target_context = target_context[600:]\n","                        answer['answer_start'] -= 600\n","                    \n","                    \n","                    train_contexts.append(target_context)    #answers의 한 answer당 해당하는 context 저장\n","                    ### context 넘겨서 자르기 ###\n","                    train_questions.append(question)  #answers의 한 answer당 해당하는 question 저장\n","                    train_answers.append(answer)      #answers의 한 answer 저장\n","  \n","    valid_contexts = []\n","    valid_questions = []\n","    valid_answers = []\n","    for group in tqdm(valid_dict['data']):       #딕셔너리 하나씩 꺼낸다.\n","        for passage in group['paragraphs']:     #딕셔너리의 paragraphs\n","            context = passage['context']        #paragraphs의 context\n","            for qa in passage['qas']:           #paragraphs의 qas\n","                question = qa['question']       #paragraphs의 question\n","                for answer in qa['answers']:    #question의 answers\n","                    ### context 넘겨서 자르기 ###\n","                    target_context = context\n","                    if answer['answer_start'] > 1400:\n","                        target_context = target_context[1200:]\n","                        answer['answer_start'] -= 1200 \n","                    elif answer['answer_start'] > 1200:\n","                        target_context = target_context[1000:]\n","                        answer['answer_start'] -= 1000\n","                    elif answer['answer_start'] > 1000:\n","                        target_context = target_context[800:]\n","                        answer['answer_start'] -= 800\n","                    elif answer['answer_start'] > 800:\n","                        target_context = target_context[600:]\n","                        answer['answer_start'] -= 600\n","\n","                    \n","                    valid_contexts.append(target_context)    #answers의 한 answer당 해당하는 context 저장\n","                    ### context 넘겨서 자르기 ###\n","                    valid_questions.append(question)  #answers의 한 answer당 해당하는 question 저장\n","                    valid_answers.append(answer)      #answers의 한 answer 저장\n","\n","    return train_contexts, train_questions, train_answers, valid_contexts, valid_questions, valid_answers"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vKGCz_4WO9Lg"},"outputs":[],"source":["def add_end_idx(answers, contexts):\n","    for answer, context in zip(answers, contexts):\n","        gold_text = answer['text']\n","        start_idx = answer['answer_start']\n","        end_idx = start_idx + len(gold_text)\n","\n","        if context[start_idx:end_idx] == gold_text:\n","            answer['answer_end'] = end_idx\n","        elif context[start_idx-1:end_idx-1] == gold_text:\n","            print(\"there is an unitended error in dataset\") #이렇게까지 할 필요가 있나?\n","            answer['answer_start'] = start_idx - 1\n","            answer['answer_end'] = end_idx - 1\n","        elif context[start_idx-2:end_idx-2] == gold_text:\n","            print(\"there is an unitended error in dataset\")\n","            answer['answer_start'] = start_idx - 2\n","            answer['answer_end'] = end_idx - 2"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1kXdeER8O9Jp"},"outputs":[],"source":["tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Hz1kYCgCO9Hx","outputId":"e6b12493-412e-4fb7-ed08-813b589769fb"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'input_ids': [2, 156, 18876, 8381, 29956, 550, 29982, 2757, 29951, 29962, 392, 30201, 29948, 5, 188, 55, 4473, 4660, 29961, 8674, 29997, 29948, 23326, 30672, 455, 1821, 29961, 6, 30124, 103, 30277, 8381, 29956, 392, 30292, 627, 29947, 29948, 5, 550, 29982, 5337, 29951, 541, 30013, 2757, 4613, 23905, 29951, 24, 29950, 8381, 9972, 29953, 814, 9563, 17196, 2757, 12364, 298, 7418, 16239, 29951, 10954, 21036, 29956, 1358, 29954, 13956, 3196, 29951, 1, 7532, 29959, 29950, 204, 29961, 97, 29956, 2708, 29948, 5, 881, 29953, 8381, 29950, 10062, 29997, 29948, 28, 260, 57, 29982, 18, 136, 29973, 29997, 1312, 989, 5046, 392, 30201, 29948, 5, 8381, 29950, 15024, 29948, 30207, 29957, 299, 10299, 27674, 29990, 14, 30833, 278, 30425, 29957, 84, 30128, 27805, 29973, 27674, 29947, 1028, 2158, 30078, 29950, 8381, 9972, 29951, 29962, 2185, 29950, 97, 29965, 913, 1086, 5, 8381, 9972, 29961, 462, 29982, 2757, 943, 4613, 3306, 9563, 11560, 2883, 90, 29982, 30277, 548, 18521, 29973, 1583, 14726, 30108, 29954, 814, 29952, 397, 45, 9563, 835, 5, 26701, 347, 90, 260, 610, 29982, 3378, 4660, 29951, 29967, 8674, 29997, 29948, 5752, 455, 8381, 29956, 5046, 26845, 627, 29947, 29948, 5, 416, 8381, 9972, 29952, 5551, 19393, 29950, 299, 10299, 3741, 30481, 1874, 29947, 9363, 188, 55, 4473, 4660, 29961, 10062, 29997, 29948, 23326, 30672, 29956, 30293, 1821, 29961, 6, 30124, 103, 628, 8381, 29956, 392, 30292, 45, 29947, 24545, 58, 5337, 29953, 515, 29947, 29948, 5, 8381, 9972, 29961, 378, 14, 148, 29956, 30293, 2355, 9889, 29983, 29965, 17386, 30053, 3196, 29951, 97, 29965, 2240, 30494, 627, 29947, 29948, 5, 409, 346, 30027, 30067, 846, 30055, 29951, 541, 30013, 4473, 4660, 29953, 8381, 392, 29982, 29961, 121, 30086, 9129, 260, 509, 29982, 29947, 30080, 2673, 8381, 29960, 30067, 29961, 1314, 29982, 18, 5293, 29982, 29974, 29950, 550, 5, 28, 29982, 29947, 30080, 29948, 5, 5337, 29961, 5849, 8381, 29960, 30067, 29953, 846, 10210, 29947, 7417, 260, 1, 10062, 29990, 1623, 29959, 8345, 51, 29952, 45, 9563, 4390, 29948, 5, 2423, 1686, 131, 29990, 1263, 29953, 235, 29956, 1428, 29950, 462, 29982, 648, 188, 29961, 4884, 6158, 29947, 613, 1556, 440, 97, 29950, 9628, 109, 29952, 45, 9563, 675, 30340, 1117, 2815, 29951, 29950, 7916, 29947, 116, 29952, 627, 29947, 29948, 5, 3, 299, 10299, 27674, 29990, 84, 30128, 27805, 29973, 27674, 29947, 1028, 417, 29951, 11536, 29950, 655, 29961, 420, 3], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"]},"metadata":{},"execution_count":10}],"source":["question = \"북태평양 기단과 오호츠크해 기단이 만나 국내에 머무르는 기간은?\"\n","context = \"올여름 장마가 17일 제주도에서 시작됐다. 서울 등 중부지방은 예년보다 사나흘 정도 늦은 이달 말께 장마가 시작될 전망이다.17일 기상청에 따르면 제주도 남쪽 먼바다에 있는 장마전선의 영향으로 이날 제주도 산간 및 내륙지역에 호우주의보가 내려지면서 곳곳에 100㎜에 육박하는 많은 비가 내렸다. 제주의 장마는 평년보다 2~3일, 지난해보다는 하루 일찍 시작됐다. 장마는 고온다습한 북태평양 기단과 한랭 습윤한 오호츠크해 기단이 만나 형성되는 장마전선에서 내리는 비를 뜻한다.장마전선은 18일 제주도 먼 남쪽 해상으로 내려갔다가 20일께 다시 북상해 전남 남해안까지 영향을 줄 것으로 보인다. 이에 따라 20~21일 남부지방에도 예년보다 사흘 정도 장마가 일찍 찾아올 전망이다. 그러나 장마전선을 밀어올리는 북태평양 고기압 세력이 약해 서울 등 중부지방은 평년보다 사나흘가량 늦은 이달 말부터 장마가 시작될 것이라는 게 기상청의 설명이다. 장마전선은 이후 한 달가량 한반도 중남부를 오르내리며 곳곳에 비를 뿌릴 전망이다. 최근 30년간 평균치에 따르면 중부지방의 장마 시작일은 6월24~25일이었으며 장마기간은 32일, 강수일수는 17.2일이었다.기상청은 올해 장마기간의 평균 강수량이 350~400㎜로 평년과 비슷하거나 적을 것으로 내다봤다. 브라질 월드컵 한국과 러시아의 경기가 열리는 18일 오전 서울은 대체로 구름이 많이 끼지만 비는 오지 않을 것으로 예상돼 거리 응원에는 지장이 없을 전망이다.\"\n","tokenizer(context, question)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1E9UJAXRO9Fi"},"outputs":[],"source":["class Dataset_base(Dataset):\n","    def __init__(self, contexts, questions, answers, model_max_position_embedings, tokenizer):\n","        self.tokenizer = tokenizer\n","        self.answers = answers\n","        self.questions = questions\n","        self.contexts = contexts\n","        self.model_max_position_embedings = model_max_position_embedings\n","        print(\"Tokenizing ...\")\n","        self.encodings = self.tokenizer(self.contexts, \n","                                        self.questions,\n","                                        max_length=512, #512 truncation // \n","                                        truncation=True,\n","                                        padding=\"max_length\",\n","                                        return_token_type_ids=False)\n","        print(\"Done !!!\")\n","        self.add_token_positions()\n","        \n","    def add_token_positions(self):\n","        start_positions = []\n","        end_positions = []\n","        for i in range(len(self.answers)):\n","            start_positions.append(self.encodings.char_to_token(i, self.answers[i]['answer_start']))\n","            end_positions.append(self.encodings.char_to_token(i, self.answers[i]['answer_end'] - 1)) # -1으로 : 진짜로 답이 있는 end_position 의 인덱스를 구함.(char_to_token은 인덱스를 구함)\n","            #https://huggingface.co/docs/tokenizers/v0.13.2/en/api/encoding#tokenizers.Encoding.char_to_token\n","\n","            # positions 값이 None 값이라면, answer가 포함된 context가 잘렸다는 의미\n","            if start_positions[-1] is None:\n","                print(\"there is an error 1\")\n","                start_positions[-1] = self.model_max_position_embedings\n","            if end_positions[-1] is None:\n","                print(\"there is an error 2\")\n","                end_positions[-1] = self.model_max_position_embedings\n","\n","        self.encodings.update({'start_positions': start_positions, 'end_positions': end_positions})\n","\n","        \n","    def get_data(self):\n","        return {\"contexts\":self.contexts, 'questions':self.questions, 'answers':self.answers}\n","    \n","    \n","    def get_encodings(self):\n","        return self.encodings\n","        \n","    \n","    def __getitem__(self, idx):\n","        return {key:torch.tensor(val[idx]) for key, val in self.encodings.items()}\n","    \n","    def __len__(self):\n","        return len(self.encodings['input_ids'])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":173,"referenced_widgets":["5e6f820c304e4fdeba11aed65da3235c","8944a44751584783999ef83da3cec7c1","91c8cd03fe9b4757913068b5a0d3b9e0","684a0164475e4871bd6ca255425207ac","8575e95e916f4435b835bae54f8bb6c1","197ab8be5b164b3a8cb39e2df7b708fb","ed4296257eeb40bdbe01f0b03401fc07","1d25c1fbf6c949de8f6b8ddd53dbb6e0","5d76f8c63367496eb01b7563cc00d434","a6cead05e993440cb173e4a4c3160599","485d954ce90d4f2f9ad5f8215908a595","2b031f5e76f649cd8c71211c7b54b1c5","f1ff684066574b02900eab4ca12455a2","d27e87edea68421bbcf83e9a60377e9e","2058f0f901704f499fe7f8bf3fe2c629","d7f9073eb5c7488f99a65b040b8d9086","804e17964e7546748fa18624feb2e391","0c2dcb9af0814f02837149733dc205dc","d0368652d9bb400f9debf10ecce50cf1","dfd86aab3ce84557b89f0c2df8f309d7","950cf1a28def47b497ec92b0dd367b0a","49527bb272e743e7880c7509a191cf4c"]},"id":"ynDpvL5YRujc","outputId":"763140af-a185-4677-de2f-8fcafe09cdd9"},"outputs":[{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/8811 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5e6f820c304e4fdeba11aed65da3235c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/978 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2b031f5e76f649cd8c71211c7b54b1c5"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Tokenizing ...\n","Done !!!\n","there is an error 2\n","Tokenizing ...\n","Done !!!\n"]}],"source":["train_contexts, train_questions, train_answers, valid_contexts, valid_questions, valid_answers = read_input(\"train.json\")\n","add_end_idx(train_answers, train_contexts)                                                      #anwer 마다 answer_end 달아준다.\n","train_dataset = Dataset_base(train_contexts, train_questions, train_answers, 512, tokenizer)\n","\n","add_end_idx(valid_answers, valid_contexts)                                                      #anwer 마다 answer_end 달아준다.\n","valid_dataset = Dataset_base(valid_contexts, valid_questions, valid_answers, 512, tokenizer)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AcZAGePiPL_I"},"outputs":[],"source":["model = AutoModelForQuestionAnswering.from_pretrained(model_name)"]},{"cell_type":"code","source":["!pip install wandb -qqq\n","import wandb\n","wandb.login()"],"metadata":{"id":"6BAT_SsBaqpf"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PDPGeivV_M1w"},"outputs":[],"source":["sweep_config = {\n","      'name' : 'project2_TEST1.ipynb',\n","      'method' : 'grid',\n","      'metric':{\n","          'name': 'total_valid_loss',\n","          'goal': 'minimize'  \n","      },\n","      'parameters' : {\n","          'learning_rate' : {\n","              'values' : [1e-4, 2.5e-5, 5e-5, 7.5e-5,1e-5]  \n","          },   \n","          'batch_size' :{\n","              'values' : [4,8,16]\n","          },\n","          'epochs' : {\n","              'values' : [2] \n","          }\n","      }\n","}\n","\n","sweep_id = wandb.sweep(sweep_config)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rTg8-yjFPL85"},"outputs":[],"source":["EPOCH = 1\n","LEARNING_RATE = 5e-5\n","BATCH_SIZE = 8\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":90},"id":"NYZrOKTqQDBp","outputId":"075718dd-f66b-4a93-9545-dd30c79c5232"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\nlevenshtein_distance 값이 튀는 현상 방지.\\n - 길이가 너무 긴 정답 삭제 max_length = 20\\n - 정답과 어느정도 길이차이나는 값 삭제(위와 동일) 5.9 -> 2정도 차이나는 값 삭제 7.9\\n \\n\\n자연어처리\\n자연어처리과정 2\\n0 5\\n\\n정답 길이보다 예측 정답 길이가 길지만 2배이상 차이가 나지 않는 경우에\\n0으로 처리하면 LD_SCORE가 더 안나옴\\n따라서 위 상황에서는 1.5 ~ 1.8 정도 해당하는 길이로 잘라서 반환.\\n\\ntest 답 X\\ntrain 과정 나온 답 -> 바꿔서 바꾼 데이터로 \\n\\n\\n** max_len = 5.9 * 2 = 12\\n\\ntrain 2번\\n1 train 원래 데이터.\\n2 train LD 변환한 데이터로 한번.\\n\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":15}],"source":["# Levenshtein_distance (Evaluation)\n","\n","import numpy \n","import torch\n","import os\n","\n","def levenshtein_distance(s1,s2, debug=False): #레벤슈타인 거리 eval / 정답 s1과 도출한 모델 s2 비교 평가\n","    if len(s1) < len(s2):\n","        return levenshtein_distance(s2, s1, debug)\n","\n","    if len(s2) == 0:\n","        return len(s1)\n","\n","    previous_row = range(len(s2) + 1)\n","    for i, c1 in enumerate(s1):\n","        current_row = [i + 1]\n","        for j, c2 in enumerate(s2):\n","            insertions = previous_row[j + 1] + 1\n","            deletions = current_row[j] + 1\n","            substitutions = previous_row[j] + (c1 != c2)\n","            current_row.append(min(insertions, deletions, substitutions))\n","\n","        if debug:\n","            print(current_row[1:])\n","\n","        previous_row = current_row\n","\n","    return previous_row[-1] # levenshtein_distance 값 출력력\n","\n","\n","def LD_SCORE(start_positions, end_positions, input_ids, STA_logits, END_logits):\n","\n","    answer1 = [] # 정답 저장\n","    answer2 = [] # 예측 정답 저장\n","\n","    if len(input_ids) != BATCH_SIZE: #오류 해결\n","       print(\"input_ids ERROR\")\n","       return 0\n","\n","    for i in range(BATCH_SIZE): # 기존 정답 // index 1 is out of bounds for dimension 0 with size 1 오류 발생\n","        if input_ids[i] == [] :\n","            print(\"input_ids ERROR\")\n","            continue       \n","        else:\n","            PRED_IDE = input_ids[i][start_positions[i]: end_positions[i] + 1]\n","            PRED_ANS = tokenizer.decode(PRED_IDE)\n","            answer1.append(PRED_ANS)\n","            #print(PRED_ANS)\n","\n","    for i in range(BATCH_SIZE): # 예측 정답\n","        if input_ids[i] == [] :\n","            print(\"input_ids ERROR\")\n","            continue\n","        else:\n","            TK_start_index, TK_end_index = STA_logits.argmax(dim=-1), END_logits.argmax(dim=-1)\n","            PRED_IDE2 = input_ids[i][TK_start_index[i]: TK_end_index[i] + 1]\n","            PRED_ANS2 = tokenizer.decode(PRED_IDE2)\n","            answer2.append(PRED_ANS2)\n","            # print(PRED_ANS2)\n","\n","    batch_score = LD_comparison(answer1, answer2)\n","\n","    return batch_score\n","\n","\n","def LD_comparison(answer1,answer2):\n","\n","    # 배치마다 레벤슈타인 거리 평균 구해서 출력. (train, valid 과정에서 배치마다 평균거리 구하고.)\n","    # train 전체 평균 거리\n","    # valid 전체 평균 거리\n","\n","    batch_LD_score = []\n","\n","    for i in range(BATCH_SIZE):\n","        if answer1[i] == answer2[i]: # 같으면 LD 구하는 과정 생략.\n","            batch_LD_score.append(0)\n","        else:\n","            batch_LD_score.append(levenshtein_distance(answer1[i],answer2[i]))\n","\n","    sum_LD_score = sum(batch_LD_score)\n","    LD_avg = sum_LD_score / BATCH_SIZE # 배치의 레벤슈타인 거리 평균\n","    #print(LD_avg)\n","\n","    return LD_avg\n","\n","\n","\n","'''\n","levenshtein_distance 값이 튀는 현상 방지.\n"," - 길이가 너무 긴 정답 삭제 max_length = 20\n"," - 정답과 어느정도 길이차이나는 값 삭제(위와 동일) 5.9 -> 2정도 차이나는 값 삭제 7.9\n"," \n","\n","자연어처리\n","자연어처리과정 2\n","0 5\n","\n","정답 길이보다 예측 정답 길이가 길지만 2배이상 차이가 나지 않는 경우에\n","0으로 처리하면 LD_SCORE가 더 안나옴\n","따라서 위 상황에서는 1.5 ~ 1.8 정도 해당하는 길이로 잘라서 반환.\n","\n","test 답 X\n","train 과정 나온 답 -> 바꿔서 바꾼 데이터로 \n","\n","\n","** max_len = 5.9 * 2 = 12\n","\n","train 2번\n","1 train 원래 데이터.\n","2 train LD 변환한 데이터로 한번.\n","\n","'''\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"69QT4ZwJPL4A"},"outputs":[],"source":["# LD SCORE 저장\n","train_LD_avg = [] \n","valid_LD_avg = []\n","\n","def train_runner(model, train_dataset, valid_dataset , batch_size, num_train_epochs, learning_rate):\n","\n","    #wandb.init(project = 'project2_test1',reinit=True)\n","\n","    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","    \n","    model.to(device)\n","    model.train()\n","    train_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size)\n","    valid_dataloader = DataLoader(dataset = valid_dataset, batch_size = batch_size)\n","\n","    lowest_total_valid_loss = 9999.\n","\n","    global_total_step = len(train_dataloader) * num_train_epochs\n","    optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=0)\n","    print(\"TRAIN START\")\n","    with tqdm(total=global_total_step, unit='step') as t:\n","        total = 0\n","        total_loss = 0\n","        for epoch in range(num_train_epochs):\n","            for iteration,batch in enumerate(train_dataloader):\n","                optimizer.zero_grad()\n","                input_ids = batch['input_ids'].to(device)\n","                attention_mask = batch['attention_mask'].to(device)\n","                start_positions = batch['start_positions'].to(device)\n","                end_positions = batch['end_positions'].to(device)                \n","\n","                outputs = model(input_ids,\n","                             attention_mask=attention_mask,\n","                             start_positions=start_positions,\n","                             end_positions=end_positions)\n","                \n","                ### LD_SCORE - train\n","                STA_logits, END_logits = outputs.start_logits, outputs.end_logits\n","                score_save1 = LD_SCORE(start_positions, end_positions, input_ids, STA_logits, END_logits)\n","                train_LD_avg.append(score_save1)\n","                #wandb.log({'train_batch_LD':score_save1})\n","\n","                ####\n","                #loss_logit_change(start_positions, end_positions, input_ids, STA_logits, END_logits)\n","                loss = outputs.loss\n","                #wandb loss\n","                #wandb.log({'Train_loss':loss.item()})\n","\n","                loss.backward()\n","                optimizer.step()\n","                \n","                batch_loss = loss.item() * len(input_ids)\n","                total += len(input_ids)\n","                total_loss += batch_loss\n","                global_total_step += 1\n","                t.set_postfix(loss=\"{:.6f}\".format(total_loss / total), batch_loss=\"{:.6f}\".format(batch_loss))\n","                t.update(1)\n","\n","                # wandb loss\n","                #wandb.log({'Train_batch_loss':batch_loss})\n","                #wandb.log({'LOSS':total_loss / total})\n","                \n","                del input_ids\n","                del attention_mask\n","                del start_positions\n","                del end_positions\n","                del outputs\n","                del loss\n","\n","                ## validation ##\n","                if iteration != 0 and iteration % int(len(train_dataloader) / 5) == 0:\n","                    total_valid_loss = 0\n","                    for batch_val in valid_dataloader:\n","                        model.eval()\n","                        optimizer.zero_grad()\n","\n","                        input_ids = batch_val['input_ids'].to(device)\n","                        attention_mask = batch_val['attention_mask'].to(device)\n","                        start_positions = batch_val['start_positions'].to(device)\n","                        end_positions = batch_val['end_positions'].to(device)\n","                \n","                        with torch.no_grad():\n","                            outputs = model(input_ids,\n","                                    attention_mask=attention_mask,\n","                                    start_positions=start_positions,\n","                                    end_positions=end_positions)\n","                            \n","                            ### LD_SCORE - valid\n","                            STA_logits, END_logits = outputs.start_logits, outputs.end_logits\n","                            score_save2 = LD_SCORE(start_positions, end_positions, input_ids, STA_logits, END_logits)\n","                            valid_LD_avg.append(score_save2)\n","\n","                            # wandb LD\n","                            #wandb.log({'Valid_batch_LD':score_save2})\n","\n","                            loss = outputs.loss\n","                            total_valid_loss += loss.item()\n","\n","                            # wandb loss\n","                            #wandb.log({'Valid_loss':loss.item()})\n","                            #wandb.log({'total_valid_loss':total_valid_loss})\n","                    \n","                    if total_valid_loss < lowest_total_valid_loss:\n","                        print(f\"lowest_total_valid_loss: {total_valid_loss} epoch : {epoch} iteration : {iteration}\")\n","                        torch.save(model.state_dict(),'./output_model_best')\n","                        lowest_total_valid_loss = total_valid_loss\n","                ## validation ##\n","\n","    #model.save_pretrained(\"./klue_output_model\")\n","    # train, valid 평균 LD 거리\n","    train_AVG = sum(train_LD_avg) / len(train_LD_avg)\n","    valid_AVG = sum(valid_LD_avg) / len(valid_LD_avg)\n","    #wandb.log({'train_AVG_LD':train_AVG})\n","    #wandb.log({'valid_AVG_LD':valid_AVG})\n","\n","    print('train LD average',train_AVG,'valid LD average',valid_AVG)\n","    print(\"TRAIN END\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zXl1Yo7y_3JH"},"outputs":[],"source":["# wandb sweep train 실행.\n","\n","# wandb.agent( sweep_id , function=train_runner, count=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":363,"referenced_widgets":["2b09566edcc84b1b8c02534b624fe748","d90d5876b0d141dda52d8ca9607810e5","cd5e9d06927742678f141237533dd71f","51f1bff4f8aa455a8da361e4e881ef54","16121a06461b47a3be7e151f56a51e73","3995f94cfe94449ab94bf50484dcc99b","6d13d27204624588a5cf73c51a43e58f","635ac5d7539f4288b196f77e75091440","517733d0c2944c56b531c8ace8f850e7","640345307e3d4ed6908e892c6e6854db","80850f3a370a48548d0110238ddbca8d"]},"id":"ywX4WIJXRuje","outputId":"edd4f1de-ef42-431b-851f-b3628ca67012"},"outputs":[{"output_type":"stream","name":"stdout","text":["TRAIN START\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/1986 [00:00<?, ?step/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2b09566edcc84b1b8c02534b624fe748"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["input_ids ERROR\n","lowest_total_valid_loss: 465.49361473321915 epoch : 0 iteration : 397\n","input_ids ERROR\n","lowest_total_valid_loss: 423.7243643403053 epoch : 0 iteration : 794\n","input_ids ERROR\n","lowest_total_valid_loss: 409.4988069832325 epoch : 0 iteration : 1191\n","input_ids ERROR\n","lowest_total_valid_loss: 400.55272579193115 epoch : 0 iteration : 1588\n","input_ids ERROR\n","input_ids ERROR\n","lowest_total_valid_loss: 394.0965484380722 epoch : 0 iteration : 1985\n","train LD average 15.977907854984894 valid LD average 13.96737668161435\n","TRAIN END\n"]}],"source":["train_runner(model,train_dataset,valid_dataset, BATCH_SIZE, EPOCH, LEARNING_RATE)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"he9L3jYZC8Rr","colab":{"base_uri":"https://localhost:8080/"},"outputId":"dcd9e759-9261-43d5-84f7-8cd88f6ad677"},"outputs":[{"output_type":"stream","name":"stdout","text":["predicted Train Avgrage LD 4.079225352112676 predicted Valid Avgrage LD 4.052006172839506\n"]}],"source":["MAX_LEN = 10\n","\n","# train / valid 과정에서 나온 이상값 제거 (정확한 점수 X)\n","\n","train_LD_avg = [v for v in train_LD_avg if v < MAX_LEN]  \n","TR_AVG = sum(train_LD_avg) / len(train_LD_avg) \n","\n","valid_LD_avg = [v for v in valid_LD_avg if v < MAX_LEN]\n","VA_AVG = sum(valid_LD_avg) / len(valid_LD_avg)\n","\n","print('predicted Train Avgrage LD',TR_AVG,'predicted Valid Avgrage LD',VA_AVG) # 예측값 확인."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZDVfXLBsPLrQ"},"outputs":[],"source":["def read_dev(path):\n","    with open(path, 'rb') as f:\n","        kdict = json.load(f)\n","\n","    contexts = []\n","    questions = []\n","    answers = []\n","    guids = []\n","\n","    for group in tqdm(kdict['data']):\n","        for passage in group['paragraphs']:\n","            context = passage['context']\n","            for qa in passage['qas']:\n","                question = qa['question']\n","                guid = qa['guid']\n","                #temp_answer = []\n","                #for answer in qa['answers']:\n","                    #temp_answer.append(answer['text'])\n","                #if len(temp_answer) != 0: # answers의 길이가 0 == 답변할 수 없는 질문\n","                    #contexts.append(context)\n","                    #questions.append(question)\n","                    #answers.append(temp_answer)\n","                contexts.append(context)##\n","                questions.append(question)##\n","                guids.append(guid)\n","\n","    #return contexts, questions, answers\n","    return contexts, questions , guids"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OAg0_WkAPLjw","colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["36ed4ed1d7c94da4920f4ee46e2f3c13","727fba6f4aa84e98b60e1a6a33b4b660","fa713be164c046dabbb6e9e14f532a39","8c7916185bd0464aa7f7402f9aeb5f35","bc046ebbef77441596053a73d96b4fce","c4f545332f194687acfe2dac90878d21","48aa72c2fc11433faf69c829b765e450","87b5ae1b08e04d6d8b1087b8f24e69f4","e307d68009574ac2a3d759002de6fcbb","2c409768b9384332b17de3886033252d","cf2f06d7504b40a9813a618c20fb1315"]},"outputId":"0f9c2f58-c5c1-4f4b-93a9-d4c5c5db1e01"},"outputs":[{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/3709 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"36ed4ed1d7c94da4920f4ee46e2f3c13"}},"metadata":{}}],"source":["#dev_contexts, dev_questions, dev_answers = read_dev(\"test.json\")\n","dev_contexts, dev_questions, dev_guids = read_dev(\"test.json\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qS8G0_kBhMhW"},"outputs":[],"source":["import re\n","def remove_post(text):\n","        ''' 불필요한 기호 제거 '''\n","        text = text.strip()\n","        text = re.sub(\"'\", \"\", text)\n","        text = re.sub('\"', \"\", text)\n","        text = re.sub('《', \"\", text)\n","        text = re.sub('》', \"\", text)\n","        text = re.sub('<', \"\", text)\n","        text = re.sub('>', \"\", text)\n","        text = re.sub('〈', \"\", text)\n","        text = re.sub('〉', \"\", text)\n","        text = re.sub(\"\\(\", \"\", text)\n","        text = re.sub(\"\\)\", \"\", text)\n","        text = re.sub(\"‘\", \"\", text)\n","        text = re.sub(\"’\", \"\", text)\n","        text = re.sub(\"  \", \" \", text)\n","        text = re.sub(\"#\", \"\", text)\n","        return text"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4HmQRZQp1NKT"},"outputs":[],"source":["'''\n","idea  \n","틀린 답을 바로 빈칸으로 내보내는게 과연 이득일까?\n","logit 확률값을 적용해서 틀린답이면 다음으로 높은 확률값이 정답일수도 있지않을까?\n","top 5의 확률값을 다 적용해도 틀린 답이라면 그때 빈칸을 내보내도 괜찮을듯.\n","'''\n","# start, end logit의 확률값을 이용한 예측 정답값\n","# logit의 상위 5개 확률을 리스트로 뽑아 틀린 정답이었다면 다음 확률로 넘어가서 확인.\n","\n","def to_list(tensor):\n","    return tensor.detach().cpu().tolist()\n","\n","def logits_change(input_ids, STA_logits, END_logits):\n","\n","    # 로짓의 확률값 ~ 상위 5개를 선택 \n","    # 틀린 추론이었다면 다음 선택 (틀린 추론 : start > end, 길이가 너무 긴 문장.)\n","    change_logit = 0\n","    cnt = 0\n","    \n","    # 기존 정답\n","    save_s = STA_logits\n","    save_e = END_logits\n","\n","    STK_start_index, STK_end_index = save_s.argmax(dim=-1), save_e.argmax(dim=-1)\n","    save_pred_ids = tokenizer.decode(input_ids[0][STK_start_index: STK_end_index + 1])\n","    #print(save_pred_ids) \n","\n","    # 바뀐 정답\n","    # Tensor형태의 logit의 확률값을 리스트로 만들어줌 \n","    STA_logits = to_list(STA_logits)[0]\n","    END_logits = to_list(END_logits)[0]\n","\n","    # 리스트로 만든 확률값 -> 큰 순서대로 정렬 + 인덱스 \n","    start_idx_and_logit = sorted(enumerate(STA_logits), key=lambda x: x[1], reverse=True)\n","    end_idx_and_logit = sorted(enumerate(END_logits), key=lambda x: x[1], reverse=True)\n","\n","    # 확률값 큰 순서대로 Top 5 \n","    start_idx_and_logit = start_idx_and_logit[:5]\n","    end_idx_and_logit = end_idx_and_logit[:5]\n","\n","    TK_start_index, TK_end_index = start_idx_and_logit, end_idx_and_logit\n","\n","    # 확률이 높은 순서대로 점검. 틀린 값이면 다음 확률로 넘어가고 맞는 답이면 저장후 반복문 종료\n","    for i in range(5):\n","        if TK_start_index[i][0] > TK_end_index[i][0] or TK_end_index[i][0] - TK_start_index[i][0] > 10 : \n","            cnt += 1\n","            continue\n","        else : \n","            change_logit += 1\n","            pred_ids = input_ids[0][TK_start_index[i][0]: TK_end_index[i][0] + 1]\n","            pred_ids = tokenizer.decode(pred_ids)\n","            #print(pred_ids, 'change')\n","            break\n","\n","    if change_logit == 0 :\n","        return save_pred_ids\n","    elif cnt == 5:\n","        return ''\n","    else : \n","        if pred_ids == save_pred_ids:\n","            #print('same answer')\n","            return pred_ids\n","        else :\n","            #print('different answer')\n","            return pred_ids\n","\n","    \n","\n","# start_logits , end_logits\n","# index를 추적하면서 시작, 종료 index에 대한 확률이 가장 높은것을 선택하는 방법.\n","# 만약에 차이가 큰 start, end 값을 반환할때 이 정보들을 저장하지 않고 넘긴다면? \n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rHaddLdiPf5z"},"outputs":[],"source":["def prediction(contexts, questions, guids):\n","    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","\n","    model.load_state_dict(torch.load('./output_model_best'))\n","    model.to(device)\n","    \n","    model.eval()\n","    \n","    result = []\n","    \n","    with torch.no_grad():\n","        \n","        for context, question, guid in zip(contexts, questions, guids):\n","            encodings = tokenizer(context, question, max_length=512, truncation=True,\n","                                     padding=\"max_length\", return_token_type_ids=False)\n","            encodings = {key: torch.tensor([val]) for key, val in encodings.items()}\n","            \n","            input_ids = encodings[\"input_ids\"].to(device)\n","            attention_mask = encodings[\"attention_mask\"].to(device)\n","            \n","            outputs = model(input_ids, attention_mask=attention_mask)\n","            start_logits, end_logits = outputs.start_logits, outputs.end_logits\n","\n","            ######\n","            pred =logits_change(input_ids, start_logits, end_logits)\n","            '''\n","            token_start_index, token_end_index = start_logits.argmax(dim=-1), end_logits.argmax(dim=-1)\n","            pred_ids = input_ids[0][token_start_index: token_end_index + 1]\n","            \n","            if token_start_index > token_end_index:\n","                pred = ''\n","            else:\n","                pred = tokenizer.decode(pred_ids)\n","                pred = pred[:10] # 1.5~1.8\n","            '''\n","            \n","\n","\n","            pred = pred[:8] # 6~12 테스트 결과 8이 가장 좋은 점수.\n","            pred = remove_post(pred)\n","\n","            tp = (guid,pred)\n","            \n","            result.append(tp)\n","    return result"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L20_dlBvPf3p"},"outputs":[],"source":["pred_answers = prediction(dev_contexts, dev_questions, dev_guids)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-FdYepZSPf1i"},"outputs":[],"source":["import csv\n","f = open('sangrimlee_bert-base-multilingual-cased-korquad_validation_pred_truncation.csv','w', newline='')\n","wr = csv.writer(f)\n","wr.writerow(['Id','Predicted'])\n","\n","for tp in pred_answers:\n","    wr.writerow([tp[0],tp[1]])\n","\n","f.close()"]},{"cell_type":"code","source":["# pred_answers에서 도출한 정답 비교\n","import csv\n","\n","def calculate_Leven(source, ref, result_file):\n","    with open(source, 'r') as input1:\n","        with open(ref, 'r') as input2:\n","            with open(result_file, 'w') as csvoutput:\n","                reader1 = csv.reader(input1)\n","                reader2 = list(csv.reader(input2))\n","                writer = csv.writer(csvoutput)\n","                result = []\n","                mean = []\n","                headers = next(reader1)\n","                result.append(headers)\n","                index = 0\n","                for row1 in reader1:\n","                    #print(\"First row\")\n","                    #print(row1[1])\n","                    index+=1\n","                    #print(reader2[index][1])\n","                    a = levenshtein_distance(row1[1], reader2[index][1])\n","                    #print(row1[1],'////',reader2[index][1])\n","                    '''\n","                    max = 0\n","                    while max < 1:\n","                        for row2 in reader2:\n","                            a = distance(row1[1],row2[1])\n","                            print(a)\n","                            b = 1 - a/len(row1[1])\n","                            if b > max:\n","                                max = b\n","                                SKU = row2[1]\n","                    '''\n","                    mean.append(a)\n","                    row1.append(a)\n","                    result.append(row1)\n","                mean1 = sum(mean) / len(mean)\n","                print(mean1)\n","                writer.writerows(result)"],"metadata":{"id":"a7IvIiXqaz5n"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nKbGAvSZmRyu","colab":{"base_uri":"https://localhost:8080/"},"outputId":"0595f895-aac9-4f29-e6d0-69865809527d"},"outputs":[{"output_type":"stream","name":"stdout","text":["3.0489021956087825\n"]}],"source":["calculate_Leven('klue.csv', 'sangrimlee_bert-base-multilingual-cased-korquad_validation_pred_truncation.csv', 'result_file.csv')"]},{"cell_type":"code","source":["### downloading pickle ###\n","import pickle\n","\n","with open('input_ids_and_logits.pickle', 'rb') as handle:\n","    A_model = pickle.load(handle)\n","\n","with open('input_ids_and_logits.pickle', 'rb') as handle:\n","    B_model = pickle.load(handle)\n","\n","with open('input_ids_and_logits.pickle', 'rb') as handle:\n","    C_model = pickle.load(handle)\n","    "],"metadata":{"id":"mZkdqMPJegqI"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ihJBQMJDqFXv"},"outputs":[],"source":["# logit 값에 대한 soft voting - 3 model\n","\n","def to_list(tensor):\n","    return tensor.detach().cpu().tolist()\n","\n","def logits_voting(A_model, B_model, C_model):\n","    # 각 모델마다 input_ids, start_logits, end_logits값 저장. 순서대로 저장 리스트에 + guids // context 필요.\n","    # 모델 logit 확률 top 5 구해서 모델 1 top1 모델 2 top1 모델3 top1 모델1 top2 순으로 체크.\n","    # print(list_dict_all[0][\"start_logits\"]) pickle\n","    # A_model\n","\n","    ind = 0\n","    RESULT = []\n","\n","    if len(A_model)!= len(B_model) or len(A_model) != len(C_model) or len(B_model) != len(C_model):\n","        print('error')\n","    else:\n","        llogit = len(A_model)\n","\n","    while 1:\n","        # input_ids\n","        A_input_ids = A_model[ind]['input_ids']\n","        B_input_ids = B_model[ind]['input_ids']\n","        C_input_ids = C_model[ind]['input_ids']\n","\n","        # guid\n","        A_guid = A_model[ind]['guid']\n","        B_guid = A_model[ind]['guid']\n","        C_guid = A_model[ind]['guid']\n","\n","        if A_input_ids != B_input_ids or B_input_ids != C_input_ids or A_input_ids != C_input_ids:\n","            print('input_error')\n","        if A_guid != B_guid or B_guid != C_guid or A_guid != C_guid:\n","            print('guid_error')\n","\n","        # logit 값 순서대로 불러오기기\n","        A_start_logit = A_model[ind]['start_logits']\n","        A_end_logit = A_model[ind]['end_logits']\n","        B_start_logit = B_model[ind]['start_logits']\n","        B_end_logit = B_model[ind]['end_logits']\n","        C_start_logit = C_model[ind]['start_logits']\n","        C_end_logit = C_model[ind]['end_logits']\n","\n","        # 로짓값 -> 확률 큰 순서대로 리스트형태 + 인덱스 -> top 5 자르기기\n","        A_start_idx_and_logit = (sorted(enumerate(A_start_logit), key=lambda x: x[1], reverse=True))[:5]\n","        A_end_idx_and_logit = (sorted(enumerate(A_end_logit), key=lambda x: x[1], reverse=True))[:5]\n","        B_start_idx_and_logit = (sorted(enumerate(B_start_logit), key=lambda x: x[1], reverse=True))[:5]\n","        B_end_idx_and_logit = (sorted(enumerate(B_end_logit), key=lambda x: x[1], reverse=True))[:5]\n","        C_start_idx_and_logit = (sorted(enumerate(C_start_logit), key=lambda x: x[1], reverse=True))[:5]\n","        C_end_idx_and_logit = (sorted(enumerate(C_end_logit), key=lambda x: x[1], reverse=True))[:5]\n","\n","\n","        cnt = 0\n","        for i in range(5):\n","            # 모델 A, B, C 에 대한 각각의 로짓값 저장해서 확률 5 리스트로 저장 후 비교.\n","            # A->B->C->A 순? A2 > B1 ? 어떤 순서대로 비교할건지 \n","            # 정답인 경우 멈추고 값 저장 후 종료\n","            # 토크나이저 다 다른데 어떻게 비교함? -> 정답 context \n","            # soft voting -> start + end 확률값 더해서 순위대로 정렬 후 정답인지 확인 -> 정답일때는 해당 모델에 대한 input 불러와서 answer, guid 저장.\n","\n","            if A_start_idx_and_logit[i][0] < A_end_idx_and_logit[i][0] or A_end_idx_and_logit[i][0] - A_start_idx_and_logit[i][0] <= 10 : \n","                cnt += 1\n","                pred_ids = A_input_ids[A_start_idx_and_logit[i][0]: A_end_idx_and_logit[i][0] + 1] # 해당 정답에 대한 인덱스 ~> context 변환 하는 과정 필요.\n","                # 토큰 -> 정답 변경.\n","                A_token = A_model['tokenizer'] # 저장형태 확인 // 근데 이렇게하면 decode가 되나? 해봐야 될듯?\n","                tokenizer = AutoTokenizer.from_pretrained(A_token)\n","                pred_ids = tokenizer.decode(pred_ids)\n","                guid = A_guid\n","                break\n","\n","            elif B_start_idx_and_logit[i][0] < B_end_idx_and_logit[i][0] or B_end_idx_and_logit[i][0] - B_start_idx_and_logit[i][0] <= 10 : \n","                cnt += 1\n","                pred_ids = B_input_ids[B_start_idx_and_logit[i][0]: B_end_idx_and_logit[i][0] + 1]\n","                B_token = B_model['tokenizer']\n","                tokenizer = AutoTokenizer.from_pretrained(B_token)\n","                pred_ids = tokenizer.decode(pred_ids)\n","                guid = B_guid\n","                break\n","\n","            elif C_start_idx_and_logit[i][0] < C_end_idx_and_logit[i][0] or C_end_idx_and_logit[i][0] - C_start_idx_and_logit[i][0] <= 10 : \n","                cnt += 1\n","                pred_ids = C_input_ids[C_start_idx_and_logit[i][0]: C_end_idx_and_logit[i][0] + 1]\n","                C_token = C_model['tokenizer']\n","                tokenizer = AutoTokenizer.from_pretrained(C_token)\n","                pred_ids = tokenizer.decode(pred_ids)\n","                guid = C_guid\n","                break\n","\n","\n","                 \n","\n","        # guids??? 저장해야함.\n","        if cnt == 0:\n","           tp = (guid,'')\n","           RESULT.append(tp)\n","        else:\n","           tp = (guid,pred_ids)\n","           RESULT.append(tp)\n","\n","        if llogit == ids:\n","            break\n","        ids += 1\n","\n","\n","            # 1 23 맞는 정답만 도출.\n","\n","\n"]},{"cell_type":"code","source":["answer_lt = logits_voting(A_model, B_model, C_model)"],"metadata":{"id":"ZO9ttfoUfLb4"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[]},"gpuClass":"premium","kernelspec":{"display_name":"torch","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13 (main, Oct 13 2022, 21:23:06) [MSC v.1916 64 bit (AMD64)]"},"vscode":{"interpreter":{"hash":"c044c91b12eb47f047c895d462d020aa8e4307e7bfafc2c2e5cdad580c7ef67a"}},"widgets":{"application/vnd.jupyter.widget-state+json":{"5e6f820c304e4fdeba11aed65da3235c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8944a44751584783999ef83da3cec7c1","IPY_MODEL_91c8cd03fe9b4757913068b5a0d3b9e0","IPY_MODEL_684a0164475e4871bd6ca255425207ac"],"layout":"IPY_MODEL_8575e95e916f4435b835bae54f8bb6c1"}},"8944a44751584783999ef83da3cec7c1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_197ab8be5b164b3a8cb39e2df7b708fb","placeholder":"​","style":"IPY_MODEL_ed4296257eeb40bdbe01f0b03401fc07","value":"100%"}},"91c8cd03fe9b4757913068b5a0d3b9e0":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_1d25c1fbf6c949de8f6b8ddd53dbb6e0","max":8811,"min":0,"orientation":"horizontal","style":"IPY_MODEL_5d76f8c63367496eb01b7563cc00d434","value":8811}},"684a0164475e4871bd6ca255425207ac":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a6cead05e993440cb173e4a4c3160599","placeholder":"​","style":"IPY_MODEL_485d954ce90d4f2f9ad5f8215908a595","value":" 8811/8811 [00:00&lt;00:00, 219108.96it/s]"}},"8575e95e916f4435b835bae54f8bb6c1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"197ab8be5b164b3a8cb39e2df7b708fb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ed4296257eeb40bdbe01f0b03401fc07":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1d25c1fbf6c949de8f6b8ddd53dbb6e0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5d76f8c63367496eb01b7563cc00d434":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a6cead05e993440cb173e4a4c3160599":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"485d954ce90d4f2f9ad5f8215908a595":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2b031f5e76f649cd8c71211c7b54b1c5":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f1ff684066574b02900eab4ca12455a2","IPY_MODEL_d27e87edea68421bbcf83e9a60377e9e","IPY_MODEL_2058f0f901704f499fe7f8bf3fe2c629"],"layout":"IPY_MODEL_d7f9073eb5c7488f99a65b040b8d9086"}},"f1ff684066574b02900eab4ca12455a2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_804e17964e7546748fa18624feb2e391","placeholder":"​","style":"IPY_MODEL_0c2dcb9af0814f02837149733dc205dc","value":"100%"}},"d27e87edea68421bbcf83e9a60377e9e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d0368652d9bb400f9debf10ecce50cf1","max":978,"min":0,"orientation":"horizontal","style":"IPY_MODEL_dfd86aab3ce84557b89f0c2df8f309d7","value":978}},"2058f0f901704f499fe7f8bf3fe2c629":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_950cf1a28def47b497ec92b0dd367b0a","placeholder":"​","style":"IPY_MODEL_49527bb272e743e7880c7509a191cf4c","value":" 978/978 [00:00&lt;00:00, 58090.88it/s]"}},"d7f9073eb5c7488f99a65b040b8d9086":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"804e17964e7546748fa18624feb2e391":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0c2dcb9af0814f02837149733dc205dc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d0368652d9bb400f9debf10ecce50cf1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dfd86aab3ce84557b89f0c2df8f309d7":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"950cf1a28def47b497ec92b0dd367b0a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"49527bb272e743e7880c7509a191cf4c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2b09566edcc84b1b8c02534b624fe748":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d90d5876b0d141dda52d8ca9607810e5","IPY_MODEL_cd5e9d06927742678f141237533dd71f","IPY_MODEL_51f1bff4f8aa455a8da361e4e881ef54"],"layout":"IPY_MODEL_16121a06461b47a3be7e151f56a51e73"}},"d90d5876b0d141dda52d8ca9607810e5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3995f94cfe94449ab94bf50484dcc99b","placeholder":"​","style":"IPY_MODEL_6d13d27204624588a5cf73c51a43e58f","value":"100%"}},"cd5e9d06927742678f141237533dd71f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_635ac5d7539f4288b196f77e75091440","max":1986,"min":0,"orientation":"horizontal","style":"IPY_MODEL_517733d0c2944c56b531c8ace8f850e7","value":1986}},"51f1bff4f8aa455a8da361e4e881ef54":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_640345307e3d4ed6908e892c6e6854db","placeholder":"​","style":"IPY_MODEL_80850f3a370a48548d0110238ddbca8d","value":" 1986/1986 [05:57&lt;00:00,  7.39step/s, batch_loss=9.194740, loss=2.029299]"}},"16121a06461b47a3be7e151f56a51e73":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3995f94cfe94449ab94bf50484dcc99b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6d13d27204624588a5cf73c51a43e58f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"635ac5d7539f4288b196f77e75091440":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"517733d0c2944c56b531c8ace8f850e7":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"640345307e3d4ed6908e892c6e6854db":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"80850f3a370a48548d0110238ddbca8d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"36ed4ed1d7c94da4920f4ee46e2f3c13":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_727fba6f4aa84e98b60e1a6a33b4b660","IPY_MODEL_fa713be164c046dabbb6e9e14f532a39","IPY_MODEL_8c7916185bd0464aa7f7402f9aeb5f35"],"layout":"IPY_MODEL_bc046ebbef77441596053a73d96b4fce"}},"727fba6f4aa84e98b60e1a6a33b4b660":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c4f545332f194687acfe2dac90878d21","placeholder":"​","style":"IPY_MODEL_48aa72c2fc11433faf69c829b765e450","value":"100%"}},"fa713be164c046dabbb6e9e14f532a39":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_87b5ae1b08e04d6d8b1087b8f24e69f4","max":3709,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e307d68009574ac2a3d759002de6fcbb","value":3709}},"8c7916185bd0464aa7f7402f9aeb5f35":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2c409768b9384332b17de3886033252d","placeholder":"​","style":"IPY_MODEL_cf2f06d7504b40a9813a618c20fb1315","value":" 3709/3709 [00:00&lt;00:00, 240629.13it/s]"}},"bc046ebbef77441596053a73d96b4fce":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c4f545332f194687acfe2dac90878d21":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"48aa72c2fc11433faf69c829b765e450":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"87b5ae1b08e04d6d8b1087b8f24e69f4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e307d68009574ac2a3d759002de6fcbb":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2c409768b9384332b17de3886033252d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cf2f06d7504b40a9813a618c20fb1315":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}