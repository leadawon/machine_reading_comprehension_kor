{"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"g0d8tiKv3Fk9","executionInfo":{"status":"ok","timestamp":1673093326415,"user_tz":-540,"elapsed":2416,"user":{"displayName":"이다원","userId":"11923633709773630529"}},"outputId":"b1aa3846-944f-42fd-e932-52635ff7aa11"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"markdown","metadata":{"id":"zkiNxKnFO05Z"},"source":["## data"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"T4euzcqrOuMh","outputId":"3dc48772-26d0-4bf4-8be9-f89ac25579a0","executionInfo":{"status":"ok","timestamp":1673093331028,"user_tz":-540,"elapsed":4618,"user":{"displayName":"이다원","userId":"11923633709773630529"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: transformers in /usr/local/lib/python3.8/dist-packages (4.25.1)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.13.2)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.25.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.8.2)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.11.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (4.0.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n"]}],"source":["!pip install transformers"]},{"cell_type":"markdown","metadata":{"id":"I5JxfHVARuja"},"source":["## gogo"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"_yskKCUsOuIw","executionInfo":{"status":"ok","timestamp":1673093331029,"user_tz":-540,"elapsed":13,"user":{"displayName":"이다원","userId":"11923633709773630529"}}},"outputs":[],"source":["tokenizer_name = \"monologg/koelectra-base-v3-finetuned-korquad\"\n","model_name = \"monologg/koelectra-base-v3-finetuned-korquad\""]},{"cell_type":"code","execution_count":4,"metadata":{"id":"DQr5Ivl7OuGZ","executionInfo":{"status":"ok","timestamp":1673093333972,"user_tz":-540,"elapsed":2955,"user":{"displayName":"이다원","userId":"11923633709773630529"}}},"outputs":[],"source":["import json\n","import random\n","\n","import torch\n","import numpy as np\n","from tqdm.notebook import tqdm\n","from torch.utils.data import Dataset, DataLoader\n","from transformers import AutoTokenizer, AutoModelForQuestionAnswering, AdamW"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IkWsSNujOuER","outputId":"d6c03e53-5988-4b2e-f192-2247855b3e00","executionInfo":{"status":"ok","timestamp":1673093334886,"user_tz":-540,"elapsed":934,"user":{"displayName":"이다원","userId":"11923633709773630529"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'title': '제주도 장마 시작 … 중부는 이달 말부터',\n"," 'paragraphs': [{'context': '올여름 장마가 17일 제주도에서 시작됐다. 서울 등 중부지방은 예년보다 사나흘 정도 늦은 이달 말께 장마가 시작될 전망이다.17일 기상청에 따르면 제주도 남쪽 먼바다에 있는 장마전선의 영향으로 이날 제주도 산간 및 내륙지역에 호우주의보가 내려지면서 곳곳에 100㎜에 육박하는 많은 비가 내렸다. 제주의 장마는 평년보다 2~3일, 지난해보다는 하루 일찍 시작됐다. 장마는 고온다습한 북태평양 기단과 한랭 습윤한 오호츠크해 기단이 만나 형성되는 장마전선에서 내리는 비를 뜻한다.장마전선은 18일 제주도 먼 남쪽 해상으로 내려갔다가 20일께 다시 북상해 전남 남해안까지 영향을 줄 것으로 보인다. 이에 따라 20~21일 남부지방에도 예년보다 사흘 정도 장마가 일찍 찾아올 전망이다. 그러나 장마전선을 밀어올리는 북태평양 고기압 세력이 약해 서울 등 중부지방은 평년보다 사나흘가량 늦은 이달 말부터 장마가 시작될 것이라는 게 기상청의 설명이다. 장마전선은 이후 한 달가량 한반도 중남부를 오르내리며 곳곳에 비를 뿌릴 전망이다. 최근 30년간 평균치에 따르면 중부지방의 장마 시작일은 6월24~25일이었으며 장마기간은 32일, 강수일수는 17.2일이었다.기상청은 올해 장마기간의 평균 강수량이 350~400㎜로 평년과 비슷하거나 적을 것으로 내다봤다. 브라질 월드컵 한국과 러시아의 경기가 열리는 18일 오전 서울은 대체로 구름이 많이 끼지만 비는 오지 않을 것으로 예상돼 거리 응원에는 지장이 없을 전망이다.',\n","   'qas': [{'question': '북태평양 기단과 오호츠크해 기단이 만나 국내에 머무르는 기간은?',\n","     'answers': [{'text': '한 달가량', 'answer_start': 478},\n","      {'text': '한 달', 'answer_start': 478}],\n","     'guid': '798db07f0b9046759deed9d4a35ce31e'}]}],\n"," 'news_category': '종합',\n"," 'source': 'hankyung'}"]},"metadata":{},"execution_count":5}],"source":["with open(\"/content/drive/MyDrive/WARNING_PRIVATE_FOLDER/goorm_nlp_8th_group3/goorm_nlp_8th_group3/project2/train.json\", 'rb') as f:\n","    input_dict = json.load(f)\n","input_dict[\"data\"][0]"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0DxIyRfTOuCI","outputId":"475274a0-c51d-4493-e9fa-49e4bb9fa8cd","executionInfo":{"status":"ok","timestamp":1673093334887,"user_tz":-540,"elapsed":17,"user":{"displayName":"이다원","userId":"11923633709773630529"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'title': '부산정보산업진흥원, 과기부 지역SW서비스사업화 지원사업 4개 과제 선정',\n"," 'paragraphs': [{'context': '부산시와 (재)부산정보산업진흥원(원장 이인숙)이 ‘2020~2021년 지역SW서비스사업화 지원사업’ 공모사업에 4개 과제가 선정되어 본격적인 사업 착수에 나선다. 과학기술정보통신부가 주관하는 ‘지역SW서비스사업화 지원사업’은 강소SW기업 및 초기 스타트업의 SW서비스 사업화 지원과 신시장 진출 지원을 통해 기업 경쟁력 강화와 지역경제 활성화를 도모하는 사업이다. 올해부터 2개년으로 진행되며, 국비와 시비, 민자 등 2년간 약 37억원의 예산이 투입된다. 앞서 진흥원은 부산의 미래 먹거리산업인 스마트해양, 지능형기계, 지능정보서비스 분야로 사전 수요조사를 진행했고, 평가를 통해 선정된 5개 과제를 공모사업에 신청했다. 그 결과 부산의 4개 과제가 최종 선정되는 쾌거를 거뒀다. 당 사업은 전국 진흥기관을 대상으로 공모를 시작해, 총 17개 지역에서 42개 과제가 선정되었으며, 4개 과제가 선정된 곳은 부산과 강원지역 뿐이다. 금번 선정된 과제들은 ‘인공지능융합센서와 서보 이송 로봇을 이용한 전단보강재의 자동용접시스템 개발’ 등 총 4개 과제다. 부산시가 지원하고, 부산정보산업진흥원과 지역기업, 대학, 연구소 등이 컨소시엄을 구성하여 기술개발 및 사업화 지원을 추진한다. 2개의 Track으로 구분되는 이번사업은 Track 1(SW중소기업)에서 ㈜에이아이플랫폼, 엔컴(주), Track 2(스타트업)에서는 ㈜토즈, 삼보테크놀로지를 지원한다. ○ ‘Track 1‘의 (주)에이아이플랫폼이 주관기업으로 진행하는 <인공지능 기반 망막 내 아밀로이드 플라크 영상 분석을 통한 치매조기진단 플랫폼 상용화>는 치매 확진의 원인이 되는 중요 단백질(아밀로이드 플라크)을 자체개발 관측장비로 진단한다. 이를 통해 치매를 조기 발견하여, 각종 경제적 비용과 치료 및 예방 등 사회적 문제를 해 결하고 시민들이 쉽게 접근 가능한 실효성 있는 치매관리체계 개발을 목표로 한다. ○ 엔컴(주)이 주관기업으로 참여하는 <AI영상분석 기반 가공철근 생산성 향상 시스템 기술개발 및 사업화>는 산업안전, 환경규제, 생산체계의 변화로 침체된 부산 핵심 산업인 철강업 활성화에 나선다. 실시간으로 절곡되어 나오는 가공철근의 형상을 인식하고 불량 형상 판단 시 적합한 교정 값을 절곡설비에 전달함으로써, 무중단 생산이 가능한 영상분석 기술과 생산설비 자동화 제어기술을 개발한다. ○ ‘Track 2’의 ㈜토즈는 자립기반이 약한 국내 중소형 조선소의 산업기술 변화에 혁신적인 대응을 위해 <가상현실 기반 원격 다자간 선박 및 해양구조물 사전 검사 시스템>을 개발한다. 선박 건조 前, 설계 단계에서 설계자 뿐만 아니라 생산관리자, 품질관리자, 선급검사관, 선주감독관 등의 이해관계자가 공동으로 가상의 환경에서 선박 및 해양구조물의 자재 배치와 간섭, 작업성, 설계 오작 등에 대한 검사를 진행할 수 있는 기술을 확보하여 조선소의 업무효율을 극대화 할 예정이다. ○ 삼보테크놀로지는 재래식 건설 부자재의 시공성, 안전성, 내구성 등의 문제점을 보완하여 시민 안전과 건설근로자의 환경개선, 생산성 및 수익성 향상을 위해 <인공지능융합센서와 새들형 토치 서보 이송 로봇을 이용한 고속 SRD 전단보강재 자동용접시스템>을 개발한다. 로봇응용 SRD 용접자동화 설비를 제작하고, 용접 모니터링 및 품질검사 소프트웨어를 개발하여 건설분야에 4차산업 대비 지능형 생산자동화 기반기술을 확보할 예정이다. (재)부산정보산업진흥원 이인숙 원장은 “이번 코로나19 사태로 인해 부산 기업들이 매출과 고용유지, 자재수급 등에 큰 타격을 입었지만, 지역SW서비스사업화 지원사업을 통해 지역과 기업차원에서 기반을 다지는 계기가 됐으면 좋겠다‘며 ”진흥원은 어려운 사태를 대비해 지역 기업들을 지원할 수 있는 다른 방편을 계속 모색 중이며, 더욱 성장해 나갈 수 있도록 적극 지원하겠다“고 전했다.',\n","   'qas': [{'question': '지능형 생산자동화 기반기술을 개발중인 스타트업은?',\n","     'answers': [{'text': '삼보테크놀로지', 'answer_start': 1422}],\n","     'guid': '67c85e4f86ae43939b807684537c909c'}]}],\n"," 'news_category': '경제',\n"," 'source': 'acrofan'}"]},"metadata":{},"execution_count":6}],"source":["input_dict[\"data\"][1]"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"2EvB1moGOt55","executionInfo":{"status":"ok","timestamp":1673093334888,"user_tz":-540,"elapsed":10,"user":{"displayName":"이다원","userId":"11923633709773630529"}}},"outputs":[],"source":["from copy import deepcopy\n","def split_input_dict(input_dict, ratio = 0.1, seed = 42):\n","    split_point = int(len(input_dict['data']) * ratio)\n","    random.seed(seed)\n","    random.shuffle(input_dict['data'])\n","    valid_dict = deepcopy(input_dict)\n","    train_dict = input_dict\n","\n","    valid_dict['data'] = input_dict['data'][:split_point]\n","    train_dict['data'] = input_dict['data'][split_point:]\n","    return train_dict, valid_dict"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"DHJppuYaO9Nl","executionInfo":{"status":"ok","timestamp":1673093334888,"user_tz":-540,"elapsed":9,"user":{"displayName":"이다원","userId":"11923633709773630529"}}},"outputs":[],"source":["def read_input(path):\n","    with open(path, 'rb') as f:\n","        input_dict = json.load(f)\n","    train_dict,valid_dict =split_input_dict(input_dict)\n","    train_contexts = []\n","    train_questions = []\n","    train_answers = []\n","    for group in tqdm(train_dict['data']):       #딕셔너리 하나씩 꺼낸다.\n","        for passage in group['paragraphs']:     #딕셔너리의 paragraphs\n","            context = passage['context']        #paragraphs의 context\n","            for qa in passage['qas']:           #paragraphs의 qas\n","                question = qa['question']       #paragraphs의 question\n","                for answer in qa['answers']:    #question의 answers\n","                    train_contexts.append(context)    #answers의 한 answer당 해당하는 context 저장\n","                    train_questions.append(question)  #answers의 한 answer당 해당하는 question 저장\n","                    train_answers.append(answer)      #answers의 한 answer 저장\n","  \n","    valid_contexts = []\n","    valid_questions = []\n","    valid_answers = []\n","    for group in tqdm(valid_dict['data']):       #딕셔너리 하나씩 꺼낸다.\n","        for passage in group['paragraphs']:     #딕셔너리의 paragraphs\n","            context = passage['context']        #paragraphs의 context\n","            for qa in passage['qas']:           #paragraphs의 qas\n","                question = qa['question']       #paragraphs의 question\n","                for answer in qa['answers']:    #question의 answers\n","                    valid_contexts.append(context)    #answers의 한 answer당 해당하는 context 저장\n","                    valid_questions.append(question)  #answers의 한 answer당 해당하는 question 저장\n","                    valid_answers.append(answer)      #answers의 한 answer 저장\n","    return train_contexts, train_questions, train_answers, valid_contexts, valid_questions, valid_answers"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"vKGCz_4WO9Lg","executionInfo":{"status":"ok","timestamp":1673093334889,"user_tz":-540,"elapsed":9,"user":{"displayName":"이다원","userId":"11923633709773630529"}}},"outputs":[],"source":["def add_end_idx(answers, contexts):\n","    for answer, context in zip(answers, contexts):\n","        gold_text = answer['text']\n","        start_idx = answer['answer_start']\n","        end_idx = start_idx + len(gold_text)\n","\n","        if context[start_idx:end_idx] == gold_text:\n","            answer['answer_end'] = end_idx\n","        elif context[start_idx-1:end_idx-1] == gold_text:\n","            print(\"there is an unitended error in dataset\") #이렇게까지 할 필요가 있나?\n","            answer['answer_start'] = start_idx - 1\n","            answer['answer_end'] = end_idx - 1\n","        elif context[start_idx-2:end_idx-2] == gold_text:\n","            print(\"there is an unitended error in dataset\")\n","            answer['answer_start'] = start_idx - 2\n","            answer['answer_end'] = end_idx - 2"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"1kXdeER8O9Jp","executionInfo":{"status":"ok","timestamp":1673093335584,"user_tz":-540,"elapsed":703,"user":{"displayName":"이다원","userId":"11923633709773630529"}}},"outputs":[],"source":["tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)"]},{"cell_type":"code","execution_count":33,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Hz1kYCgCO9Hx","outputId":"3b1a012f-4172-4de5-a5a2-f6640131cdda","executionInfo":{"status":"ok","timestamp":1673095489557,"user_tz":-540,"elapsed":13,"user":{"displayName":"이다원","userId":"11923633709773630529"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'input_ids': [2, 3161, 27913, 14323, 4070, 3, 2800, 20435, 27276, 3], 'token_type_ids': [0, 0, 0, 0, 0, 0, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"]},"metadata":{},"execution_count":33}],"source":["question = \"북태평양 기단과 오호츠크해 기단이 만나 국내에 머무르는 기간은?\"\n","context = \"올여름 장마가 17일 제주도에서 시작됐다. 서울 등 중부지방은 예년보다 사나흘 정도 늦은 이달 말께 장마가 시작될 전망이다.17일 기상청에 따르면 제주도 남쪽 먼바다에 있는 장마전선의 영향으로 이날 제주도 산간 및 내륙지역에 호우주의보가 내려지면서 곳곳에 100㎜에 육박하는 많은 비가 내렸다. 제주의 장마는 평년보다 2~3일, 지난해보다는 하루 일찍 시작됐다. 장마는 고온다습한 북태평양 기단과 한랭 습윤한 오호츠크해 기단이 만나 형성되는 장마전선에서 내리는 비를 뜻한다.장마전선은 18일 제주도 먼 남쪽 해상으로 내려갔다가 20일께 다시 북상해 전남 남해안까지 영향을 줄 것으로 보인다. 이에 따라 20~21일 남부지방에도 예년보다 사흘 정도 장마가 일찍 찾아올 전망이다. 그러나 장마전선을 밀어올리는 북태평양 고기압 세력이 약해 서울 등 중부지방은 평년보다 사나흘가량 늦은 이달 말부터 장마가 시작될 것이라는 게 기상청의 설명이다. 장마전선은 이후 한 달가량 한반도 중남부를 오르내리며 곳곳에 비를 뿌릴 전망이다. 최근 30년간 평균치에 따르면 중부지방의 장마 시작일은 6월24~25일이었으며 장마기간은 32일, 강수일수는 17.2일이었다.기상청은 올해 장마기간의 평균 강수량이 350~400㎜로 평년과 비슷하거나 적을 것으로 내다봤다. 브라질 월드컵 한국과 러시아의 경기가 열리는 18일 오전 서울은 대체로 구름이 많이 끼지만 비는 오지 않을 것으로 예상돼 거리 응원에는 지장이 없을 전망이다.\"\n","encodings = tokenizer(context, question,max_length=10,truncation=True)\n","encodings "]},{"cell_type":"code","execution_count":28,"metadata":{"id":"1E9UJAXRO9Fi","executionInfo":{"status":"ok","timestamp":1673095263647,"user_tz":-540,"elapsed":483,"user":{"displayName":"이다원","userId":"11923633709773630529"}}},"outputs":[],"source":["class KlueDataset(Dataset):\n","    def __init__(self, contexts, questions, answers, model_max_position_embedings, tokenizer):\n","        self.tokenizer = tokenizer\n","        self.answers = answers\n","        self.questions = questions\n","        self.contexts = contexts\n","        self.model_max_position_embedings = model_max_position_embedings\n","        print(\"Tokenizing ...\")\n","        self.encodings = self.tokenizer(self.contexts, \n","                                        self.questions,\n","                                        max_length=512, #512 truncation // \n","                                        truncation=True,\n","                                        padding=\"max_length\",\n","                                        return_token_type_ids=False)\n","        print(\"Done !!!\")\n","        self.add_token_positions()\n","        \n","    def add_token_positions(self):\n","        start_positions = []\n","        end_positions = []\n","        for i in range(len(self.answers)):\n","            start_positions.append(self.encodings.char_to_token(i, self.answers[i]['answer_start']))\n","            end_positions.append(self.encodings.char_to_token(i, self.answers[i]['answer_end'] - 1)) # -1으로 : 진짜로 답이 있는 end_position 의 인덱스를 구함.(char_to_token은 인덱스를 구함)\n","            #https://huggingface.co/docs/tokenizers/v0.13.2/en/api/encoding#tokenizers.Encoding.char_to_token\n","\n","            # positions 값이 None 값이라면, answer가 포함된 context가 잘렸다는 의미\n","            if start_positions[-1] is None:\n","                #print(\"there is an error 1\")\n","                start_positions[-1] = self.model_max_position_embedings\n","            if end_positions[-1] is None:\n","                #print(\"there is an error 2\")\n","                end_positions[-1] = self.model_max_position_embedings\n","            if (start_positions[-1] == None) and (start_positions[-1] != None):\n","                print(\"start to trunc\")\n","            if (start_positions[-1] != None) and (start_positions[-1] == None):\n","                print(\"end to trunc\")\n","            print(self.encodings[0]['tokens'])\n","            assert False\n","\n","        self.encodings.update({'start_positions': start_positions, 'end_positions': end_positions})\n","\n","        \n","    def get_data(self):\n","        return {\"contexts\":self.contexts, 'questions':self.questions, 'answers':self.answers}\n","    \n","    \n","    def get_encodings(self):\n","        return self.encodings\n","        \n","    \n","    def __getitem__(self, idx):\n","        return {key:torch.tensor(val[idx]) for key, val in self.encodings.items()}\n","    \n","    def __len__(self):\n","        return len(self.encodings['input_ids'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ynDpvL5YRujc"},"outputs":[],"source":["train_contexts, train_questions, train_answers, valid_contexts, valid_questions, valid_answers = read_input(\"/content/drive/MyDrive/WARNING_PRIVATE_FOLDER/goorm_nlp_8th_group3/goorm_nlp_8th_group3/project2/train.json\")\n","add_end_idx(train_answers, train_contexts)                                                      #anwer 마다 answer_end 달아준다.\n","train_dataset = KlueDataset(train_contexts, train_questions, train_answers, 512, tokenizer)\n","\n","add_end_idx(valid_answers, valid_contexts)                                                      #anwer 마다 answer_end 달아준다.\n","valid_dataset = KlueDataset(valid_contexts, valid_questions, valid_answers, 512, tokenizer)"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"AcZAGePiPL_I","executionInfo":{"status":"ok","timestamp":1673093344404,"user_tz":-540,"elapsed":2231,"user":{"displayName":"이다원","userId":"11923633709773630529"}}},"outputs":[],"source":["model = AutoModelForQuestionAnswering.from_pretrained(model_name)"]},{"cell_type":"code","execution_count":29,"metadata":{"id":"cWXsoBx--_26","executionInfo":{"status":"ok","timestamp":1673089706031,"user_tz":-540,"elapsed":38,"user":{"displayName":"이다원","userId":"11923633709773630529"}}},"outputs":[],"source":["# !pip install wandb -qqq\n","# import wandb\n","# wandb.login()"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"PDPGeivV_M1w","executionInfo":{"status":"ok","timestamp":1673093344405,"user_tz":-540,"elapsed":6,"user":{"displayName":"이다원","userId":"11923633709773630529"}}},"outputs":[],"source":["sweep_config = {\n","      'name' : 'project2_TEST1.ipynb',\n","      'method' : 'grid',\n","      'metric':{\n","          'name': 'total_valid_loss',\n","          'goal': 'minimize'  \n","      },\n","      'parameters' : {\n","          'learning_rate' : {\n","              'values' : [1e-4, 2.5e-5, 5e-5, 7.5e-5,1e-5]  \n","          },   \n","          'batch_size' :{\n","              'values' : [4,8,16]\n","          },\n","          'epochs' : {\n","              'values' : [2] \n","          }\n","      }\n","}\n","\n","# sweep_id = wandb.sweep(sweep_config)"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"rTg8-yjFPL85","executionInfo":{"status":"ok","timestamp":1673093344405,"user_tz":-540,"elapsed":5,"user":{"displayName":"이다원","userId":"11923633709773630529"}}},"outputs":[],"source":["EPOCH = 2\n","LEARNING_RATE = 5e-5\n","BATCH_SIZE = 16\n"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":72},"id":"NYZrOKTqQDBp","outputId":"7c9762e9-6b77-4d42-f4df-aceec9e74d0d","executionInfo":{"status":"ok","timestamp":1673093347412,"user_tz":-540,"elapsed":478,"user":{"displayName":"이다원","userId":"11923633709773630529"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\nlevenshtein_distance 값이 튀는 현상 방지.\\n - 길이가 너무 긴 정답 삭제 max_length = 20\\n - 정답과 어느정도 길이차이나는 값 삭제(위와 동일) 5.9 -> 2정도 차이나는 값 삭제 7.9\\n \\n\\n자연어처리\\n자연어처리과정 2\\n0 5\\n\\n정답 길이보다 예측 정답 길이가 길지만 2배이상 차이가 나지 않는 경우에\\n0으로 처리하면 LD_SCORE가 더 안나옴\\n따라서 위 상황에서는 1.5 ~ 1.8 정도 해당하는 길이로 잘라서 반환.\\n\\ntest 답 X\\ntrain 과정 나온 답 -> 바꿔서 바꾼 데이터로 \\n\\n\\n** max_len = 5.9 * 2 = 12\\n\\ntrain 2번\\n1 train 원래 데이터.\\n2 train LD 변환한 데이터로 한번.\\n\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":17}],"source":["# Levenshtein_distance (Evaluation)\n","\n","import numpy \n","import torch\n","import os\n","\n","def levenshtein_distance(s1,s2, debug=False): #레벤슈타인 거리 eval / 정답 s1과 도출한 모델 s2 비교 평가\n","    if len(s1) < len(s2):\n","        return levenshtein_distance(s2, s1, debug)\n","\n","    if len(s2) == 0:\n","        return len(s1)\n","\n","    previous_row = range(len(s2) + 1)\n","    for i, c1 in enumerate(s1):\n","        current_row = [i + 1]\n","        for j, c2 in enumerate(s2):\n","            insertions = previous_row[j + 1] + 1\n","            deletions = current_row[j] + 1\n","            substitutions = previous_row[j] + (c1 != c2)\n","            current_row.append(min(insertions, deletions, substitutions))\n","\n","        if debug:\n","            print(current_row[1:])\n","\n","        previous_row = current_row\n","\n","    return previous_row[-1] # levenshtein_distance 값 출력\n","\n","\n","def LD_SCORE(start_positions, end_positions, input_ids, STA_logits, END_logits):\n","\n","    answer1 = [] # 정답 저장\n","    answer2 = [] # 예측 정답 저장\n","\n","    if len(input_ids) != BATCH_SIZE: #오류 해결\n","       return 0\n","\n","    for i in range(BATCH_SIZE): # 기존 정답 // index 1 is out of bounds for dimension 0 with size 1 오류 발생\n","        if input_ids[i] == [] :\n","            continue       \n","        \n","        else:\n","            PRED_IDE = input_ids[i][start_positions[i]: end_positions[i] + 1]\n","            PRED_ANS = tokenizer.decode(PRED_IDE)\n","            answer1.append(PRED_ANS)\n","            #print(PRED_ANS)\n","\n","    for i in range(BATCH_SIZE): # 예측 정답\n","        if input_ids[i] == [] :\n","            continue\n","        else:\n","            TK_start_index, TK_end_index = STA_logits.argmax(dim=-1), END_logits.argmax(dim=-1)\n","            PRED_IDE2 = input_ids[i][TK_start_index[i]: TK_end_index[i] + 1]\n","            PRED_ANS2 = tokenizer.decode(PRED_IDE2)\n","            answer2.append(PRED_ANS2)\n","            # print(PRED_ANS2)\n","\n","    batch_score = LD_comparison(answer1, answer2)\n","\n","    return batch_score\n","\n","\n","def LD_comparison(answer1,answer2):\n","\n","    # 배치마다 레벤슈타인 거리 평균 구해서 출력. (train, valid 과정에서 배치마다 평균거리 구하고.)\n","    # train 전체 평균 거리\n","    # valid 전체 평균 거리\n","\n","    batch_LD_score = []\n","\n","    for i in range(BATCH_SIZE):\n","        if answer1[i] == answer2[i]: # 같으면 LD 구하는 과정 생략.\n","            batch_LD_score.append(0)\n","        else:\n","            batch_LD_score.append(levenshtein_distance(answer1[i],answer2[i]))\n","\n","    sum_LD_score = sum(batch_LD_score)\n","    LD_avg = sum_LD_score / BATCH_SIZE # 배치의 레벤슈타인 거리 평균\n","    print(LD_avg)\n","\n","    return LD_avg\n","\n","\n","\n","'''\n","levenshtein_distance 값이 튀는 현상 방지.\n"," - 길이가 너무 긴 정답 삭제 max_length = 20\n"," - 정답과 어느정도 길이차이나는 값 삭제(위와 동일) 5.9 -> 2정도 차이나는 값 삭제 7.9\n"," \n","\n","자연어처리\n","자연어처리과정 2\n","0 5\n","\n","정답 길이보다 예측 정답 길이가 길지만 2배이상 차이가 나지 않는 경우에\n","0으로 처리하면 LD_SCORE가 더 안나옴\n","따라서 위 상황에서는 1.5 ~ 1.8 정도 해당하는 길이로 잘라서 반환.\n","\n","test 답 X\n","train 과정 나온 답 -> 바꿔서 바꾼 데이터로 \n","\n","\n","** max_len = 5.9 * 2 = 12\n","\n","train 2번\n","1 train 원래 데이터.\n","2 train LD 변환한 데이터로 한번.\n","\n","'''\n"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"69QT4ZwJPL4A","executionInfo":{"status":"ok","timestamp":1673093351175,"user_tz":-540,"elapsed":5,"user":{"displayName":"이다원","userId":"11923633709773630529"}}},"outputs":[],"source":["# LD SCORE 저장\n","train_LD_avg = [] \n","valid_LD_avg = []\n","\n","def train_runner(model, train_dataset, valid_dataset , batch_size, num_train_epochs, learning_rate):\n","\n","    #wandb.init(project = 'project2_test1',reinit=True)\n","\n","    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","    \n","    model.to(device)\n","    model.train()\n","    train_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size)\n","    valid_dataloader = DataLoader(dataset = valid_dataset, batch_size = batch_size)\n","\n","    lowest_total_valid_loss = 9999.\n","\n","    global_total_step = len(train_dataloader) * num_train_epochs\n","    optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=0)\n","    print(\"TRAIN START\")\n","    with tqdm(total=global_total_step, unit='step') as t:\n","        total = 0\n","        total_loss = 0\n","        for epoch in range(num_train_epochs):\n","            for iteration,batch in enumerate(train_dataloader):\n","                optimizer.zero_grad()\n","                input_ids = batch['input_ids'].to(device)\n","                attention_mask = batch['attention_mask'].to(device)\n","                start_positions = batch['start_positions'].to(device)\n","                end_positions = batch['end_positions'].to(device)                \n","\n","                outputs = model(input_ids,\n","                             attention_mask=attention_mask,\n","                             start_positions=start_positions,\n","                             end_positions=end_positions)\n","                \n","                ### LD_SCORE - train\n","                STA_logits, END_logits = outputs.start_logits, outputs.end_logits\n","                score_save1 = LD_SCORE(start_positions, end_positions, input_ids, STA_logits, END_logits)\n","                train_LD_avg.append(score_save1)\n","                #wandb.log({'train_batch_LD':score_save1})\n","\n","                loss = outputs.loss\n","\n","                #wandb loss\n","                #wandb.log({'Train_loss':loss.item()})\n","\n","                loss.backward()\n","                optimizer.step()\n","                \n","                batch_loss = loss.item() * len(input_ids)\n","                total += len(input_ids)\n","                total_loss += batch_loss\n","                global_total_step += 1\n","                t.set_postfix(loss=\"{:.6f}\".format(total_loss / total), batch_loss=\"{:.6f}\".format(batch_loss))\n","                t.update(1)\n","\n","                # wandb loss\n","                #wandb.log({'Train_batch_loss':batch_loss})\n","                #wandb.log({'LOSS':total_loss / total})\n","                \n","                del input_ids\n","                del attention_mask\n","                del start_positions\n","                del end_positions\n","                del outputs\n","                del loss\n","\n","                ## validation ##\n","                if iteration != 0 and iteration % int(len(train_dataloader) / 5) == 0:\n","                    total_valid_loss = 0\n","                    for batch_val in valid_dataloader:\n","                        model.eval()\n","                        optimizer.zero_grad()\n","\n","                        input_ids = batch_val['input_ids'].to(device)\n","                        attention_mask = batch_val['attention_mask'].to(device)\n","                        start_positions = batch_val['start_positions'].to(device)\n","                        end_positions = batch_val['end_positions'].to(device)\n","                \n","                        with torch.no_grad():\n","                            outputs = model(input_ids,\n","                                    attention_mask=attention_mask,\n","                                    start_positions=start_positions,\n","                                    end_positions=end_positions)\n","                            \n","                            ### LD_SCORE - valid\n","                            STA_logits, END_logits = outputs.start_logits, outputs.end_logits\n","                            score_save2 = LD_SCORE(start_positions, end_positions, input_ids, STA_logits, END_logits)\n","                            valid_LD_avg.append(score_save2)\n","\n","                            # wandb LD\n","                            #wandb.log({'Valid_batch_LD':score_save2})\n","\n","                            loss = outputs.loss\n","                            total_valid_loss += loss.item()\n","\n","                            # wandb loss\n","                            #wandb.log({'Valid_loss':loss.item()})\n","                            #wandb.log({'total_valid_loss':total_valid_loss})\n","                    \n","                    if total_valid_loss < lowest_total_valid_loss:\n","                        print(f\"lowest_total_valid_loss: {total_valid_loss} epoch : {epoch} iteration : {iteration}\")\n","                        torch.save(model.state_dict(),'./output_model_best')\n","                        lowest_total_valid_loss = total_valid_loss\n","                ## validation ##\n","\n","    #model.save_pretrained(\"./klue_output_model\")\n","    # train, valid 평균 LD 거리\n","    train_AVG = sum(train_LD_avg) / len(train_LD_avg)\n","    valid_AVG = sum(valid_LD_avg) / len(valid_LD_avg)\n","\n","    print('train LD average',train_AVG,'valid LD average',valid_AVG)\n","    print(\"TRAIN END\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zXl1Yo7y_3JH"},"outputs":[],"source":["# wandb sweep train 실행.\n","\n","# wandb.agent( sweep_id , function=train_runner, count=1)"]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["5f0914111fdf4eb3ba0ed40e9a85b751","b0d7909032be4b8d8aad1caaf7e5752e","1c97d2cc6d4d40a1a7a89d6efc3d23db","64cf5f829da5410d8fe12b3a124e71c8","bd0c80fcfee94675b5cf54afc054fddd","21c07ff95d0a467dbcb65b14522ff800","9baf203547c54f73a24f60321b614012","f7b5664cec1744e1b066c073a1839c7a","a907390b40aa4ebe8545ec1a3d19b828","84679b9c3b1d4998871a03e88cb59fc2","6cf70b754755457290e3ea3c09683738"]},"id":"ywX4WIJXRuje","outputId":"8b624337-cab6-4482-b3e1-114413809942","executionInfo":{"status":"error","timestamp":1673094996162,"user_tz":-540,"elapsed":1641670,"user":{"displayName":"이다원","userId":"11923633709773630529"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["TRAIN START\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/1986 [00:00<?, ?step/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5f0914111fdf4eb3ba0ed40e9a85b751"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["107.1875\n","157.875\n","61.75\n","55.125\n","49.625\n","26.9375\n","6.125\n","4.0625\n","33.9375\n","6.375\n","3.875\n","14.75\n","40.0625\n","7.375\n","205.125\n","5.25\n","4.8125\n","5.5625\n","11.25\n","52.25\n","32.375\n","31.8125\n","4.3125\n","4.6875\n","3.9375\n","52.9375\n","3.875\n","10.9375\n","5.375\n","19.125\n","48.25\n","6.375\n","5.875\n","4.3125\n","5.625\n","11.625\n","47.625\n","8.0625\n","3.3125\n","2.4375\n","3.125\n","9.25\n","117.625\n","2.375\n","5.8125\n","6.5\n","2.5625\n","29.625\n","94.5625\n","20.5625\n","11.125\n","37.5625\n","4.1875\n","25.5625\n","4.9375\n","3.5\n","3.75\n","20.75\n","28.375\n","35.875\n","32.625\n","1.6875\n","4.9375\n","5.25\n","112.0625\n","71.4375\n","77.875\n","7.625\n","36.4375\n","40.5625\n","4.8125\n","2.9375\n","61.1875\n","34.625\n","33.625\n","34.8125\n","21.1875\n","74.25\n","47.25\n","27.8125\n","12.6875\n","36.125\n","5.8125\n","9.3125\n","1.9375\n","27.9375\n","60.4375\n","71.75\n","56.1875\n","5.125\n","57.5625\n","78.8125\n","12.8125\n","18.1875\n","24.75\n","7.0625\n","76.6875\n","1.0\n","3.125\n","1.5625\n","11.4375\n","47.125\n","3.125\n","32.0\n","3.5625\n","31.375\n","15.25\n","11.5625\n","38.75\n","6.625\n","82.125\n","24.9375\n","16.125\n","19.0625\n","3.75\n","1.75\n","6.0625\n","7.125\n","35.0625\n","2.5625\n","15.875\n","4.5\n","30.0\n","4.0\n","28.625\n","46.3125\n","3.75\n","3.4375\n","18.375\n","42.8125\n","4.4375\n","16.25\n","9.1875\n","7.8125\n","2.625\n","3.6875\n","8.25\n","3.5625\n","42.0\n","7.6875\n","14.4375\n","6.375\n","4.375\n","61.25\n","42.0\n","25.1875\n","3.125\n","7.375\n","25.5625\n","96.75\n","47.4375\n","12.25\n","54.125\n","9.375\n","3.9375\n","23.5\n","27.875\n","7.8125\n","21.25\n","52.375\n","13.3125\n","62.5625\n","4.5\n","39.3125\n","11.5625\n","12.0625\n","44.5\n","22.0625\n","4.3125\n","41.25\n","30.0\n","3.4375\n","3.125\n","3.5\n","3.5625\n","6.3125\n","1.875\n","1.5625\n","4.5\n","15.9375\n","8.375\n","25.25\n","8.8125\n","50.125\n","46.75\n","36.25\n","7.25\n","4.8125\n","2.4375\n","3.375\n","44.3125\n","77.375\n","31.3125\n","22.5625\n","21.3125\n","51.5\n","3.125\n","32.3125\n","1.5\n","78.5625\n","3.3125\n","1.125\n","5.625\n","4.25\n","49.75\n","90.6875\n","9.8125\n","3.5625\n","4.125\n","7.375\n","14.1875\n","7.0625\n","3.1875\n","58.8125\n","7.875\n","4.1875\n","4.875\n","3.75\n","28.4375\n","3.9375\n","1.625\n","8.625\n","4.5625\n","7.8125\n","15.875\n","6.125\n","55.125\n","4.1875\n","4.5625\n","5.375\n","110.1875\n","2.375\n","5.5\n","4.625\n","2.875\n","3.9375\n","2.4375\n","10.875\n","33.8125\n","2.75\n","1.5625\n","2.125\n","3.125\n","4.375\n","6.625\n","1.75\n","5.25\n","50.625\n","51.3125\n","58.6875\n","1.6875\n","1.625\n","2.375\n","3.9375\n","4.8125\n","7.0625\n","3.5625\n","2.8125\n","29.875\n","3.0\n","4.75\n","4.5625\n","3.9375\n","5.0\n","3.4375\n","3.0625\n","55.8125\n","19.125\n","2.0625\n","4.0625\n","1.6875\n","10.625\n","3.5\n","3.9375\n","4.9375\n","7.4375\n","4.625\n","4.9375\n","5.0625\n","4.6875\n","7.0\n","11.0625\n","1.5625\n","1.5\n","134.6875\n","30.625\n","105.625\n","4.8125\n","5.5\n","1.75\n","30.4375\n","4.6875\n","77.875\n","7.625\n","1.0\n","5.125\n","2.8125\n","30.9375\n","4.6875\n","1.5\n","1.6875\n","1.375\n","3.5625\n","43.9375\n","3.8125\n","3.875\n","33.9375\n","49.1875\n","22.375\n","1.3125\n","lowest_total_valid_loss: 171.04407012462616 epoch : 0 iteration : 198\n","2.9375\n","149.25\n","4.4375\n","196.6875\n","4.0\n","3.375\n","7.0\n","2.8125\n","10.3125\n","61.5\n","13.875\n","6.375\n","1.8125\n","7.9375\n","4.25\n","1.625\n","20.6875\n","5.375\n","6.5\n","2.0\n","1.75\n","5.6875\n","20.5\n","5.5\n","5.3125\n","1.875\n","11.5625\n","2.625\n","5.6875\n","6.1875\n","8.5625\n","3.5\n","4.0625\n","15.125\n","3.75\n","4.4375\n","1.375\n","3.25\n","3.4375\n","2.5625\n","39.0\n","4.875\n","52.8125\n","63.1875\n","1.6875\n","3.25\n","52.0\n","4.8125\n","2.4375\n","15.375\n","6.0625\n","2.9375\n","3.9375\n","20.9375\n","46.625\n","67.9375\n","4.4375\n","3.0\n","3.6875\n","3.1875\n","2.8125\n","2.9375\n","19.8125\n","6.125\n","41.4375\n","1.0\n","2.5\n","3.5625\n","52.875\n","4.625\n","1.6875\n","3.3125\n","49.5625\n","12.0625\n","2.0625\n","2.5625\n","2.4375\n","3.0\n","1.5\n","6.75\n","4.9375\n","3.3125\n","29.5625\n","16.8125\n","3.6875\n","6.4375\n","16.25\n","23.75\n","4.75\n","6.375\n","3.1875\n","1.4375\n","7.9375\n","1.4375\n","39.875\n","37.375\n","11.3125\n","18.5625\n","2.5\n","1.875\n","1.75\n","1.625\n","5.75\n","5.5625\n","12.5625\n","4.75\n","5.4375\n","3.9375\n","2.5\n","1.3125\n","1.125\n","3.6875\n","1.875\n","17.1875\n","3.4375\n","4.8125\n","1.1875\n","1.625\n","33.875\n","21.3125\n","5.5\n","5.0\n","7.0\n","3.3125\n","2.875\n","16.0625\n","29.0\n","6.9375\n","5.9375\n","5.25\n","12.8125\n","6.5625\n","6.3125\n","1.0625\n","30.5625\n","1.875\n","98.6875\n","61.4375\n","7.9375\n","56.375\n","29.5625\n","4.8125\n","2.5\n","38.1875\n","2.625\n","5.375\n","20.5\n","3.5625\n","3.625\n","2.9375\n","84.0\n","4.0625\n","35.6875\n","1.9375\n","3.25\n","2.4375\n","3.625\n","5.5625\n","13.0\n","3.875\n","19.5625\n","13.125\n","38.8125\n","2.0625\n","5.1875\n","19.375\n","44.0625\n","5.0625\n","2.875\n","24.8125\n","16.5625\n","8.6875\n","21.5625\n","9.4375\n","1.375\n","1.375\n","4.25\n","40.0\n","5.875\n","3.0625\n","8.9375\n","3.75\n","1.8125\n","51.125\n","2.25\n","2.75\n","16.25\n","2.5\n","2.3125\n","1.9375\n","5.875\n","2.125\n","2.25\n","10.875\n","3.625\n","43.125\n","1.625\n","36.625\n","9.0\n","3.125\n","1.4375\n","5.625\n","1.8125\n","55.0625\n","8.5625\n","2.75\n","2.125\n","4.0\n","38.1875\n","8.5625\n","3.1875\n","85.3125\n","5.5625\n","5.25\n","3.75\n","14.9375\n","7.0625\n","3.625\n","31.25\n","2.5625\n","11.6875\n","4.25\n","57.0625\n","10.0\n","6.5625\n","2.25\n","2.0\n","4.0625\n","35.375\n","1.3125\n","2.1875\n","3.6875\n","3.625\n","2.4375\n","3.75\n","1.8125\n","42.5\n","1.6875\n","2.25\n","1.25\n","2.125\n","2.9375\n","3.1875\n","5.5625\n","29.375\n","4.75\n","3.5\n","51.0625\n","21.125\n","2.6875\n","30.1875\n","1.875\n","85.625\n","64.25\n","25.0625\n","9.25\n","2.5\n","14.5625\n","3.8125\n","3.25\n","14.125\n","2.0\n","5.5\n","13.4375\n","2.9375\n","2.1875\n","2.3125\n","1.6875\n","3.25\n","15.1875\n","11.375\n","3.3125\n","4.75\n","5.625\n","2.0625\n","3.6875\n","5.125\n","8.6875\n","7.3125\n","1.4375\n","26.3125\n","1.625\n","28.8125\n","3.6875\n","2.25\n","4.3125\n","28.0625\n","29.8125\n","1.3125\n","47.5625\n","3.6875\n","50.5625\n","3.375\n","0.8125\n","4.6875\n","3.8125\n","10.25\n","3.5625\n","1.0\n","3.375\n","34.25\n","2.5\n","5.6875\n","26.8125\n","60.1875\n","47.0625\n","0.6875\n","27.625\n","1.5625\n","lowest_total_valid_loss: 161.17350336164236 epoch : 0 iteration : 396\n","3.4375\n","1.75\n","5.75\n","7.25\n","2.1875\n","3.75\n","1.4375\n","8.875\n","3.4375\n","7.0625\n","2.25\n","3.75\n","38.0625\n","5.375\n","6.8125\n","11.625\n","35.0625\n","43.6875\n","64.4375\n","5.75\n","7.375\n","4.1875\n","4.125\n","40.75\n","1.375\n","6.875\n","2.75\n","3.375\n","29.9375\n","6.25\n","1.25\n","5.625\n","4.125\n","33.1875\n","29.1875\n","17.0625\n","5.0625\n","1.625\n","1.5625\n","4.25\n","4.0625\n","2.0\n","16.25\n","9.0\n","6.5\n","2.1875\n","1.375\n","33.0\n","4.4375\n","4.25\n","1.5\n","4.9375\n","5.3125\n","2.6875\n","1.5625\n","2.5625\n","4.1875\n","19.1875\n","1.75\n","18.9375\n","90.6875\n","19.4375\n","3.25\n","3.0625\n","25.0625\n","5.25\n","8.0625\n","1.1875\n","2.3125\n","16.3125\n","2.6875\n","3.5625\n","133.75\n","16.875\n","12.625\n","18.0\n","2.6875\n","1.9375\n","4.0\n","9.5625\n","9.1875\n","13.0\n","47.625\n","4.6875\n","3.6875\n","4.25\n","2.75\n","19.625\n","25.5625\n","159.125\n","7.375\n","1.9375\n","10.4375\n","2.3125\n","14.5\n","40.5\n","3.0625\n","13.25\n","1.6875\n","2.9375\n","6.0\n","21.3125\n","1.5625\n","9.8125\n","8.5\n","3.3125\n","15.375\n","2.8125\n","22.625\n","3.375\n","2.1875\n","2.5625\n","9.25\n","4.6875\n","3.875\n","1.4375\n","2.8125\n","2.125\n","6.5625\n","1.5\n","6.5\n","47.4375\n","30.25\n","3.375\n","5.625\n","2.8125\n","1.375\n","4.25\n","0.9375\n","3.25\n","6.5625\n","11.625\n","7.5625\n","2.625\n","7.5625\n","29.3125\n","14.1875\n","6.1875\n","2.75\n","3.0\n","17.375\n","2.875\n","23.9375\n","1.3125\n","4.0625\n","13.875\n","47.75\n","4.9375\n","2.0625\n","62.5\n","12.125\n","3.6875\n","4.25\n","66.5625\n","1.8125\n","2.5625\n","7.3125\n","2.375\n","3.5625\n","3.0\n","2.0\n","1.6875\n","1.3125\n","17.375\n","3.875\n","3.625\n","5.5\n","6.3125\n","14.75\n","3.625\n","2.5\n","2.75\n","8.75\n","2.0625\n","4.1875\n","1.625\n","2.625\n","1.75\n","21.1875\n","3.0\n","35.625\n","33.5\n","4.0\n","3.6875\n","3.1875\n","2.75\n","13.0\n","3.75\n","2.1875\n","17.4375\n","5.25\n","1.0625\n","41.3125\n","2.5\n","9.5625\n","3.9375\n","4.875\n","24.0\n","10.125\n","3.5\n","1.75\n","4.6875\n","1.4375\n","174.875\n","3.5\n","3.3125\n","2.0\n","4.0625\n","3.6875\n","14.125\n","3.9375\n","3.4375\n","38.375\n","31.4375\n","3.75\n","5.875\n","6.125\n","2.625\n","3.0625\n","3.9375\n","12.9375\n","4.25\n","9.25\n","18.125\n","7.25\n","2.4375\n","5.0625\n","4.4375\n","2.0625\n","1.0625\n","2.125\n","5.1875\n","9.0\n","30.4375\n","4.3125\n","2.1875\n","2.0\n","2.4375\n","2.0625\n","0.625\n","2.125\n","3.75\n","28.3125\n","5.9375\n","51.3125\n","5.1875\n","3.625\n","18.9375\n","3.5\n","3.0\n","2.8125\n","2.8125\n","3.4375\n","4.4375\n","4.4375\n","3.3125\n","32.125\n","63.625\n","2.8125\n","4.1875\n","25.3125\n","53.8125\n","4.5\n","3.5625\n","10.9375\n","2.0625\n","8.125\n","1.5\n","2.6875\n","1.875\n","11.625\n","3.1875\n","2.0\n","5.4375\n","1.625\n","3.9375\n","3.6875\n","57.6875\n","7.125\n","1.75\n","24.5\n","1.5625\n","3.1875\n","5.8125\n","19.25\n","4.25\n","7.8125\n","7.25\n","18.4375\n","3.125\n","4.3125\n","5.9375\n","8.375\n","2.1875\n","5.0625\n","3.0625\n","3.5625\n","56.625\n","1.1875\n","3.125\n","0.6875\n","3.875\n","5.0625\n","4.5625\n","7.375\n","1.875\n","3.9375\n","7.3125\n","1.5625\n","5.0625\n","20.25\n","6.4375\n","2.875\n","2.5625\n","3.625\n","24.0\n","1.375\n","3.625\n","2.75\n","15.3125\n","6.4375\n","28.625\n","8.0625\n","4.375\n","1.625\n","62.0625\n","113.125\n","2.375\n","8.9375\n","3.0625\n","56.5625\n","2.25\n","9.6875\n","2.875\n","6.0\n","5.125\n","2.75\n","1.4375\n","3.125\n","2.1875\n","1.625\n","2.5\n","3.75\n","5.625\n","5.8125\n","3.0\n","11.5625\n","14.3125\n","2.375\n","1.3125\n","6.6875\n","6.0\n","34.875\n","4.125\n","2.125\n","31.0625\n","3.25\n","2.8125\n","2.1875\n","15.9375\n","14.4375\n","2.4375\n","5.8125\n","27.75\n","38.9375\n","6.75\n","11.625\n","3.8125\n","83.25\n","4.1875\n","6.5625\n","8.9375\n","4.125\n","1.9375\n","21.4375\n","28.5625\n","1.9375\n","3.3125\n","37.6875\n","17.125\n","48.5\n","1.625\n","3.8125\n","40.875\n","14.6875\n","1.5625\n","2.0625\n","8.8125\n","2.5\n","5.3125\n","4.9375\n","2.75\n","10.625\n","33.0625\n","31.3125\n","3.4375\n","4.3125\n","2.25\n","3.625\n","50.3125\n","3.0625\n","3.625\n","1.75\n","5.25\n","2.5625\n","1.5625\n","3.375\n","3.3125\n","2.4375\n","108.0625\n","1.875\n","1.75\n","103.625\n","37.75\n","68.5625\n","4.875\n","2.1875\n","3.9375\n","3.25\n","3.5625\n","75.1875\n","4.1875\n","56.625\n","11.0\n","65.375\n","3.4375\n","4.5\n","4.25\n","23.125\n","16.75\n","4.8125\n","16.625\n","1.875\n","2.125\n","1.75\n","5.0625\n","16.5625\n","4.25\n","1.8125\n","3.5625\n","10.0625\n","4.3125\n","2.6875\n","4.5\n","2.5625\n","12.1875\n","1.1875\n","47.125\n","3.125\n","2.875\n","4.6875\n","2.9375\n","110.75\n","16.125\n","2.875\n","2.6875\n","4.6875\n","45.5625\n","1.875\n","1.0625\n","2.1875\n","1.875\n","58.8125\n","2.0625\n","34.25\n","2.4375\n","1.9375\n","81.125\n","1.375\n","61.625\n","62.6875\n","2.625\n","4.375\n","4.4375\n","1.0625\n","13.375\n","6.8125\n","15.9375\n","7.6875\n","9.0625\n","4.8125\n","2.25\n","72.5625\n","9.25\n","7.8125\n","3.5625\n","2.0\n","3.375\n","1.1875\n","3.375\n","4.1875\n","3.1875\n","3.0625\n","3.5\n","138.6875\n","4.25\n","83.0625\n","5.125\n","13.375\n","6.6875\n","3.0\n","2.5\n","1.4375\n","5.75\n","4.0625\n","2.0\n","10.1875\n","6.375\n","3.0\n","1.4375\n","37.125\n","12.8125\n","61.1875\n","3.4375\n","2.625\n","14.3125\n","4.1875\n","3.25\n","70.4375\n","2.0\n","12.0625\n","4.5625\n","4.3125\n","11.5\n","46.0\n","3.0625\n","2.875\n","14.5\n","2.5625\n","9.0\n","4.0\n","9.4375\n","3.625\n","31.5625\n","2.4375\n","2.625\n","4.75\n","47.375\n","10.5\n","2.5\n","5.0\n","4.25\n","5.625\n","5.5\n","2.0625\n","2.4375\n","1.6875\n","2.75\n","0.875\n","2.125\n","3.625\n","3.3125\n","5.9375\n","1.5625\n","4.4375\n","50.0\n","3.9375\n","1.1875\n","6.625\n","1.625\n","2.625\n","3.375\n","47.25\n","2.8125\n","3.75\n","2.0625\n","43.8125\n","2.8125\n","6.9375\n","7.625\n","7.25\n","4.9375\n","3.3125\n","8.5\n","1.625\n","2.25\n","2.0\n","3.0\n","15.1875\n","11.3125\n","2.375\n","4.0625\n","8.1875\n","2.0625\n","4.3125\n","4.875\n","9.25\n","17.875\n","22.875\n","17.625\n","1.4375\n","1.875\n","3.875\n","2.3125\n","51.4375\n","6.4375\n","6.25\n","1.5625\n","3.0625\n","3.8125\n","25.4375\n","8.0625\n","1.1875\n","5.1875\n","3.8125\n","3.8125\n","2.9375\n","1.1875\n","58.1875\n","35.8125\n","25.6875\n","2.9375\n","3.875\n","2.0625\n","36.9375\n","1.875\n","8.0\n","1.3125\n","lowest_total_valid_loss: 154.2910830527544 epoch : 0 iteration : 792\n","6.25\n","2.75\n","3.25\n","4.125\n","6.25\n","29.0625\n","46.0\n","2.75\n","1.875\n","1.5\n","1.5625\n","4.375\n","3.3125\n","0.5\n","5.625\n","45.0625\n","42.4375\n","2.1875\n","2.625\n","4.0\n","2.75\n","4.0625\n","6.0\n","3.875\n","2.5625\n","5.5\n","4.3125\n","2.8125\n","3.0625\n","2.4375\n","1.75\n","7.5625\n","3.0\n","2.375\n","1.6875\n","2.3125\n","3.3125\n","2.4375\n","54.3125\n","3.4375\n","2.1875\n","4.8125\n","4.375\n","6.6875\n","2.625\n","3.5\n","5.8125\n","4.3125\n","5.5\n","1.25\n","1.8125\n","2.875\n","3.125\n","2.9375\n","2.6875\n","2.6875\n","2.9375\n","3.75\n","2.625\n","39.5\n","2.6875\n","33.5625\n","4.0625\n","1.6875\n","9.75\n","4.6875\n","0.75\n","1.75\n","10.0\n","4.25\n","3.1875\n","2.625\n","1.125\n","12.3125\n","2.0\n","3.25\n","8.4375\n","3.3125\n","4.25\n","3.5\n","8.0625\n","19.5\n","1.6875\n","34.125\n","3.1875\n","0.5625\n","3.0\n","1.0625\n","3.8125\n","9.25\n","4.6875\n","11.0\n","25.1875\n","0.9375\n","8.5\n","2.875\n","5.5\n","20.875\n","5.0\n","7.4375\n","4.0\n","11.9375\n","4.8125\n","5.1875\n","1.625\n","4.9375\n","2.4375\n","5.0\n","40.375\n","1.875\n","2.1875\n","1.3125\n","1.875\n","2.3125\n","3.125\n","23.1875\n","16.3125\n","4.625\n","1.6875\n","1.5625\n","2.25\n","2.4375\n","2.25\n","1.75\n","7.25\n","6.8125\n","1.9375\n","2.0625\n","3.125\n","3.5625\n","49.3125\n","4.0625\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-19-ea4fdf6c8e0f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_runner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalid_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPOCH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLEARNING_RATE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-18-b7acc2a4da08>\u001b[0m in \u001b[0;36mtrain_runner\u001b[0;34m(model, train_dataset, valid_dataset, batch_size, num_train_epochs, learning_rate)\u001b[0m\n\u001b[1;32m     37\u001b[0m                 \u001b[0;31m### LD_SCORE - train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m                 \u001b[0mSTA_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEND_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend_logits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m                 \u001b[0mscore_save1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLD_SCORE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_positions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_positions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSTA_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEND_logits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m                 \u001b[0mtrain_LD_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore_save1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m                 \u001b[0;31m#wandb.log({'train_batch_LD':score_save1})\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-17-14f8552be957>\u001b[0m in \u001b[0;36mLD_SCORE\u001b[0;34m(start_positions, end_positions, input_ids, STA_logits, END_logits)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mPRED_IDE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart_positions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mend_positions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m             \u001b[0mPRED_ANS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPRED_IDE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0manswer1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPRED_ANS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["train_runner(model,train_dataset,valid_dataset, BATCH_SIZE, EPOCH, LEARNING_RATE)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Vpr5MHx728Xb"},"outputs":[],"source":["MAX_LEN = 12 # 평균길이 5.9 * 2\n","\n","# train / valid 과정에서 나온 12 이상값 제거 / TEST 도 12이상 값 제거하니까\n","\n","train_LD_avg = [v for v in train_LD_avg if v < MAX_LEN]  \n","TR_AVG = sum(train_LD_avg) / len(train_LD_avg) \n","\n","valid_LD_avg = [v for v in valid_LD_avg if v < MAX_LEN]\n","VA_AVG = sum(valid_LD_avg) / len(valid_LD_avg)\n","\n","print('predicted Train Avgrage LD',TR_AVG,'predicted Valid Avgrage LD',VA_AVG) # 예측값 확인."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qLbJw1CnjBpG"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","t = list(range(1, 31))\n","plt.plot(t, train_losses, label=\"Train Loss\")\n","plt.plot(t, dev_losses, label=\"Dev Loss\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Loss\")\n","plt.legend()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZDVfXLBsPLrQ"},"outputs":[],"source":["def read_dev_klue(path):\n","    with open(path, 'rb') as f:\n","        klue_dict = json.load(f)\n","\n","    contexts = []\n","    questions = []\n","    answers = []\n","    guids = []\n","\n","    for group in tqdm(klue_dict['data']):\n","        for passage in group['paragraphs']:\n","            context = passage['context']\n","            for qa in passage['qas']:\n","                question = qa['question']\n","                guid = qa['guid']\n","                #temp_answer = []\n","                #for answer in qa['answers']:\n","                    #temp_answer.append(answer['text'])\n","                #if len(temp_answer) != 0: # answers의 길이가 0 == 답변할 수 없는 질문\n","                    #contexts.append(context)\n","                    #questions.append(question)\n","                    #answers.append(temp_answer)\n","                contexts.append(context)##\n","                questions.append(question)##\n","                guids.append(guid)\n","\n","    #return contexts, questions, answers\n","    return contexts, questions , guids"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OAg0_WkAPLjw"},"outputs":[],"source":["#dev_contexts, dev_questions, dev_answers = read_dev_klue(\"test.json\")\n","dev_contexts, dev_questions, dev_guids = read_dev_klue(\"test.json\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m_En1krVPgAC"},"outputs":[],"source":["def read_dev_klue(path):\n","    with open(path, 'rb') as f:\n","        klue_dict = json.load(f)\n","\n","    contexts = []\n","    questions = []\n","    answers = []\n","    guids = []\n","\n","    for group in tqdm(klue_dict['data']):\n","        for passage in group['paragraphs']:\n","            context = passage['context']\n","            for qa in passage['qas']:\n","                question = qa['question']\n","                guid = qa['guid']\n","                #temp_answer = []\n","                #for answer in qa['answers']:\n","                    #temp_answer.append(answer['text'])\n","                #if len(temp_answer) != 0: # answers의 길이가 0 == 답변할 수 없는 질문\n","                    #contexts.append(context)\n","                    #questions.append(question)\n","                    #answers.append(temp_answer)\n","                contexts.append(context)##\n","                questions.append(question)##\n","                guids.append(guid)\n","\n","    #return contexts, questions, answers\n","    return contexts, questions , guids"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_f9F5OSVPf9x"},"outputs":[],"source":["#dev_contexts, dev_questions, dev_answers = read_dev_klue(\"test.json\")\n","dev_contexts, dev_questions, dev_guids = read_dev_klue(\"test.json\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rHaddLdiPf5z"},"outputs":[],"source":["def prediction(contexts, questions, guids):\n","    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","\n","    model.load_state_dict(torch.load('./output_model_best'))\n","    model.to(device)\n","    \n","    model.eval()\n","    \n","    result = []\n","    \n","    with torch.no_grad():\n","        \n","        for context, question, guid in zip(contexts, questions, guids):\n","            encodings = tokenizer(context, question, max_length=512, truncation=True,\n","                                     padding=\"max_length\", return_token_type_ids=False)\n","            encodings = {key: torch.tensor([val]) for key, val in encodings.items()}\n","            \n","            input_ids = encodings[\"input_ids\"].to(device)\n","            attention_mask = encodings[\"attention_mask\"].to(device)\n","            \n","            outputs = model(input_ids, attention_mask=attention_mask)\n","            start_logits, end_logits = outputs.start_logits, outputs.end_logits\n","            token_start_index, token_end_index = start_logits.argmax(dim=-1), end_logits.argmax(dim=-1)\n","            pred_ids = input_ids[0][token_start_index: token_end_index + 1]\n","            if token_start_index > token_end_index:\n","                print(\"이상값 : \",tokenizer.decode(pred_ids))\n","                pred = tokenizer.decode(pred_ids)\n","            else:\n","                pred = tokenizer.decode(pred_ids)\n","                pred = pred[:12] # max_len // avg 5.9 * 2\n","            \n","\n","            tp = (guid,pred)\n","            \n","            result.append(tp)\n","    return result"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L20_dlBvPf3p"},"outputs":[],"source":["pred_answers = prediction(dev_contexts, dev_questions, dev_guids)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-FdYepZSPf1i"},"outputs":[],"source":["import csv\n","f = open('sangrimlee_bert-base-multilingual-cased-korquad_validation_pred_truncation.csv','w', newline='')\n","wr = csv.writer(f)\n","wr.writerow(['Id','Predicted'])\n","\n","for tp in pred_answers:\n","    wr.writerow([tp[0],tp[1]])\n","\n","f.close()"]}],"metadata":{"colab":{"machine_shape":"hm","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"torch","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13 (main, Oct 13 2022, 21:23:06) [MSC v.1916 64 bit (AMD64)]"},"vscode":{"interpreter":{"hash":"c044c91b12eb47f047c895d462d020aa8e4307e7bfafc2c2e5cdad580c7ef67a"}},"widgets":{"application/vnd.jupyter.widget-state+json":{"5f0914111fdf4eb3ba0ed40e9a85b751":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b0d7909032be4b8d8aad1caaf7e5752e","IPY_MODEL_1c97d2cc6d4d40a1a7a89d6efc3d23db","IPY_MODEL_64cf5f829da5410d8fe12b3a124e71c8"],"layout":"IPY_MODEL_bd0c80fcfee94675b5cf54afc054fddd"}},"b0d7909032be4b8d8aad1caaf7e5752e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_21c07ff95d0a467dbcb65b14522ff800","placeholder":"​","style":"IPY_MODEL_9baf203547c54f73a24f60321b614012","value":" 47%"}},"1c97d2cc6d4d40a1a7a89d6efc3d23db":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"danger","description":"","description_tooltip":null,"layout":"IPY_MODEL_f7b5664cec1744e1b066c073a1839c7a","max":1986,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a907390b40aa4ebe8545ec1a3d19b828","value":925}},"64cf5f829da5410d8fe12b3a124e71c8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_84679b9c3b1d4998871a03e88cb59fc2","placeholder":"​","style":"IPY_MODEL_6cf70b754755457290e3ea3c09683738","value":" 925/1986 [27:19&lt;26:21,  1.49s/step, batch_loss=17.016022, loss=1.624144]"}},"bd0c80fcfee94675b5cf54afc054fddd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"21c07ff95d0a467dbcb65b14522ff800":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9baf203547c54f73a24f60321b614012":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f7b5664cec1744e1b066c073a1839c7a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a907390b40aa4ebe8545ec1a3d19b828":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"84679b9c3b1d4998871a03e88cb59fc2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6cf70b754755457290e3ea3c09683738":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}