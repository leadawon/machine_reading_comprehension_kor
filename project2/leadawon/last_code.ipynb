{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","collapsed_sections":["SNInDOadAuk2","r5VA_73MARCd"],"authorship_tag":"ABX9TyN9L5R5OLCGoukueCx9Wsjv"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard","widgets":{"application/vnd.jupyter.widget-state+json":{"77f561f0f1c94936bcaa8b7d45e53d44":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f8fd4bc9ed4e472aa9f35f01d6ee800a","IPY_MODEL_2f2eb2539fac4f7a856403d3c9ac8701","IPY_MODEL_10f9dbd3399f49038542f1f25167e12b"],"layout":"IPY_MODEL_6fdb2a72977e416c8f6e28e9c398450d"}},"f8fd4bc9ed4e472aa9f35f01d6ee800a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_69ac1e584cbd45d2aa8ff0c38628a8fc","placeholder":"​","style":"IPY_MODEL_ecc6f48bd94a41b2bf27213443378676","value":"100%"}},"2f2eb2539fac4f7a856403d3c9ac8701":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_fe0ef43f11504adabd4131e8f6bca41f","max":16911,"min":0,"orientation":"horizontal","style":"IPY_MODEL_fd4c1612aaa64a7cb43b5a450bb55aa7","value":16911}},"10f9dbd3399f49038542f1f25167e12b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6f367e83d6eb4d4f96c0aeb2183887b5","placeholder":"​","style":"IPY_MODEL_28ce1df75f454fdf85e9de026ada0e3d","value":" 16911/16911 [00:00&lt;00:00, 225225.69it/s]"}},"6fdb2a72977e416c8f6e28e9c398450d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"69ac1e584cbd45d2aa8ff0c38628a8fc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ecc6f48bd94a41b2bf27213443378676":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fe0ef43f11504adabd4131e8f6bca41f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fd4c1612aaa64a7cb43b5a450bb55aa7":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6f367e83d6eb4d4f96c0aeb2183887b5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"28ce1df75f454fdf85e9de026ada0e3d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c90eb242866f4816afb33092b521a7f9":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_32a2e94b3c864ea6a974099f92d1cdc7","IPY_MODEL_348783115d0740bdb755dc0f91ad8ace","IPY_MODEL_d42acfc1dc464540a488e43383a7222a"],"layout":"IPY_MODEL_43e68534029c48d98e8aff9e50c1df09"}},"32a2e94b3c864ea6a974099f92d1cdc7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9dd5d89a6d9744a88cfbf3f43983788c","placeholder":"​","style":"IPY_MODEL_119b9822069246ada041afdb6af99353","value":"100%"}},"348783115d0740bdb755dc0f91ad8ace":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_7ce230603d714703b26b5283e7b0c67d","max":1878,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6e2681aab0c64ce58639f749a03b9f0d","value":1878}},"d42acfc1dc464540a488e43383a7222a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_68c92a712ade492ab0de2a622d1d5bcc","placeholder":"​","style":"IPY_MODEL_ec37f2096ab94587a8aa03a50a25b0b5","value":" 1878/1878 [00:00&lt;00:00, 93629.98it/s]"}},"43e68534029c48d98e8aff9e50c1df09":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9dd5d89a6d9744a88cfbf3f43983788c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"119b9822069246ada041afdb6af99353":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7ce230603d714703b26b5283e7b0c67d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6e2681aab0c64ce58639f749a03b9f0d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"68c92a712ade492ab0de2a622d1d5bcc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ec37f2096ab94587a8aa03a50a25b0b5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"016583368ab24c1e90b2a49621ce16c3":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_659d099dff7d405dbc43c2bb76194d5e","IPY_MODEL_4468eacacabc41189322cf18dfdb1523","IPY_MODEL_5f7a5a4c71104dae9331eced05d22997"],"layout":"IPY_MODEL_2dd5539a55704fa5b038815802d3f1e8"}},"659d099dff7d405dbc43c2bb76194d5e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9026eb12f4d344fa87a28a423829b44c","placeholder":"​","style":"IPY_MODEL_59d2f161893b42b99412aa4840dd1a29","value":" 25%"}},"4468eacacabc41189322cf18dfdb1523":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"danger","description":"","description_tooltip":null,"layout":"IPY_MODEL_919450c55c044f018e8ec7ad951e071a","max":24030,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ad06331546204bdba7c764d2d4c0ec83","value":6003}},"5f7a5a4c71104dae9331eced05d22997":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7122c69dc40a4e9dbc458b1085c73417","placeholder":"​","style":"IPY_MODEL_926578b953924f22acc125715d831a0a","value":" 6003/24030 [2:54:56&lt;6:22:21,  1.27s/step, batch_loss=1.426046, loss=1.802479]"}},"2dd5539a55704fa5b038815802d3f1e8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9026eb12f4d344fa87a28a423829b44c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"59d2f161893b42b99412aa4840dd1a29":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"919450c55c044f018e8ec7ad951e071a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ad06331546204bdba7c764d2d4c0ec83":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7122c69dc40a4e9dbc458b1085c73417":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"926578b953924f22acc125715d831a0a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7dcf62cc7eef4faab5938f30e9de7c12":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_31bb6b56a8c046a4893ce8f1b0829200","IPY_MODEL_232cc3978f8a4a7c9b7083db1ba2c2b2","IPY_MODEL_73a44608ef5d414695ba62601326aed3"],"layout":"IPY_MODEL_3e02d04e688c4c338da787ca015deb73"}},"31bb6b56a8c046a4893ce8f1b0829200":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a2e5550c0d074bbcab45059d9a9d7fb2","placeholder":"​","style":"IPY_MODEL_ea3cf60a9bc6429d87b54cd3a0023df7","value":"100%"}},"232cc3978f8a4a7c9b7083db1ba2c2b2":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e527b36c1c694a91bca427d7201965da","max":3709,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2560346c196848779b67460b02e8894e","value":3709}},"73a44608ef5d414695ba62601326aed3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ed9dd6585bf64fceb5dcbd4fca1e6766","placeholder":"​","style":"IPY_MODEL_57473b20b11044199fcd5269e06e86e6","value":" 3709/3709 [00:00&lt;00:00, 227008.62it/s]"}},"3e02d04e688c4c338da787ca015deb73":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a2e5550c0d074bbcab45059d9a9d7fb2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ea3cf60a9bc6429d87b54cd3a0023df7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e527b36c1c694a91bca427d7201965da":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2560346c196848779b67460b02e8894e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ed9dd6585bf64fceb5dcbd4fca1e6766":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"57473b20b11044199fcd5269e06e86e6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iD2fyMsZUvjr","executionInfo":{"status":"ok","timestamp":1673578140512,"user_tz":-540,"elapsed":3875,"user":{"displayName":"이다원","userId":"11923633709773630529"}},"outputId":"a3bbb528-a714-432c-98f9-8be5101a4f8d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["!pip install transformers"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3NzdBggGVDWN","executionInfo":{"status":"ok","timestamp":1673578143653,"user_tz":-540,"elapsed":3148,"user":{"displayName":"이다원","userId":"11923633709773630529"}},"outputId":"9a60a7eb-0e38-42a4-b725-3926bb0a4130"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: transformers in /usr/local/lib/python3.8/dist-packages (4.25.1)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.13.2)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.25.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.11.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.9.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (4.0.0)\n"]}]},{"cell_type":"code","source":["tokenizer_name = \"nlp04/org_and_korquad\"\n","model_name = \"nlp04/org_and_korquad\"\n","do_accumulation = False\n","do_wandb = False"],"metadata":{"id":"DzO254vpvh33"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ldhyGH4nTyGf"},"outputs":[],"source":["import json\n","import random\n","\n","import torch\n","import numpy as np\n","from tqdm.notebook import tqdm\n","from torch.utils.data import Dataset, DataLoader\n","from transformers import AutoTokenizer, AutoModelForQuestionAnswering, AdamW"]},{"cell_type":"markdown","source":["## preprocessing\n"],"metadata":{"id":"SNInDOadAuk2"}},{"cell_type":"code","source":["with open(\"/content/drive/MyDrive/WARNING_PRIVATE_FOLDER/goorm_nlp_8th_group3/new_train.json\", 'rb') as f:\n","    input_dict = json.load(f)\n","input_dict[\"data\"][0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4Nw0P9pTT9lU","executionInfo":{"status":"ok","timestamp":1673578148465,"user_tz":-540,"elapsed":928,"user":{"displayName":"이다원","userId":"11923633709773630529"}},"outputId":"296a5389-7ff6-422c-8b9f-a389f73ed98e"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'title': '제주도 장마 시작 … 중부는 이달 말부터',\n"," 'paragraphs': [{'context': '올여름 장마가 17일 제주도에서 시작됐다. 서울 등 중부지방은 예년보다 사나흘 정도 늦은 이달 말께 장마가 시작될 전망이다.17일 기상청에 따르면 제주도 남쪽 먼바다에 있는 장마전선의 영향으로 이날 제주도 산간 및 내륙지역에 호우주의보가 내려지면서 곳곳에 100㎜에 육박하는 많은 비가 내렸다. 제주의 장마는 평년보다 2~3일, 지난해보다는 하루 일찍 시작됐다. 장마는 고온다습한 북태평양 기단과 한랭 습윤한 오호츠크해 기단이 만나 형성되는 장마전선에서 내리는 비를 뜻한다.장마전선은 18일 제주도 먼 남쪽 해상으로 내려갔다가 20일께 다시 북상해 전남 남해안까지 영향을 줄 것으로 보인다. 이에 따라 20~21일 남부지방에도 예년보다 사흘 정도 장마가 일찍 찾아올 전망이다. 그러나 장마전선을 밀어올리는 북태평양 고기압 세력이 약해 서울 등 중부지방은 평년보다 사나흘가량 늦은 이달 말부터 장마가 시작될 것이라는 게 기상청의 설명이다. 장마전선은 이후 한 달가량 한반도 중남부를 오르내리며 곳곳에 비를 뿌릴 전망이다. 최근 30년간 평균치에 따르면 중부지방의 장마 시작일은 6월24~25일이었으며 장마기간은 32일, 강수일수는 17.2일이었다.기상청은 올해 장마기간의 평균 강수량이 350~400㎜로 평년과 비슷하거나 적을 것으로 내다봤다. 브라질 월드컵 한국과 러시아의 경기가 열리는 18일 오전 서울은 대체로 구름이 많이 끼지만 비는 오지 않을 것으로 예상돼 거리 응원에는 지장이 없을 전망이다.',\n","   'qas': [{'question': '북태평양 기단과 오호츠크해 기단이 만나 국내에 머무르는 기간은?',\n","     'answers': [{'text': '한 달가량', 'answer_start': 478},\n","      {'text': '한 달', 'answer_start': 478}],\n","     'guid': '798db07f0b9046759deed9d4a35ce31e'}]}],\n"," 'news_category': '종합',\n"," 'source': 'hankyung'}"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["input_dict[\"data\"][1]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nlXz6rjkT__1","executionInfo":{"status":"ok","timestamp":1673578148466,"user_tz":-540,"elapsed":24,"user":{"displayName":"이다원","userId":"11923633709773630529"}},"outputId":"f86dd607-6c96-4341-fab4-59158e1d026e"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'title': '부산정보산업진흥원, 과기부 지역SW서비스사업화 지원사업 4개 과제 선정',\n"," 'paragraphs': [{'context': '부산시와 (재)부산정보산업진흥원(원장 이인숙)이 ‘2020~2021년 지역SW서비스사업화 지원사업’ 공모사업에 4개 과제가 선정되어 본격적인 사업 착수에 나선다. 과학기술정보통신부가 주관하는 ‘지역SW서비스사업화 지원사업’은 강소SW기업 및 초기 스타트업의 SW서비스 사업화 지원과 신시장 진출 지원을 통해 기업 경쟁력 강화와 지역경제 활성화를 도모하는 사업이다. 올해부터 2개년으로 진행되며, 국비와 시비, 민자 등 2년간 약 37억원의 예산이 투입된다. 앞서 진흥원은 부산의 미래 먹거리산업인 스마트해양, 지능형기계, 지능정보서비스 분야로 사전 수요조사를 진행했고, 평가를 통해 선정된 5개 과제를 공모사업에 신청했다. 그 결과 부산의 4개 과제가 최종 선정되는 쾌거를 거뒀다. 당 사업은 전국 진흥기관을 대상으로 공모를 시작해, 총 17개 지역에서 42개 과제가 선정되었으며, 4개 과제가 선정된 곳은 부산과 강원지역 뿐이다. 금번 선정된 과제들은 ‘인공지능융합센서와 서보 이송 로봇을 이용한 전단보강재의 자동용접시스템 개발’ 등 총 4개 과제다. 부산시가 지원하고, 부산정보산업진흥원과 지역기업, 대학, 연구소 등이 컨소시엄을 구성하여 기술개발 및 사업화 지원을 추진한다. 2개의 Track으로 구분되는 이번사업은 Track 1(SW중소기업)에서 ㈜에이아이플랫폼, 엔컴(주), Track 2(스타트업)에서는 ㈜토즈, 삼보테크놀로지를 지원한다. ○ ‘Track 1‘의 (주)에이아이플랫폼이 주관기업으로 진행하는 <인공지능 기반 망막 내 아밀로이드 플라크 영상 분석을 통한 치매조기진단 플랫폼 상용화>는 치매 확진의 원인이 되는 중요 단백질(아밀로이드 플라크)을 자체개발 관측장비로 진단한다. 이를 통해 치매를 조기 발견하여, 각종 경제적 비용과 치료 및 예방 등 사회적 문제를 해 결하고 시민들이 쉽게 접근 가능한 실효성 있는 치매관리체계 개발을 목표로 한다. ○ 엔컴(주)이 주관기업으로 참여하는 <AI영상분석 기반 가공철근 생산성 향상 시스템 기술개발 및 사업화>는 산업안전, 환경규제, 생산체계의 변화로 침체된 부산 핵심 산업인 철강업 활성화에 나선다. 실시간으로 절곡되어 나오는 가공철근의 형상을 인식하고 불량 형상 판단 시 적합한 교정 값을 절곡설비에 전달함으로써, 무중단 생산이 가능한 영상분석 기술과 생산설비 자동화 제어기술을 개발한다. ○ ‘Track 2’의 ㈜토즈는 자립기반이 약한 국내 중소형 조선소의 산업기술 변화에 혁신적인 대응을 위해 <가상현실 기반 원격 다자간 선박 및 해양구조물 사전 검사 시스템>을 개발한다. 선박 건조 前, 설계 단계에서 설계자 뿐만 아니라 생산관리자, 품질관리자, 선급검사관, 선주감독관 등의 이해관계자가 공동으로 가상의 환경에서 선박 및 해양구조물의 자재 배치와 간섭, 작업성, 설계 오작 등에 대한 검사를 진행할 수 있는 기술을 확보하여 조선소의 업무효율을 극대화 할 예정이다. ○ 삼보테크놀로지는 재래식 건설 부자재의 시공성, 안전성, 내구성 등의 문제점을 보완하여 시민 안전과 건설근로자의 환경개선, 생산성 및 수익성 향상을 위해 <인공지능융합센서와 새들형 토치 서보 이송 로봇을 이용한 고속 SRD 전단보강재 자동용접시스템>을 개발한다. 로봇응용 SRD 용접자동화 설비를 제작하고, 용접 모니터링 및 품질검사 소프트웨어를 개발하여 건설분야에 4차산업 대비 지능형 생산자동화 기반기술을 확보할 예정이다. (재)부산정보산업진흥원 이인숙 원장은 “이번 코로나19 사태로 인해 부산 기업들이 매출과 고용유지, 자재수급 등에 큰 타격을 입었지만, 지역SW서비스사업화 지원사업을 통해 지역과 기업차원에서 기반을 다지는 계기가 됐으면 좋겠다‘며 ”진흥원은 어려운 사태를 대비해 지역 기업들을 지원할 수 있는 다른 방편을 계속 모색 중이며, 더욱 성장해 나갈 수 있도록 적극 지원하겠다“고 전했다.',\n","   'qas': [{'question': '지능형 생산자동화 기반기술을 개발중인 스타트업은?',\n","     'answers': [{'text': '삼보테크놀로지', 'answer_start': 1422}],\n","     'guid': '67c85e4f86ae43939b807684537c909c'}]}],\n"," 'news_category': '경제',\n"," 'source': 'acrofan'}"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["from copy import deepcopy\n","def split_input_dict(input_dict, ratio = 0.1, seed = 42):\n","    split_point = int(len(input_dict['data']) * ratio)\n","    random.seed(seed)\n","    random.shuffle(input_dict['data'])\n","    valid_dict = deepcopy(input_dict)\n","    train_dict = input_dict\n","\n","    valid_dict['data'] = input_dict['data'][:split_point]\n","    train_dict['data'] = input_dict['data'][split_point:]\n","    return train_dict, valid_dict"],"metadata":{"id":"rUTHPL_my3e9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def read_input(path):\n","    with open(path, 'rb') as f:\n","        input_dict = json.load(f)\n","    train_dict,valid_dict =split_input_dict(input_dict)\n","    train_contexts = []\n","    train_questions = []\n","    train_answers = []\n","    for group in tqdm(train_dict['data']):       #딕셔너리 하나씩 꺼낸다.\n","        for passage in group['paragraphs']:     #딕셔너리의 paragraphs\n","            context = passage['context']        #paragraphs의 context\n","            for qa in passage['qas']:           #paragraphs의 qas\n","                question = qa['question']       #paragraphs의 question\n","                for answer in qa['answers']:    #question의 answers\n","                    ### context 넘겨서 자르기 ###\n","                    target_context = context\n","                    if answer['answer_start'] > 1400:\n","                        target_context = target_context[1000:]\n","                        answer['answer_start'] -= 1000 \n","                    elif answer['answer_start'] > 1200:\n","                        target_context = target_context[800:]\n","                        answer['answer_start'] -= 800\n","                    elif answer['answer_start'] > 1000:\n","                        target_context = target_context[600:]\n","                        answer['answer_start'] -= 600\n","                    elif answer['answer_start'] > 800:\n","                        target_context = target_context[400:]\n","                        answer['answer_start'] -= 400\n","                    \n","                    train_contexts.append(target_context)    #answers의 한 answer당 해당하는 context 저장\n","                    ### context 넘겨서 자르기 ###\n","                    train_questions.append(question)  #answers의 한 answer당 해당하는 question 저장\n","                    train_answers.append(answer)      #answers의 한 answer 저장\n","  \n","    valid_contexts = []\n","    valid_questions = []\n","    valid_answers = []\n","    for group in tqdm(valid_dict['data']):      #딕셔너리 하나씩 꺼낸다.\n","        for passage in group['paragraphs']:     #딕셔너리의 paragraphs\n","            context = passage['context']        #paragraphs의 context\n","            for qa in passage['qas']:           #paragraphs의 qas\n","                question = qa['question']       #paragraphs의 question\n","                for answer in qa['answers']:    #question의 answers\n","                    ### context 넘겨서 자르기 ###\n","                    target_context = context\n","                    if answer['answer_start'] > 1400:\n","                        target_context = target_context[1000:]\n","                        answer['answer_start'] -= 1000 \n","                    elif answer['answer_start'] > 1200:\n","                        target_context = target_context[800:]\n","                        answer['answer_start'] -= 800\n","                    elif answer['answer_start'] > 1000:\n","                        target_context = target_context[600:]\n","                        answer['answer_start'] -= 600\n","                    elif answer['answer_start'] > 800:\n","                        target_context = target_context[400:]\n","                        answer['answer_start'] -= 400\n","                    \n","                    valid_contexts.append(target_context)    #answers의 한 answer당 해당하는 context 저장\n","                    ### context 넘겨서 자르기 ###\n","                    valid_questions.append(question)  #answers의 한 answer당 해당하는 question 저장\n","                    valid_answers.append(answer)      #answers의 한 answer 저장\n","    return train_contexts, train_questions, train_answers, valid_contexts, valid_questions, valid_answers"],"metadata":{"id":"QyvCyOeyUAuX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def add_end_idx(answers, contexts):\n","    for answer, context in zip(answers, contexts):\n","        gold_text = answer['text']\n","        start_idx = answer['answer_start']\n","        end_idx = start_idx + len(gold_text)\n","        answer['answer_end'] = end_idx\n","        if context[start_idx:end_idx] == gold_text:\n","            answer['answer_end'] = end_idx\n","        elif context[start_idx-1:end_idx-1] == gold_text:\n","            print(\"there is an unitended error in dataset\")\n","            answer['answer_start'] = start_idx - 1\n","            answer['answer_end'] = end_idx - 1\n","        elif context[start_idx-2:end_idx-2] == gold_text:\n","            print(\"there is an unitended error in dataset\")\n","            answer['answer_start'] = start_idx - 2\n","            answer['answer_end'] = end_idx - 2\n","        else:\n","            print(\"nope\")"],"metadata":{"id":"OBumo39dUH8R"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)"],"metadata":{"id":"MEN_bEdcUJZq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["question = \"북태평양 기단과 오호츠크해 기단이 만나 국내에 머무르는 기간은?\"\n","context = \"올여름 장마가 17일 제주도에서 시작됐다. 서울 등 중부지방은 예년보다 사나흘 정도 늦은 이달 말께 장마가 시작될 전망이다.17일 기상청에 따르면 제주도 남쪽 먼바다에 있는 장마전선의 영향으로 이날 제주도 산간 및 내륙지역에 호우주의보가 내려지면서 곳곳에 100㎜에 육박하는 많은 비가 내렸다. 제주의 장마는 평년보다 2~3일, 지난해보다는 하루 일찍 시작됐다. 장마는 고온다습한 북태평양 기단과 한랭 습윤한 오호츠크해 기단이 만나 형성되는 장마전선에서 내리는 비를 뜻한다.장마전선은 18일 제주도 먼 남쪽 해상으로 내려갔다가 20일께 다시 북상해 전남 남해안까지 영향을 줄 것으로 보인다. 이에 따라 20~21일 남부지방에도 예년보다 사흘 정도 장마가 일찍 찾아올 전망이다. 그러나 장마전선을 밀어올리는 북태평양 고기압 세력이 약해 서울 등 중부지방은 평년보다 사나흘가량 늦은 이달 말부터 장마가 시작될 것이라는 게 기상청의 설명이다. 장마전선은 이후 한 달가량 한반도 중남부를 오르내리며 곳곳에 비를 뿌릴 전망이다. 최근 30년간 평균치에 따르면 중부지방의 장마 시작일은 6월24~25일이었으며 장마기간은 32일, 강수일수는 17.2일이었다.기상청은 올해 장마기간의 평균 강수량이 350~400㎜로 평년과 비슷하거나 적을 것으로 내다봤다. 브라질 월드컵 한국과 러시아의 경기가 열리는 18일 오전 서울은 대체로 구름이 많이 끼지만 비는 오지 않을 것으로 예상돼 거리 응원에는 지장이 없을 전망이다.\"\n","tokenizer(context, question)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DVb8hk2WUKx0","executionInfo":{"status":"ok","timestamp":1673578241759,"user_tz":-540,"elapsed":8,"user":{"displayName":"이다원","userId":"11923633709773630529"}},"outputId":"01dbeb13-6d28-4f97-925d-c7db8cf9c90b"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'input_ids': [0, 1446, 22555, 11477, 2116, 3932, 2210, 6530, 27135, 3670, 2367, 2062, 18, 3671, 886, 9775, 16311, 2073, 12982, 2178, 2062, 15513, 3309, 3681, 798, 2073, 5277, 1041, 2678, 11477, 2116, 3670, 2651, 4016, 28674, 18, 3932, 2210, 11945, 2170, 3881, 2460, 6530, 7831, 1060, 10526, 2170, 1513, 2259, 11477, 2165, 2020, 2079, 3979, 6233, 3814, 6530, 24028, 1116, 12468, 17552, 2170, 24902, 3802, 2178, 2116, 23772, 31369, 5844, 2170, 3911, 3569, 2170, 10760, 2205, 2259, 1039, 2073, 1187, 2116, 5740, 2062, 18, 4364, 2079, 11477, 2259, 18673, 2178, 2062, 22, 97, 23, 2210, 16, 3736, 2178, 4000, 4051, 5947, 3670, 2367, 2062, 18, 11477, 2259, 19880, 2062, 2219, 2470, 1174, 18956, 26797, 2145, 1891, 2398, 1322, 2399, 2470, 22152, 2128, 2292, 2097, 26797, 2052, 4026, 4605, 2496, 2259, 11477, 2165, 2020, 27135, 4848, 2259, 1187, 2138, 936, 4538, 18, 11477, 2165, 2020, 2073, 3801, 2210, 6530, 1060, 7831, 7755, 6233, 12314, 4795, 3619, 2210, 2678, 3690, 25848, 2097, 4997, 18787, 2299, 2118, 3979, 2069, 1567, 575, 6233, 4090, 18, 1504, 2170, 3653, 3619, 97, 4041, 2210, 6904, 16311, 6509, 12982, 2178, 2062, 9229, 3681, 11477, 2116, 5947, 20930, 4016, 28674, 18, 3678, 11477, 2165, 2020, 2069, 18306, 26914, 2259, 1174, 18956, 27406, 4815, 2052, 10352, 3671, 886, 9775, 16311, 2073, 18673, 2178, 2062, 15513, 3309, 2116, 2199, 798, 2073, 5277, 1041, 3797, 11477, 2116, 3670, 2651, 575, 2052, 23548, 578, 11945, 2079, 3788, 28674, 18, 11477, 2165, 2020, 2073, 3719, 1891, 814, 2116, 2199, 5409, 15089, 2144, 2138, 18282, 2307, 5844, 2170, 1187, 2138, 1223, 2388, 4016, 28674, 18, 3744, 3740, 2440, 2366, 27923, 2170, 3881, 2460, 9775, 16311, 2079, 11477, 3670, 2210, 2073, 26, 2429, 21608, 97, 3912, 2210, 2052, 2359, 4007, 11477, 2015, 2366, 2073, 4987, 2210, 16, 12068, 2210, 2113, 2259, 3932, 18, 22, 2210, 2052, 2359, 2062, 18, 11945, 2073, 3753, 11477, 2015, 2366, 2079, 4233, 21474, 2052, 10906, 97, 5808, 3569, 2200, 18673, 2145, 4574, 2205, 9253, 1536, 2069, 575, 6233, 11997, 2062, 18, 6621, 5339, 3629, 2145, 4694, 2079, 3682, 2116, 4834, 2259, 3801, 2210, 4400, 3671, 2073, 7663, 7830, 2052, 3732, 712, 3683, 1187, 2259, 12065, 1380, 2069, 575, 6233, 4045, 2811, 4142, 6504, 2170, 2259, 11012, 2052, 1415, 2069, 4016, 28674, 18, 2, 1174, 18956, 26797, 2145, 22152, 2128, 2292, 2097, 26797, 2052, 4026, 3739, 2170, 13402, 2259, 3960, 2073, 35, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"]},"metadata":{},"execution_count":21}]},{"cell_type":"code","source":["class KlueDataset(Dataset):\n","    def __init__(self, contexts, questions, answers, model_max_position_embedings, tokenizer):\n","        self.tokenizer = tokenizer\n","        self.answers = answers\n","        self.questions = questions\n","        self.contexts = contexts\n","        self.model_max_position_embedings = model_max_position_embedings\n","        print(\"Tokenizing ...\")\n","        self.encodings = self.tokenizer(self.contexts, \n","                                        self.questions,\n","                                        max_length=512, #512 truncation\n","                                        truncation=True,\n","                                        padding=\"max_length\",\n","                                        return_token_type_ids=False)\n","        print(\"Done !!!\")\n","        self.add_token_positions()\n","        \n","    def add_token_positions(self):\n","        start_positions = []\n","        end_positions = []\n","        for i in range(len(self.answers)):\n","            start_positions.append(self.encodings.char_to_token(i, self.answers[i]['answer_start']))\n","            end_positions.append(self.encodings.char_to_token(i, self.answers[i]['answer_end'] - 1)) # -1으로 : 진짜로 답이 있는 end_position 의 인덱스를 구함.(char_to_token은 인덱스를 구함)\n","\n","            # positions 값이 None 값이라면, answer가 포함된 context가 잘렸다는 의미\n","            if start_positions[-1] is None:\n","                print(\"there is an error 1\")\n","                start_positions[-1] = self.model_max_position_embedings\n","            if end_positions[-1] is None:\n","                print(\"there is an error 2\")\n","                end_positions[-1] = self.model_max_position_embedings\n","\n","        self.encodings.update({'start_positions': start_positions, 'end_positions': end_positions})\n","\n","        \n","    def get_data(self):\n","        return {\"contexts\":self.contexts, 'questions':self.questions, 'answers':self.answers}\n","    \n","    \n","    def get_encodings(self):\n","        return self.encodings\n","        \n","    \n","    def __getitem__(self, idx):\n","        return {key:torch.tensor(val[idx]) for key, val in self.encodings.items()}\n","    \n","    def __len__(self):\n","        return len(self.encodings['input_ids'])"],"metadata":{"id":"1Q4tWEnKUMMa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_contexts, train_questions, train_answers, valid_contexts, valid_questions, valid_answers = read_input(\"/content/drive/MyDrive/WARNING_PRIVATE_FOLDER/goorm_nlp_8th_group3/new_train.json\")\n","add_end_idx(train_answers, train_contexts)                                                      #anwer 마다 answer_end 달아준다.\n","train_dataset = KlueDataset(train_contexts, train_questions, train_answers, 512, tokenizer)\n","\n","add_end_idx(valid_answers, valid_contexts)                                                      #anwer 마다 answer_end 달아준다.\n","valid_dataset = KlueDataset(valid_contexts, valid_questions, valid_answers, 512, tokenizer)"],"metadata":{"id":"rU76rNKaUO5i","colab":{"base_uri":"https://localhost:8080/","height":375,"referenced_widgets":["77f561f0f1c94936bcaa8b7d45e53d44","f8fd4bc9ed4e472aa9f35f01d6ee800a","2f2eb2539fac4f7a856403d3c9ac8701","10f9dbd3399f49038542f1f25167e12b","6fdb2a72977e416c8f6e28e9c398450d","69ac1e584cbd45d2aa8ff0c38628a8fc","ecc6f48bd94a41b2bf27213443378676","fe0ef43f11504adabd4131e8f6bca41f","fd4c1612aaa64a7cb43b5a450bb55aa7","6f367e83d6eb4d4f96c0aeb2183887b5","28ce1df75f454fdf85e9de026ada0e3d","c90eb242866f4816afb33092b521a7f9","32a2e94b3c864ea6a974099f92d1cdc7","348783115d0740bdb755dc0f91ad8ace","d42acfc1dc464540a488e43383a7222a","43e68534029c48d98e8aff9e50c1df09","9dd5d89a6d9744a88cfbf3f43983788c","119b9822069246ada041afdb6af99353","7ce230603d714703b26b5283e7b0c67d","6e2681aab0c64ce58639f749a03b9f0d","68c92a712ade492ab0de2a622d1d5bcc","ec37f2096ab94587a8aa03a50a25b0b5"]},"executionInfo":{"status":"ok","timestamp":1673578253161,"user_tz":-540,"elapsed":10240,"user":{"displayName":"이다원","userId":"11923633709773630529"}},"outputId":"6a8e7b38-981a-46f5-a4b2-332c03c408cb"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/16911 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"77f561f0f1c94936bcaa8b7d45e53d44"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/1878 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c90eb242866f4816afb33092b521a7f9"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["nope\n","nope\n","nope\n","there is an unitended error in dataset\n","there is an unitended error in dataset\n","Tokenizing ...\n","Done !!!\n","there is an error 1\n","there is an error 2\n","there is an error 2\n","there is an error 1\n","there is an error 2\n","there is an error 1\n","there is an error 2\n","Tokenizing ...\n","Done !!!\n"]}]},{"cell_type":"code","source":["model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n","torch.save(model.state_dict(),'./output_model_best')"],"metadata":{"id":"s-UGcsCxUQSd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["EPOCH = 3\n","LEARNING_RATE = 5e-5\n","BATCH_SIZE = 4\n","if do_accumulation:\n","    ACCUMULATION = 4\n","else:\n","    ACCUMULATION = 1"],"metadata":{"id":"LUqMZYh_CS0z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## do_wandb\n"],"metadata":{"id":"r5VA_73MARCd"}},{"cell_type":"code","source":["if do_wandb:\n","    !pip install wandb -qqq\n","    import wandb\n","    wandb.login()"],"metadata":{"id":"iQmen9OC9i9Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["if do_wandb:\n","    sweep_config = {\n","      'name' : 'wandb-monologg_koelectra-small-v2-distilled-korquad-384_lr.ipynb',\n","      'method' : 'grid',\n","      'metric':{\n","          'name': 'valid_total_loss',\n","          'goal': 'minimize'  \n","      },\n","      'parameters' : {\n","          'learning_rate' : {\n","              'values' : [5e-6,5e-5,5e-4] #\n","          },   \n","          'batch_size' :{\n","              'values' : [4]\n","          },\n","          'epochs' : {\n","              'values' : [2] \n","          }\n","      }\n","    }\n","\n","    sweep_id = wandb.sweep(sweep_config)"],"metadata":{"id":"EkLJgAJUAy3E"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train_runner_wandb():\n","    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","    \n","    ## wandb start ##\n","    wandb.init(project = 'test_sweep', reinit = True) \n","    train_batch_size = wandb.config.batch_size\n","    #valid_batch_size = wandb.config.batch_size\n","    valid_batch_size = 8\n","    learning_rate = wandb.config.learning_rate\n","    num_train_epochs = wandb.config.epochs\n","    ## wandb end ##\n","\n","    model.to(device)\n","    model.train()\n","    train_dataloader = DataLoader(dataset=train_dataset, batch_size=train_batch_size//ACCUMULATION)\n","    valid_dataloader = DataLoader(dataset = valid_dataset, batch_size = valid_batch_size//ACCUMULATION)\n","\n","    lowest_total_valid_loss = 9999.\n","    step = 0\n","    global_total_step = len(train_dataloader) * num_train_epochs\n","    optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=0)\n","    print(\"TRAIN START\")\n","    with tqdm(total=global_total_step, unit='step') as t:\n","        total = 0\n","        total_loss = 0\n","        for epoch in range(num_train_epochs):\n","            for iteration,batch in enumerate(train_dataloader):\n","                if not do_accumulation:\n","                    optimizer.zero_grad()\n","                input_ids = batch['input_ids'].to(device)\n","                attention_mask = batch['attention_mask'].to(device)\n","                start_positions = batch['start_positions'].to(device)\n","                end_positions = batch['end_positions'].to(device)\n","                outputs = model(input_ids,\n","                             attention_mask=attention_mask,\n","                             start_positions=start_positions,\n","                             end_positions=end_positions)\n","                loss = outputs.loss\n","\n","                if do_accumulation:\n","                    (loss / ACCUMULATION).backward()\n","\n","                    step += 1\n","                    if step % ACCUMULATION:\n","                        continue\n","\n","                    clip_grad_norm_(model.parameters(), max_norm=1.)\n","                    optimizer.step()\n","                    optimizer.zero_grad(set_to_none=True)\n","                else:\n","\n","                    loss.backward()\n","                    optimizer.step()\n","                \n","                batch_loss = loss.item() * len(input_ids)\n","                total += len(input_ids)\n","                total_loss += batch_loss / ACCUMULATION\n","                global_total_step += 1\n","                t.set_postfix(loss=\"{:.6f}\".format(total_loss / total), batch_loss=\"{:.6f}\".format(batch_loss))\n","                t.update(1)\n","\n","                ## wandb start ##\n","                if iteration % 20 == 0:\n","                    wandb.log({'train_batch_loss':batch_loss})\n","                ## wandb end ##\n","\n","                del input_ids\n","                del attention_mask\n","                del start_positions\n","                del end_positions\n","                del outputs\n","                del loss\n","\n","                ## validation ##\n","                if iteration != 0 and iteration % int(len(train_dataloader) / 10) == 0:\n","                    total_valid_loss = 0\n","                    for batch_val in valid_dataloader:\n","                        model.eval()\n","                        optimizer.zero_grad()\n","\n","                        input_ids = batch_val['input_ids'].to(device)\n","                        attention_mask = batch_val['attention_mask'].to(device)\n","                        start_positions = batch_val['start_positions'].to(device)\n","                        end_positions = batch_val['end_positions'].to(device)\n","                \n","                        with torch.no_grad():\n","                            outputs = model(input_ids,\n","                                    attention_mask=attention_mask,\n","                                    start_positions=start_positions,\n","                                    end_positions=end_positions)\n","                            loss = outputs.loss\n","                            total_valid_loss += loss.item()\n","\n","                    wandb.log({\"valid_total_loss\": total_valid_loss}) ##wandb\n","\n","                    if total_valid_loss < lowest_total_valid_loss:\n","                        print(f\"lowest_total_valid_loss: {total_valid_loss} epoch : {epoch} iteration : {iteration}\")\n","                        torch.save(model.state_dict(),'./output_model_best')\n","                        lowest_total_valid_loss = total_valid_loss\n","                ## validation ##\n","                \n","    print(\"TRAIN END\")"],"metadata":{"id":"Cc3moKwtBLe1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["if do_wandb:\n","    wandb.agent( sweep_id , function=train_runner_wandb, count=3)"],"metadata":{"id":"GD6xn8iGCCch"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## training"],"metadata":{"id":"tideN-xKAb1P"}},{"cell_type":"code","source":["def train_runner(model, train_dataset, valid_dataset , batch_size, num_train_epochs, learning_rate):\n","    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","    \n","    model.to(device)\n","    model.train()\n","    train_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size//ACCUMULATION)\n","    valid_dataloader = DataLoader(dataset = valid_dataset, batch_size = batch_size//ACCUMULATION)\n","\n","    lowest_total_valid_loss = 9999.\n","\n","    global_total_step = len(train_dataloader) * num_train_epochs\n","    optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=0)\n","    print(\"TRAIN START\")\n","\n","    step = 0\n","\n","    with tqdm(total=global_total_step, unit='step') as t:\n","        total = 0\n","        total_loss = 0\n","        for epoch in range(num_train_epochs):\n","            for iteration,batch in enumerate(train_dataloader):\n","                if not do_accumulation:\n","                    optimizer.zero_grad()\n","\n","                input_ids = batch['input_ids'].to(device)\n","                attention_mask = batch['attention_mask'].to(device)\n","                start_positions = batch['start_positions'].to(device)\n","                end_positions = batch['end_positions'].to(device)\n","                outputs = model(input_ids,\n","                             attention_mask=attention_mask,\n","                             start_positions=start_positions,\n","                             end_positions=end_positions)\n","                loss = outputs.loss\n","\n","                if do_accumulation:\n","                    (loss / ACCUMULATION).backward()\n","\n","                    step += 1\n","                    if step % ACCUMULATION:\n","                        continue\n","\n","                    clip_grad_norm_(model.parameters(), max_norm=1.)\n","                    optimizer.step()\n","                    optimizer.zero_grad(set_to_none=True)\n","                else:\n","\n","                    loss.backward()\n","                    optimizer.step()\n","                \n","                batch_loss = loss.item() * len(input_ids)\n","                total += len(input_ids)\n","                total_loss += batch_loss\n","                global_total_step += 1\n","                t.set_postfix(loss=\"{:.6f}\".format(total_loss / total), batch_loss=\"{:.6f}\".format(batch_loss))\n","                t.update(1)\n","                \n","                del input_ids\n","                del attention_mask\n","                del start_positions\n","                del end_positions\n","                del outputs\n","                del loss\n","\n","                ## validation ##\n","                if iteration != 0 and iteration % int(len(train_dataloader) / 10) == 0:\n","                    total_valid_loss = 0\n","                    for batch_val in valid_dataloader:\n","                        model.eval()\n","                        optimizer.zero_grad()\n","\n","                        input_ids = batch_val['input_ids'].to(device)\n","                        attention_mask = batch_val['attention_mask'].to(device)\n","                        start_positions = batch_val['start_positions'].to(device)\n","                        end_positions = batch_val['end_positions'].to(device)\n","                \n","                        with torch.no_grad():\n","                            outputs = model(input_ids,\n","                                    attention_mask=attention_mask,\n","                                    start_positions=start_positions,\n","                                    end_positions=end_positions)\n","                            loss = outputs.loss\n","                            total_valid_loss += loss.item()\n","                    \n","                    if total_valid_loss < lowest_total_valid_loss:\n","                        print(f\"lowest_total_valid_loss: {total_valid_loss} epoch : {epoch} iteration : {iteration}\")\n","                        torch.save(model.state_dict(),'./output_model_best')\n","                        lowest_total_valid_loss = total_valid_loss\n","                        early_stop_stack = 0\n","                    else:\n","                        early_stop_stack += 1\n","                        if early_stop_stack == 3:\n","                            print(\"SYSTEM HALT\")\n","                            return 0\n","                ## validation ##\n","    print(\"TRAIN END\")"],"metadata":{"id":"hAoMuSXCUUc0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_runner(model,train_dataset,valid_dataset, BATCH_SIZE, EPOCH, LEARNING_RATE)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":470,"referenced_widgets":["016583368ab24c1e90b2a49621ce16c3","659d099dff7d405dbc43c2bb76194d5e","4468eacacabc41189322cf18dfdb1523","5f7a5a4c71104dae9331eced05d22997","2dd5539a55704fa5b038815802d3f1e8","9026eb12f4d344fa87a28a423829b44c","59d2f161893b42b99412aa4840dd1a29","919450c55c044f018e8ec7ad951e071a","ad06331546204bdba7c764d2d4c0ec83","7122c69dc40a4e9dbc458b1085c73417","926578b953924f22acc125715d831a0a"]},"id":"0ixcdbKvUV-A","outputId":"caf36552-ca38-4df7-98c5-368d3d95cd05","executionInfo":{"status":"error","timestamp":1673588762618,"user_tz":-540,"elapsed":10498131,"user":{"displayName":"이다원","userId":"11923633709773630529"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["TRAIN START\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/24030 [00:00<?, ?step/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"016583368ab24c1e90b2a49621ce16c3"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["lowest_total_valid_loss: 1561.5310022234917 epoch : 0 iteration : 801\n","lowest_total_valid_loss: 1554.736141115427 epoch : 0 iteration : 3204\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-27-ea4fdf6c8e0f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_runner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalid_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPOCH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLEARNING_RATE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-26-15346717857a>\u001b[0m in \u001b[0;36mtrain_runner\u001b[0;34m(model, train_dataset, valid_dataset, batch_size, num_train_epochs, learning_rate)\u001b[0m\n\u001b[1;32m     27\u001b[0m                              end_positions=end_positions)\n\u001b[1;32m     28\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","source":["## prediction , postprocessing"],"metadata":{"id":"EWqvotWSDzZQ"}},{"cell_type":"code","source":["def read_dev(path):\n","    with open(path, 'rb') as f:\n","        dev_dict = json.load(f)\n","\n","    contexts = []\n","    questions = []\n","    answers = []\n","    guids = []\n","\n","    for group in tqdm(dev_dict['data']):\n","        for passage in group['paragraphs']:\n","            context = passage['context']\n","            for qa in passage['qas']:\n","                question = qa['question']\n","                guid = qa['guid']\n","                contexts.append(context)\n","                questions.append(question)\n","                guids.append(guid)\n","\n","    return contexts, questions , guids"],"metadata":{"id":"gpww3QngUXmP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dev_contexts, dev_questions, dev_guids = read_dev(\"/content/drive/MyDrive/WARNING_PRIVATE_FOLDER/goorm_nlp_8th_group3/goorm_nlp_8th_group3/project2/test.json\")"],"metadata":{"id":"3dT95zo0UZH7","colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["7dcf62cc7eef4faab5938f30e9de7c12","31bb6b56a8c046a4893ce8f1b0829200","232cc3978f8a4a7c9b7083db1ba2c2b2","73a44608ef5d414695ba62601326aed3","3e02d04e688c4c338da787ca015deb73","a2e5550c0d074bbcab45059d9a9d7fb2","ea3cf60a9bc6429d87b54cd3a0023df7","e527b36c1c694a91bca427d7201965da","2560346c196848779b67460b02e8894e","ed9dd6585bf64fceb5dcbd4fca1e6766","57473b20b11044199fcd5269e06e86e6"]},"executionInfo":{"status":"ok","timestamp":1673588768420,"user_tz":-540,"elapsed":1505,"user":{"displayName":"이다원","userId":"11923633709773630529"}},"outputId":"664c9447-131e-481a-8932-59375ec3caca"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/3709 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7dcf62cc7eef4faab5938f30e9de7c12"}},"metadata":{}}]},{"cell_type":"code","source":["import re\n","def remove_post(text):\n","        ''' 불필요한 기호 제거 '''\n","        \n","        text = re.sub(\"'\", \"\", text)\n","        text = re.sub('\"', \"\", text)\n","        text = re.sub('《', \"\", text)\n","        text = re.sub('》', \"\", text)\n","        text = re.sub('<', \"\", text)\n","        text = re.sub('>', \"\", text)\n","        text = re.sub('〈', \"\", text)\n","        text = re.sub('〉', \"\", text)\n","        text = re.sub(\"\\(\", \"\", text)\n","        text = re.sub(\"\\)\", \"\", text)\n","        text = re.sub(\"‘\", \"\", text)\n","        text = re.sub(\"’\", \"\", text)\n","        text = re.sub(\"  \", \" \", text)\n","        text = re.sub(\"#\", \"\", text)\n","        text = text.strip()\n","        return text"],"metadata":{"id":"T3L9vhb2JYXq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# start, end logit의 확률값을 이용한 예측 정답값\n","# logit의 상위 5개 확률을 리스트로 뽑아 틀린 정답이었다면 다음 확률로 넘어가서 확인.\n","\n","def to_list(tensor):\n","    return tensor.detach().cpu().tolist()\n","\n","def logits_change(input_ids, STA_logits, END_logits):\n","\n","    # 로짓의 확률값 ~ 상위 5개를 선택 \n","    # 틀린 추론이었다면 다음 선택 (틀린 추론 : start > end, 길이가 너무 긴 문장.)\n","    change_logit = 0\n","    cnt = 0\n","    \n","    # 기존 정답\n","    save_s = STA_logits\n","    save_e = END_logits\n","\n","    STK_start_index, STK_end_index = save_s.argmax(dim=-1), save_e.argmax(dim=-1)\n","    save_pred_ids = tokenizer.decode(input_ids[0][STK_start_index: STK_end_index + 1])\n","\n","    # 바뀐 정답\n","    STA_logits = to_list(STA_logits)[0]\n","    END_logits = to_list(END_logits)[0]\n","\n","    start_idx_and_logit = sorted(enumerate(STA_logits), key=lambda x: x[1], reverse=True)\n","    end_idx_and_logit = sorted(enumerate(END_logits), key=lambda x: x[1], reverse=True)\n","\n","    start_idx_and_logit = start_idx_and_logit[:5]\n","    end_idx_and_logit = end_idx_and_logit[:5]\n","\n","    TK_start_index, TK_end_index = start_idx_and_logit, end_idx_and_logit\n","\n","    pred_prob_loss = 0\n","\n","    for i in range(5):\n","        if TK_start_index[i][0] > TK_end_index[i][0] or TK_end_index[i][0] - TK_start_index[i][0] > 10 : \n","            cnt += 1\n","            continue\n","        else : \n","            change_logit += 1\n","            pred_ids = input_ids[0][TK_start_index[i][0]: TK_end_index[i][0] + 1]\n","            pred_prob_loss = TK_start_index[i][1] + TK_end_index[i][1]\n","            pred_ids = tokenizer.decode(pred_ids)\n","            #print(pred_ids, 'change')\n","            break\n","\n","    if change_logit == 0 :\n","        return save_pred_ids, max(save_s[0]) + max(save_e[0])\n","    elif cnt == 5:\n","        return '', 0\n","    else : \n","        if pred_ids == save_pred_ids:\n","            #print('same answer')\n","            return pred_ids , max(save_s[0]) + max(save_e[0])\n","        else :\n","            #print('different answer')\n","            return pred_ids , pred_prob_loss\n","\n","    \n","\n","# start_logits , end_logits\n","# index를 추적하면서 시작, 종료 index에 대한 확률이 가장 높은것을 선택하는 방법.\n","# 만약에 차이가 큰 start, end 값을 반환할때 이 정보들을 저장하지 않고 넘긴다면? \n"],"metadata":{"id":"VUok6PUJHVWg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pickle\n","\n","def prediction(contexts, questions, guids):\n","    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","\n","    model.load_state_dict(torch.load('./output_model_best'))\n","    model.to(device)\n","    \n","    model.eval()\n","    \n","    result = []\n","    list_dict_all = []\n","    list_dict_all2 = []\n","    with torch.no_grad():\n","        \n","        for context, question, guid in zip(contexts, questions, guids):\n","            encodings = tokenizer(context, question, max_length=512, truncation=True,\n","                                     padding=\"max_length\", return_token_type_ids=False)\n","            encodings = {key: torch.tensor([val]) for key, val in encodings.items()}\n","            \n","            input_ids = encodings[\"input_ids\"].to(device)\n","            attention_mask = encodings[\"attention_mask\"].to(device)\n","            \n","            outputs = model(input_ids, attention_mask=attention_mask)\n","            start_logits, end_logits = outputs.start_logits, outputs.end_logits\n","\n","            ### uploading pickle ###\n","            dict_all = {\"input_ids\":input_ids,\"start_logits\":start_logits,\"end_logits\":end_logits,\"tokenizer_name\":tokenizer_name,\"guid\":guid}\n","            list_dict_all.append(dict(dict_all))\n","            ### uploading pickle ###\n","\n","            pred, prob1 =logits_change(input_ids, start_logits, end_logits)\n","\n","\n","\n","            ### context 뒷부분 test ###\n","            if len(context) > 800:\n","                if len(context) > 1600:\n","                    context=context[800:]\n","                elif len(context) > 1400:\n","                    context = context[600:]\n","                elif len(context) > 1200:\n","                    context = context[400:]\n","                elif len(context) > 1000:\n","                    context = context[200:]\n","                else:\n","                    context = context[100:]\n","                encodings = tokenizer(context, question, max_length=512, truncation=True,\n","                                     padding=\"max_length\", return_token_type_ids=False)\n","                encodings = {key: torch.tensor([val]) for key, val in encodings.items()}\n","            \n","                input_ids = encodings[\"input_ids\"].to(device)\n","                attention_mask = encodings[\"attention_mask\"].to(device)\n","            \n","                outputs = model(input_ids, attention_mask=attention_mask)\n","                start_logits2, end_logits2 = outputs.start_logits, outputs.end_logits\n","                ### uploading pickle ###\n","                dict_all2 = {\"input_ids\":input_ids,\"start_logits\":start_logits,\"end_logits\":end_logits,\"tokenizer_name\":tokenizer_name,\"guid\":guid}\n","                list_dict_all2.append(dict(dict_all))\n","\n","                pred2, prob2 = logits_change(input_ids, start_logits2, end_logits2)\n","\n","                if prob1 < prob2:\n","                    pred = pred2\n","            else:\n","                encodings = tokenizer(context, question, max_length=512, truncation=True,\n","                                     padding=\"max_length\", return_token_type_ids=False)\n","                encodings = {key: torch.tensor([val]) for key, val in encodings.items()}\n","            \n","                input_ids = encodings[\"input_ids\"].to(device)\n","                attention_mask = encodings[\"attention_mask\"].to(device)\n","            \n","                outputs = model(input_ids, attention_mask=attention_mask)\n","                start_logits2, end_logits2 = outputs.start_logits, outputs.end_logits\n","                ### uploading pickle ###\n","                dict_all2 = {\"input_ids\":input_ids,\"start_logits\":start_logits,\"end_logits\":end_logits,\"tokenizer_name\":tokenizer_name,\"guid\":guid}\n","                list_dict_all2.append(dict(dict_all))\n","                ### uploading pickle ###\n","            \n","\n","            \n","\n","            ### context 뒷부분 test ###\n","            pred = pred[:8]\n","            pred = remove_post(pred)\n","            tp = (guid,pred)\n","            \n","            result.append(tp)\n","    with open('nlp04_org_and_korquad_new_train_part_1.pickle', 'wb') as f:\n","            pickle.dump(list_dict_all, f)\n","    with open('nlp04_org_and_korquad_new_train_part_2.pickle', 'wb') as f:\n","            pickle.dump(list_dict_all2, f)\n","    return result"],"metadata":{"id":"1YMQbh-qUanh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pred_answers = prediction(dev_contexts, dev_questions, dev_guids)\n","#pred_answers"],"metadata":{"id":"oEQ3CJBeUbze"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import csv\n","f = open('nlp04_org_and_korquad_new_train_1.csv','w', newline='')\n","wr = csv.writer(f)\n","wr.writerow(['Id','Predicted'])\n","\n","for tp in pred_answers:\n","    wr.writerow([tp[0],tp[1]])\n","\n","f.close()"],"metadata":{"id":"-Es9B1ZFjMbP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Levenshtein_distance (Evaluation)\n","\n","import numpy \n","import torch\n","import os\n","\n","def levenshtein_distance(s1,s2, debug=False): #레벤슈타인 거리 eval / 정답 s1과 도출한 모델 s2 비교 평가\n","    if len(s1) < len(s2):\n","        return levenshtein_distance(s2, s1, debug)\n","\n","    if len(s2) == 0:\n","        return len(s1)\n","\n","    previous_row = range(len(s2) + 1)\n","    for i, c1 in enumerate(s1):\n","        current_row = [i + 1]\n","        for j, c2 in enumerate(s2):\n","            insertions = previous_row[j + 1] + 1\n","            deletions = current_row[j] + 1\n","            substitutions = previous_row[j] + (c1 != c2)\n","            current_row.append(min(insertions, deletions, substitutions))\n","\n","        if debug:\n","            print(current_row[1:])\n","\n","        previous_row = current_row\n","\n","    return previous_row[-1] # levenshtein_distance 값 출력\n","\n","\n","def LD_SCORE(start_positions, end_positions, input_ids, STA_logits, END_logits):\n","\n","    answer1 = [] # 정답 저장\n","    answer2 = [] # 예측 정답 저장\n","\n","    if len(input_ids) != BATCH_SIZE: #오류 해결\n","       print(\"input_ids ERROR\")\n","       return 0\n","\n","    for i in range(BATCH_SIZE): # 기존 정답 // index 1 is out of bounds for dimension 0 with size 1 오류 발생\n","        if input_ids[i] == [] :\n","            print(\"input_ids ERROR\")\n","            continue       \n","        else:\n","            PRED_IDE = input_ids[i][start_positions[i]: end_positions[i] + 1]\n","            PRED_ANS = tokenizer.decode(PRED_IDE)\n","            answer1.append(PRED_ANS)\n","            #print(PRED_ANS)\n","\n","    for i in range(BATCH_SIZE): # 예측 정답\n","        if input_ids[i] == [] :\n","            print(\"input_ids ERROR\")\n","            continue\n","        else:\n","            TK_start_index, TK_end_index = STA_logits.argmax(dim=-1), END_logits.argmax(dim=-1)\n","            PRED_IDE2 = input_ids[i][TK_start_index[i]: TK_end_index[i] + 1]\n","            PRED_ANS2 = tokenizer.decode(PRED_IDE2)\n","            answer2.append(PRED_ANS2)\n","            # print(PRED_ANS2)\n","\n","    batch_score = LD_comparison(answer1, answer2)\n","\n","    return batch_score\n","\n","\n","def LD_comparison(answer1,answer2):\n","\n","    # 배치마다 레벤슈타인 거리 평균 구해서 출력. (train, valid 과정에서 배치마다 평균거리 구하고.)\n","    # train 전체 평균 거리\n","    # valid 전체 평균 거리\n","\n","    batch_LD_score = []\n","\n","    for i in range(BATCH_SIZE):\n","        if answer1[i] == answer2[i]: # 같으면 LD 구하는 과정 생략.\n","            batch_LD_score.append(0)\n","        else:\n","            batch_LD_score.append(levenshtein_distance(answer1[i],answer2[i]))\n","\n","    sum_LD_score = sum(batch_LD_score)\n","    LD_avg = sum_LD_score / BATCH_SIZE # 배치의 레벤슈타인 거리 평균\n","    print(LD_avg)\n","\n","    return LD_avg\n","\n","\n","\n","\n","# levenshtein_distance 값이 튀는 현상 방지.\n","#  - 길이가 너무 긴 정답 삭제 max_length = 20\n","#  - 정답과 어느정도 길이차이나는 값 삭제(위와 동일) 5.9 -> 2정도 차이나는 값 삭제 7.9\n"," \n","\n","# 자연어처리\n","# 자연어처리과정 2\n","# 0 5\n","\n","# 정답 길이보다 예측 정답 길이가 길지만 2배이상 차이가 나지 않는 경우에\n","# 0으로 처리하면 LD_SCORE가 더 안나옴\n","# 따라서 위 상황에서는 1.5 ~ 1.8 정도 해당하는 길이로 잘라서 반환.\n","\n","# test 답 X\n","# train 과정 나온 답 -> 바꿔서 바꾼 데이터로 \n","\n","\n","# ** max_len = 5.9 * 2 = 12\n","\n","# train 2번\n","# 1 train 원래 데이터.\n","# 2 train LD 변환한 데이터로 한번.\n","\n","\n","\n"],"metadata":{"id":"znO7xNhXh-JG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# pred_answers에서 도출한 정답 비교\n","import csv\n","\n","def calculate_Leven(source, ref, result_file):\n","    with open(source, 'r') as input1:\n","        with open(ref, 'r') as input2:\n","            with open(result_file, 'w') as csvoutput:\n","                reader1 = csv.reader(input1)\n","                reader2 = list(csv.reader(input2))\n","                writer = csv.writer(csvoutput)\n","                result = []\n","                mean = []\n","                headers = next(reader1)\n","                result.append(headers)\n","                index = 0\n","                for row1 in reader1:\n","                    index+=1\n","                    a = levenshtein_distance(row1[1], reader2[index][1])\n","                    mean.append(a)\n","                    row1.append(a)\n","                    result.append(row1)\n","                mean1 = sum(mean) / len(mean)\n","                print(mean1)\n","                writer.writerows(result)"],"metadata":{"id":"vPg9ohQOiQ-c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import files\n","uploaded = files.upload()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":184},"id":"idYSu_idiRt1","executionInfo":{"status":"ok","timestamp":1673589838977,"user_tz":-540,"elapsed":30860,"user":{"displayName":"이다원","userId":"11923633709773630529"}},"outputId":"1408f488-804f-4120-c8ed-47401a5292d8"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","     <input type=\"file\" id=\"files-37a6dc6a-82e0-486f-9b01-ffca69e9162f\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-37a6dc6a-82e0-486f-9b01-ffca69e9162f\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script>// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) => {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable<!Object>} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) => {\n","    inputElement.addEventListener('change', (e) => {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) => {\n","    cancel.onclick = () => {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) => {\n","      const reader = new FileReader();\n","      reader.onload = (e) => {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position < fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","</script> "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saving ainize_klue-bert-base-mrc.csv to ainize_klue-bert-base-mrc.csv\n","Saving ainize_klue-bert-base-mrc_10truncation_real.csv to ainize_klue-bert-base-mrc_10truncation_real.csv\n","Saving ainize_klue-bert-base-mrc-10truncation.csv to ainize_klue-bert-base-mrc-10truncation.csv\n","Saving klue_10trunc_postprocess.csv to klue_10trunc_postprocess.csv\n"]}]},{"cell_type":"code","source":["calculate_Leven('ainize_klue-bert-base-mrc_LogitPRED.csv', 'nlp04_org_and_korquad_new_train_1.csv', 'result_file.csv')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"acfGFS9_iioA","executionInfo":{"status":"ok","timestamp":1673589838980,"user_tz":-540,"elapsed":44,"user":{"displayName":"이다원","userId":"11923633709773630529"}},"outputId":"292c376a-74a5-4816-eb5a-2762a060aa4f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2.463323353293413\n"]}]},{"cell_type":"code","source":["calculate_Leven('ainize_klue-bert-base-mrc.csv', 'nlp04_org_and_korquad_new_train_1.csv', 'result_file.csv')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9r2Xca6Vi5ZB","executionInfo":{"status":"ok","timestamp":1673589838981,"user_tz":-540,"elapsed":36,"user":{"displayName":"이다원","userId":"11923633709773630529"}},"outputId":"eb0c8da3-0544-4a16-ab50-2b00c002165b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["11.368263473053892\n"]}]},{"cell_type":"code","source":["calculate_Leven('ainize_klue-bert-base-mrc_10truncation_real.csv', 'nlp04_org_and_korquad_new_train_1.csv', 'result_file.csv')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ne9qu_j7jBjh","executionInfo":{"status":"ok","timestamp":1673589838982,"user_tz":-540,"elapsed":28,"user":{"displayName":"이다원","userId":"11923633709773630529"}},"outputId":"f6361edb-2dfe-44a0-9dfe-92076456fdcb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2.9620758483033933\n"]}]},{"cell_type":"code","source":["calculate_Leven('ainize_klue-bert-base-mrc-10truncation.csv', 'nlp04_org_and_korquad_new_train_1.csv', 'result_file.csv')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XM5queVvjCIj","executionInfo":{"status":"ok","timestamp":1673589838983,"user_tz":-540,"elapsed":23,"user":{"displayName":"이다원","userId":"11923633709773630529"}},"outputId":"fb2f502d-75a0-45d7-8d25-a160b49cbe30"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["3.2669660678642716\n"]}]},{"cell_type":"code","source":["calculate_Leven('klue_10trunc_postprocess.csv', 'nlp04_org_and_korquad_new_train_1.csv', 'result_file.csv')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RObH3voEjCnf","executionInfo":{"status":"ok","timestamp":1673589839574,"user_tz":-540,"elapsed":608,"user":{"displayName":"이다원","userId":"11923633709773630529"}},"outputId":"05d49859-c8aa-4f49-d0d1-f5882572574d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2.9598303393213574\n"]}]}]}